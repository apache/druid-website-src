"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[5584],{28453:(e,t,o)=>{o.d(t,{R:()=>r,x:()=>d});var n=o(96540);const s={},i=n.createContext(s);function r(e){const t=n.useContext(i);return n.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),n.createElement(i.Provider,{value:t},e.children)}},34123:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>a,contentTitle:()=>d,default:()=>h,frontMatter:()=>r,metadata:()=>n,toc:()=>l});const n=JSON.parse('{"id":"operations/export-metadata","title":"Export Metadata Tool","description":"\x3c!--","source":"@site/docs/32.0.0/operations/export-metadata.md","sourceDirName":"operations","slug":"/operations/export-metadata","permalink":"/docs/32.0.0/operations/export-metadata","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"export-metadata","title":"Export Metadata Tool"},"sidebar":"docs","previous":{"title":"Deep storage migration","permalink":"/docs/32.0.0/operations/deep-storage-migration"},"next":{"title":"Metadata Migration","permalink":"/docs/32.0.0/operations/metadata-migration"}}');var s=o(74848),i=o(28453);const r={id:"export-metadata",title:"Export Metadata Tool"},d=void 0,a={},l=[{value:"<code>export-metadata</code> Options",id:"export-metadata-options",level:2},{value:"Connection Properties",id:"connection-properties",level:3},{value:"Output Path",id:"output-path",level:3},{value:"Export Format Options",id:"export-format-options",level:3},{value:"Deep Storage Migration",id:"deep-storage-migration",level:3},{value:"Migration to S3 Deep Storage",id:"migration-to-s3-deep-storage",level:4},{value:"Migration to HDFS Deep Storage",id:"migration-to-hdfs-deep-storage",level:4},{value:"Migration to New Local Deep Storage Path",id:"migration-to-new-local-deep-storage-path",level:4},{value:"Running the tool",id:"running-the-tool",level:2},{value:"Importing Metadata",id:"importing-metadata",level:2},{value:"Derby",id:"derby",level:3},{value:"MySQL",id:"mysql",level:3},{value:"PostgreSQL",id:"postgresql",level:3}];function c(e){const t={code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:["Druid includes an ",(0,s.jsx)(t.code,{children:"export-metadata"})," tool for assisting with migration of cluster metadata and deep storage."]}),"\n",(0,s.jsx)(t.p,{children:"This tool exports the contents of the following Druid metadata tables:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"segments"}),"\n",(0,s.jsx)(t.li,{children:"rules"}),"\n",(0,s.jsx)(t.li,{children:"config"}),"\n",(0,s.jsx)(t.li,{children:"datasource"}),"\n",(0,s.jsx)(t.li,{children:"supervisors"}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"Additionally, the tool can rewrite the local deep storage location descriptors in the rows of the segments table\nto point to new deep storage locations (S3, HDFS, and local rewrite paths are supported)."}),"\n",(0,s.jsx)(t.p,{children:"The tool has the following limitations:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsx)(t.li,{children:"Only exporting from Derby metadata is currently supported"}),"\n",(0,s.jsx)(t.li,{children:"If rewriting load specs for deep storage migration, only migrating from local deep storage is currently supported."}),"\n"]}),"\n",(0,s.jsxs)(t.h2,{id:"export-metadata-options",children:[(0,s.jsx)(t.code,{children:"export-metadata"})," Options"]}),"\n",(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(t.code,{children:"export-metadata"})," tool provides the following options:"]}),"\n",(0,s.jsx)(t.h3,{id:"connection-properties",children:"Connection Properties"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"--connectURI"}),": The URI of the Derby database, e.g. ",(0,s.jsx)(t.code,{children:"jdbc:derby://localhost:1527/var/druid/metadata.db;create=true"})]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"--user"}),": Username"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"--password"}),": Password"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"--base"}),": corresponds to the value of ",(0,s.jsx)(t.code,{children:"druid.metadata.storage.tables.base"})," in the configuration, ",(0,s.jsx)(t.code,{children:"druid"})," by default."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"output-path",children:"Output Path"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"--output-path"}),", ",(0,s.jsx)(t.code,{children:"-o"}),": The output directory of the tool. CSV files for the Druid segments, rules, config, datasource, and supervisors tables will be written to this directory."]}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"export-format-options",children:"Export Format Options"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"--use-hex-blobs"}),", ",(0,s.jsx)(t.code,{children:"-x"}),": If set, export BLOB payload columns as hexadecimal strings. This needs to be set if importing back into Derby. Default is false."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"--booleans-as-strings"}),", ",(0,s.jsx)(t.code,{children:"-t"}),': If set, write boolean values as "true" or "false" instead of "1" and "0". This needs to be set if importing back into Derby. Default is false.']}),"\n"]}),"\n",(0,s.jsx)(t.h3,{id:"deep-storage-migration",children:"Deep Storage Migration"}),"\n",(0,s.jsx)(t.h4,{id:"migration-to-s3-deep-storage",children:"Migration to S3 Deep Storage"}),"\n",(0,s.jsx)(t.p,{children:"By setting the options below, the tool will rewrite the segment load specs to point to a new S3 deep storage location."}),"\n",(0,s.jsx)(t.p,{children:"This helps users migrate segments stored in local deep storage to S3."}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"--s3bucket"}),", ",(0,s.jsx)(t.code,{children:"-b"}),": The S3 bucket that will hold the migrated segments"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"--s3baseKey"}),", ",(0,s.jsx)(t.code,{children:"-k"}),": The base S3 key where the migrated segments will be stored"]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"When copying the local deep storage segments to S3, the rewrite performed by this tool requires that the directory structure of the segments be unchanged."}),"\n",(0,s.jsx)(t.p,{children:"For example, if the cluster had the following local deep storage configuration:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"druid.storage.type=local\ndruid.storage.storageDirectory=/druid/segments\n"})}),"\n",(0,s.jsxs)(t.p,{children:["If the target S3 bucket was ",(0,s.jsx)(t.code,{children:"migration"}),", with a base key of ",(0,s.jsx)(t.code,{children:"example"}),", the contents of ",(0,s.jsx)(t.code,{children:"s3://migration/example/"})," must be identical to that of ",(0,s.jsx)(t.code,{children:"/druid/segments"})," on the old local filesystem."]}),"\n",(0,s.jsx)(t.h4,{id:"migration-to-hdfs-deep-storage",children:"Migration to HDFS Deep Storage"}),"\n",(0,s.jsx)(t.p,{children:"By setting the options below, the tool will rewrite the segment load specs to point to a new HDFS deep storage location."}),"\n",(0,s.jsx)(t.p,{children:"This helps users migrate segments stored in local deep storage to HDFS."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.code,{children:"--hadoopStorageDirectory"}),", ",(0,s.jsx)(t.code,{children:"-h"}),": The HDFS path that will hold the migrated segments"]}),"\n",(0,s.jsxs)(t.p,{children:["When copying the local deep storage segments to HDFS, the rewrite performed by this tool requires that the directory structure of the segments be unchanged, with the exception of directory names containing colons (",(0,s.jsx)(t.code,{children:":"}),")."]}),"\n",(0,s.jsx)(t.p,{children:"For example, if the cluster had the following local deep storage configuration:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"druid.storage.type=local\ndruid.storage.storageDirectory=/druid/segments\n"})}),"\n",(0,s.jsxs)(t.p,{children:["If the target hadoopStorageDirectory was ",(0,s.jsx)(t.code,{children:"/migration/example"}),", the contents of ",(0,s.jsx)(t.code,{children:"hdfs:///migration/example/"})," must be identical to that of ",(0,s.jsx)(t.code,{children:"/druid/segments"})," on the old local filesystem."]}),"\n",(0,s.jsxs)(t.p,{children:["Additionally, the segments paths in local deep storage contain colons(",(0,s.jsx)(t.code,{children:":"}),") in their names, e.g.:"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.code,{children:"wikipedia/2016-06-27T02:00:00.000Z_2016-06-27T03:00:00.000Z/2019-05-03T21:57:15.950Z/1/index.zip"})}),"\n",(0,s.jsxs)(t.p,{children:["HDFS cannot store files containing colons, and this tool expects the colons to be replaced with underscores (",(0,s.jsx)(t.code,{children:"_"}),") in HDFS."]}),"\n",(0,s.jsxs)(t.p,{children:["In this example, the ",(0,s.jsx)(t.code,{children:"wikipedia"})," segment above under ",(0,s.jsx)(t.code,{children:"/druid/segments"})," in local deep storage would need to be migrated to HDFS under ",(0,s.jsx)(t.code,{children:"hdfs:///migration/example/"})," with the following path:"]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.code,{children:"wikipedia/2016-06-27T02_00_00.000Z_2016-06-27T03_00_00.000Z/2019-05-03T21_57_15.950Z/1/index.zip"})}),"\n",(0,s.jsx)(t.h4,{id:"migration-to-new-local-deep-storage-path",children:"Migration to New Local Deep Storage Path"}),"\n",(0,s.jsx)(t.p,{children:"By setting the options below, the tool will rewrite the segment load specs to point to a new local deep storage location."}),"\n",(0,s.jsx)(t.p,{children:"This helps users migrate segments stored in local deep storage to a new path (e.g., a new NFS mount)."}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.code,{children:"--newLocalPath"}),", ",(0,s.jsx)(t.code,{children:"-n"}),": The new path on the local filesystem that will hold the migrated segments"]}),"\n",(0,s.jsx)(t.p,{children:"When copying the local deep storage segments to a new path, the rewrite performed by this tool requires that the directory structure of the segments be unchanged."}),"\n",(0,s.jsx)(t.p,{children:"For example, if the cluster had the following local deep storage configuration:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"druid.storage.type=local\ndruid.storage.storageDirectory=/druid/segments\n"})}),"\n",(0,s.jsxs)(t.p,{children:["If the new path  was ",(0,s.jsx)(t.code,{children:"/migration/example"}),", the contents of ",(0,s.jsx)(t.code,{children:"/migration/example/"})," must be identical to that of ",(0,s.jsx)(t.code,{children:"/druid/segments"})," on the local filesystem."]}),"\n",(0,s.jsx)(t.h2,{id:"running-the-tool",children:"Running the tool"}),"\n",(0,s.jsx)(t.p,{children:"To use the tool, you can run the following from the root of the Druid package:"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-bash",children:'cd ${DRUID_ROOT}\nmkdir -p /tmp/csv\njava -classpath "lib/*" -Dlog4j.configurationFile=conf/druid/cluster/_common/log4j2.xml -Ddruid.extensions.directory="extensions" -Ddruid.extensions.loadList=[] org.apache.druid.cli.Main tools export-metadata --connectURI "jdbc:derby://localhost:1527/var/druid/metadata.db;" -o /tmp/csv\n'})}),"\n",(0,s.jsx)(t.p,{children:"In the example command above:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"lib"})," is the Druid lib directory"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"extensions"})," is the Druid extensions directory"]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.code,{children:"/tmp/csv"})," is the output directory. Please make sure that this directory exists."]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{id:"importing-metadata",children:"Importing Metadata"}),"\n",(0,s.jsxs)(t.p,{children:["After running the tool, the output directory will contain ",(0,s.jsx)(t.code,{children:"<table-name>_raw.csv"})," and ",(0,s.jsx)(t.code,{children:"<table-name>.csv"})," files."]}),"\n",(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(t.code,{children:"<table-name>_raw.csv"})," files are intermediate files used by the tool, containing the table data as exported by Derby without modification."]}),"\n",(0,s.jsxs)(t.p,{children:["The ",(0,s.jsx)(t.code,{children:"<table-name>.csv"})," files are used for import into another database such as MySQL and PostgreSQL and have any configured deep storage location rewrites applied."]}),"\n",(0,s.jsx)(t.p,{children:"Example import commands for Derby, MySQL, and PostgreSQL are shown below."}),"\n",(0,s.jsxs)(t.p,{children:["These example import commands expect ",(0,s.jsx)(t.code,{children:"/tmp/csv"})," and its contents to be accessible from the server. For other options, such as importing from the client filesystem, please refer to the database's documentation."]}),"\n",(0,s.jsx)(t.h3,{id:"derby",children:"Derby"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"CALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SEGMENTS','/tmp/csv/druid_segments.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_RULES','/tmp/csv/druid_rules.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_CONFIG','/tmp/csv/druid_config.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_DATASOURCE','/tmp/csv/druid_dataSource.csv',',','\"',null,0);\n\nCALL SYSCS_UTIL.SYSCS_IMPORT_TABLE (null,'DRUID_SUPERVISORS','/tmp/csv/druid_supervisors.csv',',','\"',null,0);\n"})}),"\n",(0,s.jsx)(t.h3,{id:"mysql",children:"MySQL"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"LOAD DATA INFILE '/tmp/csv/druid_segments.csv' INTO TABLE druid_segments FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,dataSource,created_date,start,end,partitioned,version,used,payload); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_rules.csv' INTO TABLE druid_rules FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,dataSource,version,payload); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_config.csv' INTO TABLE druid_config FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (name,payload); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_dataSource.csv' INTO TABLE druid_dataSource FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (dataSource,created_date,commit_metadata_payload,commit_metadata_sha1); SHOW WARNINGS;\n\nLOAD DATA INFILE '/tmp/csv/druid_supervisors.csv' INTO TABLE druid_supervisors FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\\\"' (id,spec_id,created_date,payload); SHOW WARNINGS;\n"})}),"\n",(0,s.jsx)(t.h3,{id:"postgresql",children:"PostgreSQL"}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{className:"language-sql",children:"COPY druid_segments(id,dataSource,created_date,start,\"end\",partitioned,version,used,payload) FROM '/tmp/csv/druid_segments.csv' DELIMITER ',' CSV;\n\nCOPY druid_rules(id,dataSource,version,payload) FROM '/tmp/csv/druid_rules.csv' DELIMITER ',' CSV;\n\nCOPY druid_config(name,payload) FROM '/tmp/csv/druid_config.csv' DELIMITER ',' CSV;\n\nCOPY druid_dataSource(dataSource,created_date,commit_metadata_payload,commit_metadata_sha1) FROM '/tmp/csv/druid_dataSource.csv' DELIMITER ',' CSV;\n\nCOPY druid_supervisors(id,spec_id,created_date,payload) FROM '/tmp/csv/druid_supervisors.csv' DELIMITER ',' CSV;\n"})})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);