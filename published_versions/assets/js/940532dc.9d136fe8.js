"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[2233],{28264:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>d,default:()=>l,frontMatter:()=>s,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"development/extensions-contrib/thrift","title":"Thrift","description":"\x3c!--","source":"@site/docs/32.0.0/development/extensions-contrib/thrift.md","sourceDirName":"development/extensions-contrib","slug":"/development/extensions-contrib/thrift","permalink":"/docs/32.0.0/development/extensions-contrib/thrift","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"thrift","title":"Thrift"}}');var i=n(74848),o=n(28453);const s={id:"thrift",title:"Thrift"},d=void 0,c={},a=[{value:"LZO Support",id:"lzo-support",level:2},{value:"Thrift Parser",id:"thrift-parser",level:2}];function h(e){const t={a:"a",code:"code",h2:"h2",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(t.p,{children:["To use this Apache Druid extension, ",(0,i.jsx)(t.a,{href:"/docs/32.0.0/configuration/extensions#loading-extensions",children:"include"})," ",(0,i.jsx)(t.code,{children:"druid-thrift-extensions"})," in the extensions load list."]}),"\n",(0,i.jsxs)(t.p,{children:["This extension enables Druid to ingest thrift compact data online (",(0,i.jsx)(t.code,{children:"ByteBuffer"}),") and offline (SequenceFile of type ",(0,i.jsx)(t.code,{children:"<Writable, BytesWritable>"})," or LzoThriftBlock File)."]}),"\n",(0,i.jsx)(t.p,{children:"You may want to use another version of thrift, change the dependency in pom and compile yourself."}),"\n",(0,i.jsx)(t.h2,{id:"lzo-support",children:"LZO Support"}),"\n",(0,i.jsxs)(t.p,{children:["If you plan to read LZO-compressed Thrift files, you will need to download version 0.4.19 of the ",(0,i.jsx)(t.a,{href:"https://mvnrepository.com/artifact/com.hadoop.gplcompression/hadoop-lzo/0.4.19",children:"hadoop-lzo JAR"})," and place it in your ",(0,i.jsx)(t.code,{children:"extensions/druid-thrift-extensions"})," directory."]}),"\n",(0,i.jsx)(t.h2,{id:"thrift-parser",children:"Thrift Parser"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Field"}),(0,i.jsx)(t.th,{children:"Type"}),(0,i.jsx)(t.th,{children:"Description"}),(0,i.jsx)(t.th,{children:"Required"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"type"}),(0,i.jsx)(t.td,{children:"String"}),(0,i.jsxs)(t.td,{children:["This should say ",(0,i.jsx)(t.code,{children:"thrift"})]}),(0,i.jsx)(t.td,{children:"yes"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"parseSpec"}),(0,i.jsx)(t.td,{children:"JSON Object"}),(0,i.jsx)(t.td,{children:"Specifies the timestamp and dimensions of the data. Should be a JSON parseSpec."}),(0,i.jsx)(t.td,{children:"yes"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"thriftJar"}),(0,i.jsx)(t.td,{children:"String"}),(0,i.jsxs)(t.td,{children:["path of thrift jar, if not provided, it will try to find the thrift class in classpath. Thrift jar in batch ingestion should be uploaded to HDFS first and configure ",(0,i.jsx)(t.code,{children:"jobProperties"})," with ",(0,i.jsx)(t.code,{children:'"tmpjars":"/path/to/your/thrift.jar"'})]}),(0,i.jsx)(t.td,{children:"no"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"thriftClass"}),(0,i.jsx)(t.td,{children:"String"}),(0,i.jsx)(t.td,{children:"classname of thrift"}),(0,i.jsx)(t.td,{children:"yes"})]})]})]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:["Batch Ingestion example - ",(0,i.jsx)(t.code,{children:"inputFormat"})," and ",(0,i.jsx)(t.code,{children:"tmpjars"})," should be set."]}),"\n"]}),"\n",(0,i.jsxs)(t.p,{children:["This is for batch ingestion using the HadoopDruidIndexer. The inputFormat of inputSpec in ioConfig could be one of ",(0,i.jsx)(t.code,{children:'"org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat"'})," and ",(0,i.jsx)(t.code,{children:"com.twitter.elephantbird.mapreduce.input.LzoThriftBlockInputFormat"}),". Be careful, when ",(0,i.jsx)(t.code,{children:"LzoThriftBlockInputFormat"})," is used, thrift class must be provided twice."]}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-json",children:'{\n  "type": "index_hadoop",\n  "spec": {\n    "dataSchema": {\n      "dataSource": "book",\n      "parser": {\n        "type": "thrift",\n        "jarPath": "book.jar",\n        "thriftClass": "org.apache.druid.data.input.thrift.Book",\n        "protocol": "compact",\n        "parseSpec": {\n          "format": "json",\n          ...\n        }\n      },\n      "metricsSpec": [],\n      "granularitySpec": {}\n    },\n    "ioConfig": {\n      "type": "hadoop",\n      "inputSpec": {\n        "type": "static",\n        "inputFormat": "org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat",\n        // "inputFormat": "com.twitter.elephantbird.mapreduce.input.LzoThriftBlockInputFormat",\n        "paths": "/user/to/some/book.seq"\n      }\n    },\n    "tuningConfig": {\n      "type": "hadoop",\n      "jobProperties": {\n        "tmpjars":"/user/h_user_profile/du00/druid/test/book.jar",\n        // "elephantbird.class.for.MultiInputFormat" : "${YOUR_THRIFT_CLASS_NAME}"\n      }\n    }\n  }\n}\n'})})]})}function l(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},28453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>d});var r=n(96540);const i={},o=r.createContext(i);function s(e){const t=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function d(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(o.Provider,{value:t},e.children)}}}]);