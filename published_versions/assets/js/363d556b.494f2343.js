"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[2747],{28453:(e,r,s)=>{s.d(r,{R:()=>o,x:()=>d});var t=s(96540);const n={},i=t.createContext(n);function o(e){const r=t.useContext(i);return t.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function d(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),t.createElement(i.Provider,{value:r},e.children)}},88405:(e,r,s)=>{s.r(r),s.d(r,{assets:()=>a,contentTitle:()=>d,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"multi-stage-query/reference","title":"SQL-based ingestion reference","description":"\x3c!--","source":"@site/docs/33.0.0/multi-stage-query/reference.md","sourceDirName":"multi-stage-query","slug":"/multi-stage-query/reference","permalink":"/docs/33.0.0/multi-stage-query/reference","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"reference","title":"SQL-based ingestion reference","sidebar_label":"Reference"},"sidebar":"docs","previous":{"title":"Examples","permalink":"/docs/33.0.0/multi-stage-query/examples"},"next":{"title":"Known issues","permalink":"/docs/33.0.0/multi-stage-query/known-issues"}}');var n=s(74848),i=s(28453);const o={id:"reference",title:"SQL-based ingestion reference",sidebar_label:"Reference"},d=void 0,a={},l=[{value:"SQL reference",id:"sql-reference",level:2},{value:"<code>EXTERN</code> Function",id:"extern-function",level:3},{value:"<code>EXTERN</code> as an input source",id:"extern-as-an-input-source",level:4},{value:"<code>EXTERN</code> to export to a destination",id:"extern-to-export-to-a-destination",level:4},{value:"S3 - Amazon S3",id:"s3---amazon-s3",level:5},{value:"GOOGLE - Google Cloud Storage",id:"google---google-cloud-storage",level:5},{value:"LOCAL - local file storage",id:"local---local-file-storage",level:5},{value:"<code>INSERT</code>",id:"insert",level:3},{value:"<code>REPLACE</code>",id:"replace",level:3},{value:"<code>REPLACE</code> all data",id:"replace-all-data",level:4},{value:"<code>REPLACE</code> specific time ranges",id:"replace-specific-time-ranges",level:4},{value:"<code>PARTITIONED BY</code>",id:"partitioned-by",level:3},{value:"<code>CLUSTERED BY</code>",id:"clustered-by",level:3},{value:"Context parameters",id:"context-parameters",level:2},{value:"Joins",id:"joins",level:2},{value:"Broadcast",id:"broadcast",level:3},{value:"Sort-merge",id:"sort-merge",level:3},{value:"Durable storage",id:"durable-storage",level:2},{value:"Durable storage configurations",id:"durable-storage-configurations",level:3},{value:"Durable storage cleaner configurations",id:"durable-storage-cleaner-configurations",level:3},{value:"Limits",id:"limits",level:2},{value:"Error codes",id:"error-codes",level:2}];function c(e){const r={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(r.admonition,{type:"info",children:(0,n.jsxs)(r.p,{children:["This page describes SQL-based batch ingestion using the ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/",children:(0,n.jsx)(r.code,{children:"druid-multi-stage-query"})}),"\nextension, new in Druid 24.0. Refer to the ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/ingestion/#batch",children:"ingestion methods"})," table to determine which\ningestion method is right for you."]})}),"\n",(0,n.jsx)(r.h2,{id:"sql-reference",children:"SQL reference"}),"\n",(0,n.jsxs)(r.p,{children:["This topic is a reference guide for the multi-stage query architecture in Apache Druid. For examples of real-world\nusage, refer to the ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/examples",children:"Examples"})," page."]}),"\n",(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.code,{children:"INSERT"})," and ",(0,n.jsx)(r.code,{children:"REPLACE"})," load data into a Druid datasource from either an external input source, or from another\ndatasource. When loading from an external datasource, you typically must provide the kind of input source,\nthe data format, and the schema (signature) of the input file. Druid provides ",(0,n.jsx)(r.em,{children:"table functions"})," to allow you to\nspecify the external file. There are two kinds. ",(0,n.jsx)(r.code,{children:"EXTERN"})," works with the JSON-serialized specs for the three\nitems, using the same JSON you would use in native ingest. A set of other, input-source-specific functions\nuse SQL syntax to specify the format and the input schema. There is one function for each input source. The\ninput-source-specific functions allow you to use SQL query parameters to specify the set of files (or URIs),\nmaking it easy to reuse the same SQL statement for each ingest: just specify the set of files to use each time."]}),"\n",(0,n.jsxs)(r.h3,{id:"extern-function",children:[(0,n.jsx)(r.code,{children:"EXTERN"})," Function"]}),"\n",(0,n.jsxs)(r.p,{children:["Use the ",(0,n.jsx)(r.code,{children:"EXTERN"})," function to read external data or write to an external location."]}),"\n",(0,n.jsxs)(r.h4,{id:"extern-as-an-input-source",children:[(0,n.jsx)(r.code,{children:"EXTERN"})," as an input source"]}),"\n",(0,n.jsx)(r.p,{children:"The function has two variations.\nFunction variation 1, with the input schema expressed as JSON:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-sql",children:"SELECT\n <column>\nFROM TABLE(\n  EXTERN(\n    '<Druid input source>',\n    '<Druid input format>',\n    '<row signature>'\n  )\n)\n"})}),"\n",(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.code,{children:"EXTERN"})," consists of the following parts:"]}),"\n",(0,n.jsxs)(r.ol,{children:["\n",(0,n.jsxs)(r.li,{children:["Any ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/ingestion/input-sources",children:"Druid input source"})," as a JSON-encoded string."]}),"\n",(0,n.jsxs)(r.li,{children:["Any ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/ingestion/data-formats",children:"Druid input format"})," as a JSON-encoded string."]}),"\n",(0,n.jsxs)(r.li,{children:["A row signature, as a JSON-encoded array of column descriptors. Each column descriptor must have a\n",(0,n.jsx)(r.code,{children:"name"})," and a ",(0,n.jsx)(r.code,{children:"type"}),". The type can be ",(0,n.jsx)(r.code,{children:"string"}),", ",(0,n.jsx)(r.code,{children:"long"}),", ",(0,n.jsx)(r.code,{children:"double"}),", or ",(0,n.jsx)(r.code,{children:"float"}),". This row signature is\nused to map the external data into the SQL layer."]}),"\n"]}),"\n",(0,n.jsxs)(r.p,{children:["Variation 2, with the input schema expressed in SQL using an ",(0,n.jsx)(r.code,{children:"EXTEND"})," clause. See the next\nsection for more detail on ",(0,n.jsx)(r.code,{children:"EXTEND"}),". This format also uses named arguments to make the SQL easier to read:"]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-sql",children:"SELECT\n <column>\nFROM TABLE(\n  EXTERN(\n    inputSource => '<Druid input source>',\n    inputFormat => '<Druid input format>'\n  )) (<columns>)\n\n"})}),"\n",(0,n.jsxs)(r.p,{children:["The input source and format are as above. The columns are expressed as in a SQL ",(0,n.jsx)(r.code,{children:"CREATE TABLE"}),".\nExample: ",(0,n.jsx)(r.code,{children:"(timestamp VARCHAR, metricType VARCHAR, value BIGINT)"}),". The optional ",(0,n.jsx)(r.code,{children:"EXTEND"})," keyword\ncan precede the column list: ",(0,n.jsx)(r.code,{children:"EXTEND (timestamp VARCHAR...)"}),"."]}),"\n",(0,n.jsxs)(r.p,{children:["For more information, see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/concepts#read-external-data-with-extern",children:"Read external data with EXTERN"}),"."]}),"\n",(0,n.jsxs)(r.h4,{id:"extern-to-export-to-a-destination",children:[(0,n.jsx)(r.code,{children:"EXTERN"})," to export to a destination"]}),"\n",(0,n.jsxs)(r.p,{children:["You can use ",(0,n.jsx)(r.code,{children:"EXTERN"})," to specify a destination to export data.\nThis variation of ",(0,n.jsx)(r.code,{children:"EXTERN"})," accepts the details of the destination as the only argument and requires an ",(0,n.jsx)(r.code,{children:"AS"})," clause to specify the format of the exported rows."]}),"\n",(0,n.jsxs)(r.p,{children:["When you export data, Druid creates metadata files in a subdirectory named ",(0,n.jsx)(r.code,{children:"_symlink_format_manifest"}),".\nWithin the ",(0,n.jsx)(r.code,{children:"_symlink_format_manifest/manifest"})," directory, the ",(0,n.jsx)(r.code,{children:"manifest"})," file lists absolute paths to exported files using the symlink manifest format. For example:"]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-text",children:"s3://export-bucket/export/query-6564a32f-2194-423a-912e-eead470a37c4-worker2-partition2.csv\ns3://export-bucket/export/query-6564a32f-2194-423a-912e-eead470a37c4-worker1-partition1.csv\ns3://export-bucket/export/query-6564a32f-2194-423a-912e-eead470a37c4-worker0-partition0.csv\n...\ns3://export-bucket/export/query-6564a32f-2194-423a-912e-eead470a37c4-worker0-partition24.csv\n"})}),"\n",(0,n.jsx)(r.p,{children:"Keep the following in mind when using EXTERN to export rows:"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsx)(r.li,{children:"Only INSERT statements are supported."}),"\n",(0,n.jsxs)(r.li,{children:["Only ",(0,n.jsx)(r.code,{children:"CSV"})," format is supported as an export format."]}),"\n",(0,n.jsxs)(r.li,{children:["Partitioning (",(0,n.jsx)(r.code,{children:"PARTITIONED BY"}),") and clustering (",(0,n.jsx)(r.code,{children:"CLUSTERED BY"}),") aren't supported with EXTERN statements."]}),"\n",(0,n.jsx)(r.li,{children:"You can export to Amazon S3, Google GCS, or local storage."}),"\n",(0,n.jsx)(r.li,{children:"The destination provided should contain no other files or directories."}),"\n"]}),"\n",(0,n.jsxs)(r.p,{children:["When you export data, use the ",(0,n.jsx)(r.code,{children:"rowsPerPage"})," context parameter to restrict the size of exported files.\nWhen the number of rows in the result set exceeds the value of the parameter, Druid splits the output into multiple files."]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-sql",children:"INSERT INTO\n  EXTERN(<destination function>)\nAS CSV\nSELECT\n  <column>\nFROM <table>\n"})}),"\n",(0,n.jsx)(r.h5,{id:"s3---amazon-s3",children:"S3 - Amazon S3"}),"\n",(0,n.jsxs)(r.p,{children:["To export results to S3, pass the ",(0,n.jsx)(r.code,{children:"s3()"})," function as an argument to the ",(0,n.jsx)(r.code,{children:"EXTERN"})," function.\nExport to S3 requires the ",(0,n.jsx)(r.code,{children:"druid-s3-extensions"})," extension.\nFor a list of S3 permissions the MSQ task engine requires to perform export, see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/security#s3",children:"Permissions for durable storage"}),"."]}),"\n",(0,n.jsxs)(r.p,{children:["The ",(0,n.jsx)(r.code,{children:"s3()"})," function configures the connection to AWS.\nPass all arguments for ",(0,n.jsx)(r.code,{children:"s3()"})," as named parameters with their values enclosed in single quotes. For example:"]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-sql",children:"INSERT INTO\n  EXTERN(\n    s3(bucket => 'your_bucket', prefix => 'prefix/to/files')\n  )\nAS CSV\nSELECT\n  <column>\nFROM <table>\n"})}),"\n",(0,n.jsx)(r.p,{children:"Supported arguments for the function:"}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{children:"Parameter"}),(0,n.jsx)(r.th,{children:"Required"}),(0,n.jsx)(r.th,{children:"Description"}),(0,n.jsx)(r.th,{children:"Default"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"bucket"})}),(0,n.jsx)(r.td,{children:"Yes"}),(0,n.jsxs)(r.td,{children:["S3 bucket destination for exported files. You must add the bucket and prefix combination to the ",(0,n.jsx)(r.code,{children:"druid.export.storage.s3.allowedExportPaths"})," allow list."]}),(0,n.jsx)(r.td,{children:"n/a"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"prefix"})}),(0,n.jsx)(r.td,{children:"Yes"}),(0,n.jsxs)(r.td,{children:["Destination path in the bucket to create exported files. The export query expects the destination path to be empty. If the location includes other files, the query will fail. You must add the bucket and prefix combination to the ",(0,n.jsx)(r.code,{children:"druid.export.storage.s3.allowedExportPaths"})," allow list."]}),(0,n.jsx)(r.td,{children:"n/a"})]})]})]}),"\n",(0,n.jsx)(r.p,{children:"Configure the following runtime parameters to export to an S3 destination:"}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{children:"Runtime parameter"}),(0,n.jsx)(r.th,{children:"Required"}),(0,n.jsx)(r.th,{children:"Description"}),(0,n.jsx)(r.th,{children:"Default"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.export.storage.s3.allowedExportPaths"})}),(0,n.jsx)(r.td,{children:"Yes"}),(0,n.jsxs)(r.td,{children:["Array of S3 prefixes allowed as export destinations. Export queries fail if the export destination does not match any of the configured prefixes. For example: ",(0,n.jsx)(r.code,{children:'[\\"s3://bucket1/export/\\", \\"s3://bucket2/export/\\"]'})]}),(0,n.jsx)(r.td,{children:"n/a"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.export.storage.s3.tempLocalDir"})}),(0,n.jsx)(r.td,{children:"No"}),(0,n.jsx)(r.td,{children:"Directory for local storage where the worker stores temporary files before uploading the data to S3."}),(0,n.jsx)(r.td,{children:"n/a"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.export.storage.s3.maxRetry"})}),(0,n.jsx)(r.td,{children:"No"}),(0,n.jsx)(r.td,{children:"Maximum number of  attempts for S3 API calls to avoid failures due to transient errors."}),(0,n.jsx)(r.td,{children:"10"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.export.storage.s3.chunkSize"})}),(0,n.jsx)(r.td,{children:"No"}),(0,n.jsxs)(r.td,{children:["Individual chunk size to store temporarily in ",(0,n.jsx)(r.code,{children:"tempDir"}),". Large chunk sizes reduce the number of API calls to S3, but require more disk space to store temporary chunks."]}),(0,n.jsx)(r.td,{children:"100MiB"})]})]})]}),"\n",(0,n.jsx)(r.h5,{id:"google---google-cloud-storage",children:"GOOGLE - Google Cloud Storage"}),"\n",(0,n.jsxs)(r.p,{children:["To export query results to Google Cloud Storage (GCS), pass the ",(0,n.jsx)(r.code,{children:"google()"})," function as an argument to the ",(0,n.jsx)(r.code,{children:"EXTERN"})," function.\nExport to GCS requires the ",(0,n.jsx)(r.code,{children:"druid-google-extensions"})," extension."]}),"\n",(0,n.jsxs)(r.p,{children:["The ",(0,n.jsx)(r.code,{children:"google()"})," function configures the connection to GCS. Pass the arguments for ",(0,n.jsx)(r.code,{children:"google()"})," as named parameters with their values enclosed in single quotes. For example:"]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-sql",children:"INSERT INTO\n  EXTERN(\n    google(bucket => 'your_bucket', prefix => 'prefix/to/files')\n  )\nAS CSV\nSELECT\n  <column>\nFROM <table>\n"})}),"\n",(0,n.jsx)(r.p,{children:"Supported arguments for the function:"}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{children:"Parameter"}),(0,n.jsx)(r.th,{children:"Required"}),(0,n.jsx)(r.th,{children:"Description"}),(0,n.jsx)(r.th,{children:"Default"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"bucket"})}),(0,n.jsx)(r.td,{children:"Yes"}),(0,n.jsxs)(r.td,{children:["GCS bucket destination for exported files. You must add the bucket and prefix combination to the ",(0,n.jsx)(r.code,{children:"druid.export.storage.google.allowedExportPaths"})," allow list."]}),(0,n.jsx)(r.td,{children:"n/a"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"prefix"})}),(0,n.jsx)(r.td,{children:"Yes"}),(0,n.jsxs)(r.td,{children:["Destination path in the bucket to create exported files. The export query expects the destination path to be empty. If the location includes other files, the query will fail. You must add the bucket and prefix combination to the ",(0,n.jsx)(r.code,{children:"druid.export.storage.google.allowedExportPaths"})," allow list."]}),(0,n.jsx)(r.td,{children:"n/a"})]})]})]}),"\n",(0,n.jsx)(r.p,{children:"Configure the following runtime parameters to export query results to a GCS destination:"}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{children:"Runtime parameter"}),(0,n.jsx)(r.th,{children:"Required"}),(0,n.jsx)(r.th,{children:"Description"}),(0,n.jsx)(r.th,{children:"Default"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.export.storage.google.allowedExportPaths"})}),(0,n.jsx)(r.td,{children:"Yes"}),(0,n.jsxs)(r.td,{children:["Array of GCS prefixes allowed as export destinations. Export queries fail if the export destination does not match any of the configured prefixes. For example: ",(0,n.jsx)(r.code,{children:'[\\"gs://bucket1/export/\\", \\"gs://bucket2/export/\\"]'})]}),(0,n.jsx)(r.td,{children:"n/a"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.export.storage.google.tempLocalDir"})}),(0,n.jsx)(r.td,{children:"No"}),(0,n.jsx)(r.td,{children:"Directory for local storage where the worker stores temporary files before uploading the data to GCS."}),(0,n.jsx)(r.td,{children:"n/a"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.export.storage.google.maxRetry"})}),(0,n.jsx)(r.td,{children:"No"}),(0,n.jsx)(r.td,{children:"Maximum number of attempts for GCS API calls to avoid failures due to transient errors."}),(0,n.jsx)(r.td,{children:"10"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.export.storage.google.chunkSize"})}),(0,n.jsx)(r.td,{children:"No"}),(0,n.jsxs)(r.td,{children:["Individual chunk size to store temporarily in ",(0,n.jsx)(r.code,{children:"tempDir"}),". Large chunk sizes reduce the number of API calls to GS, but require more disk space to store temporary chunks."]}),(0,n.jsx)(r.td,{children:"4 MiB"})]})]})]}),"\n",(0,n.jsx)(r.h5,{id:"local---local-file-storage",children:"LOCAL - local file storage"}),"\n",(0,n.jsx)(r.p,{children:"You can export queries to local storage. This process writes the results to the filesystem on the MSQ worker.\nThis is useful in a single node setup or for testing but is not suitable for production use cases."}),"\n",(0,n.jsxs)(r.p,{children:["To export results to local storage, pass the ",(0,n.jsx)(r.code,{children:"LOCAL()"})," function as an argument to the EXTERN function.\nYou must configure the runtime property ",(0,n.jsx)(r.code,{children:"druid.export.storage.baseDir"})," as an absolute path on the Indexer or Middle Manager to use local storage as an export destination.\nYou can export data to paths that match this value as a prefix.\nPass all arguments to ",(0,n.jsx)(r.code,{children:"LOCAL()"})," as named parameters with values enclosed in single quotes. For example:"]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-sql",children:"INSERT INTO\n  EXTERN(\n    local(exportPath => 'exportLocation/query1')\n  )\nAS CSV\nSELECT\n  <column>\nFROM <table>\n"})}),"\n",(0,n.jsx)(r.p,{children:"Supported arguments for the function:"}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{children:"Parameter"}),(0,n.jsx)(r.th,{children:"Required"}),(0,n.jsx)(r.th,{children:"Description"}),(0,n.jsx)(r.th,{children:"Default"})]})}),(0,n.jsx)(r.tbody,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"exportPath"})}),(0,n.jsx)(r.td,{children:"Yes"}),(0,n.jsxs)(r.td,{children:["Absolute path to a subdirectory of ",(0,n.jsx)(r.code,{children:"druid.export.storage.baseDir"})," where Druid exports the query results. The destination must be empty. If the location includes other files or directories, the query will fail."]}),(0,n.jsx)(r.td,{children:"n/a"})]})})]}),"\n",(0,n.jsxs)(r.p,{children:["For more information, see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/concepts#write-to-an-external-destination-with-extern",children:"Export external data with EXTERN"}),"."]}),"\n",(0,n.jsx)(r.h3,{id:"insert",children:(0,n.jsx)(r.code,{children:"INSERT"})}),"\n",(0,n.jsxs)(r.p,{children:["Use the ",(0,n.jsx)(r.code,{children:"INSERT"})," statement to insert data."]}),"\n",(0,n.jsxs)(r.p,{children:["Unlike standard SQL, ",(0,n.jsx)(r.code,{children:"INSERT"})," loads data into the target table according to column name, not positionally. If necessary,\nuse ",(0,n.jsx)(r.code,{children:"AS"})," in your ",(0,n.jsx)(r.code,{children:"SELECT"})," column list to assign the correct names. Do not rely on their positions within the SELECT\nclause."]}),"\n",(0,n.jsx)(r.p,{children:"Statement format:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-sql",children:"INSERT INTO <table name>\n< SELECT query >\nPARTITIONED BY <time frame>\n[ CLUSTERED BY <column list> ]\n"})}),"\n",(0,n.jsx)(r.p,{children:"INSERT consists of the following parts:"}),"\n",(0,n.jsxs)(r.ol,{children:["\n",(0,n.jsxs)(r.li,{children:["Optional ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/reference#context-parameters",children:"context parameters"}),"."]}),"\n",(0,n.jsxs)(r.li,{children:["An ",(0,n.jsx)(r.code,{children:"INSERT INTO <dataSource>"})," clause at the start of your query, such as ",(0,n.jsx)(r.code,{children:"INSERT INTO your-table"}),"."]}),"\n",(0,n.jsxs)(r.li,{children:["A clause for the data you want to insert, such as ",(0,n.jsx)(r.code,{children:"SELECT ... FROM ..."}),". You can use ",(0,n.jsx)(r.a,{href:"#extern-function",children:(0,n.jsx)(r.code,{children:"EXTERN"})}),"\nto reference external tables using ",(0,n.jsx)(r.code,{children:"FROM TABLE(EXTERN(...))"}),"."]}),"\n",(0,n.jsxs)(r.li,{children:["A ",(0,n.jsx)(r.a,{href:"#partitioned-by",children:"PARTITIONED BY"})," clause, such as ",(0,n.jsx)(r.code,{children:"PARTITIONED BY DAY"}),"."]}),"\n",(0,n.jsxs)(r.li,{children:["An optional ",(0,n.jsx)(r.a,{href:"#clustered-by",children:"CLUSTERED BY"})," clause."]}),"\n"]}),"\n",(0,n.jsxs)(r.p,{children:["For more information, see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/concepts#load-data-with-insert",children:"Load data with INSERT"}),"."]}),"\n",(0,n.jsx)(r.h3,{id:"replace",children:(0,n.jsx)(r.code,{children:"REPLACE"})}),"\n",(0,n.jsxs)(r.p,{children:["You can use the ",(0,n.jsx)(r.code,{children:"REPLACE"})," function to replace all or some of the data."]}),"\n",(0,n.jsxs)(r.p,{children:["Unlike standard SQL, ",(0,n.jsx)(r.code,{children:"REPLACE"})," loads data into the target table according to column name, not positionally. If necessary,\nuse ",(0,n.jsx)(r.code,{children:"AS"})," in your ",(0,n.jsx)(r.code,{children:"SELECT"})," column list to assign the correct names. Do not rely on their positions within the SELECT\nclause."]}),"\n",(0,n.jsxs)(r.h4,{id:"replace-all-data",children:[(0,n.jsx)(r.code,{children:"REPLACE"})," all data"]}),"\n",(0,n.jsx)(r.p,{children:"Function format to replace all data:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-sql",children:"REPLACE INTO <target table>\nOVERWRITE ALL\n< SELECT query >\nPARTITIONED BY <time granularity>\n[ CLUSTERED BY <column list> ]\n"})}),"\n",(0,n.jsxs)(r.h4,{id:"replace-specific-time-ranges",children:[(0,n.jsx)(r.code,{children:"REPLACE"})," specific time ranges"]}),"\n",(0,n.jsx)(r.p,{children:"Function format to replace specific time ranges:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-sql",children:"REPLACE INTO <target table>\nOVERWRITE WHERE __time >= TIMESTAMP '<lower bound>' AND __time < TIMESTAMP '<upper bound>'\n< SELECT query >\nPARTITIONED BY <time granularity>\n[ CLUSTERED BY <column list> ]\n"})}),"\n",(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.code,{children:"REPLACE"})," consists of the following parts:"]}),"\n",(0,n.jsxs)(r.ol,{children:["\n",(0,n.jsxs)(r.li,{children:["Optional ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/reference#context-parameters",children:"context parameters"}),"."]}),"\n",(0,n.jsxs)(r.li,{children:["A ",(0,n.jsx)(r.code,{children:"REPLACE INTO <dataSource>"})," clause at the start of your query, such as ",(0,n.jsx)(r.code,{children:'REPLACE INTO "your-table".'})]}),"\n",(0,n.jsxs)(r.li,{children:["An OVERWRITE clause after the datasource, either OVERWRITE ALL or OVERWRITE WHERE:","\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsx)(r.li,{children:"OVERWRITE ALL replaces the entire existing datasource with the results of the query."}),"\n",(0,n.jsxs)(r.li,{children:["OVERWRITE WHERE drops the time segments that match the condition you set. Conditions are based on the ",(0,n.jsx)(r.code,{children:"__time"}),"\ncolumn and use the format ",(0,n.jsx)(r.code,{children:"__time [< > = <= >=] TIMESTAMP"}),". Use them with AND, OR, and NOT between them, inclusive\nof the timestamps specified. No other expressions or functions are valid in OVERWRITE."]}),"\n"]}),"\n"]}),"\n",(0,n.jsx)(r.li,{children:"A clause for the actual data you want to use for the replacement."}),"\n",(0,n.jsxs)(r.li,{children:["A ",(0,n.jsx)(r.a,{href:"#partitioned-by",children:"PARTITIONED BY"})," clause, such as ",(0,n.jsx)(r.code,{children:"PARTITIONED BY DAY"}),"."]}),"\n",(0,n.jsxs)(r.li,{children:["An optional ",(0,n.jsx)(r.a,{href:"#clustered-by",children:"CLUSTERED BY"})," clause."]}),"\n"]}),"\n",(0,n.jsxs)(r.p,{children:["For more information, see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/concepts#replace",children:"Overwrite data with REPLACE"}),"."]}),"\n",(0,n.jsx)(r.h3,{id:"partitioned-by",children:(0,n.jsx)(r.code,{children:"PARTITIONED BY"})}),"\n",(0,n.jsxs)(r.p,{children:["The ",(0,n.jsx)(r.code,{children:"PARTITIONED BY <time granularity>"})," clause is required for ",(0,n.jsx)(r.a,{href:"#insert",children:"INSERT"})," and ",(0,n.jsx)(r.a,{href:"#replace",children:"REPLACE"}),". See\n",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/concepts#partitioning-by-time",children:"Partitioning"})," for details."]}),"\n",(0,n.jsx)(r.p,{children:"The following granularity arguments are accepted:"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:["Time unit keywords: ",(0,n.jsx)(r.code,{children:"HOUR"}),", ",(0,n.jsx)(r.code,{children:"DAY"}),", ",(0,n.jsx)(r.code,{children:"MONTH"}),", or ",(0,n.jsx)(r.code,{children:"YEAR"}),". Equivalent to ",(0,n.jsx)(r.code,{children:"FLOOR(__time TO TimeUnit)"}),"."]}),"\n",(0,n.jsxs)(r.li,{children:["Time units as ISO 8601 period strings: ",(0,n.jsx)(r.code,{children:"'PT1H'"}),", ",(0,n.jsx)(r.code,{children:"'P1D'"}),", etc. (Druid 26.0 and later.)"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"TIME_FLOOR(__time, 'granularity_string')"}),", where granularity_string is one of the ISO 8601 periods listed below. The\nfirst argument must be ",(0,n.jsx)(r.code,{children:"__time"}),"."]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"FLOOR(__time TO TimeUnit)"}),", where ",(0,n.jsx)(r.code,{children:"TimeUnit"})," is any unit supported by the ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/sql-scalar#date-and-time-functions",children:"FLOOR function"}),". The first argument must be ",(0,n.jsx)(r.code,{children:"__time"}),"."]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"ALL"})," or ",(0,n.jsx)(r.code,{children:"ALL TIME"}),", which effectively disables time partitioning by placing all data in a single time chunk. To use\nLIMIT or OFFSET at the outer level of your ",(0,n.jsx)(r.code,{children:"INSERT"})," or ",(0,n.jsx)(r.code,{children:"REPLACE"})," query, you must set ",(0,n.jsx)(r.code,{children:"PARTITIONED BY"})," to ",(0,n.jsx)(r.code,{children:"ALL"})," or ",(0,n.jsx)(r.code,{children:"ALL TIME"}),"."]}),"\n"]}),"\n",(0,n.jsxs)(r.p,{children:["Earlier versions required the ",(0,n.jsx)(r.code,{children:"TIME_FLOOR"})," notation to specify a granularity other than the keywords.\nIn the current version, the string constant provides a simpler equivalent solution."]}),"\n",(0,n.jsxs)(r.p,{children:["The following ISO 8601 periods are supported for ",(0,n.jsx)(r.code,{children:"TIME_FLOOR"})," and the string constant:"]}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsx)(r.li,{children:"PT1S"}),"\n",(0,n.jsx)(r.li,{children:"PT1M"}),"\n",(0,n.jsx)(r.li,{children:"PT5M"}),"\n",(0,n.jsx)(r.li,{children:"PT10M"}),"\n",(0,n.jsx)(r.li,{children:"PT15M"}),"\n",(0,n.jsx)(r.li,{children:"PT30M"}),"\n",(0,n.jsx)(r.li,{children:"PT1H"}),"\n",(0,n.jsx)(r.li,{children:"PT6H"}),"\n",(0,n.jsx)(r.li,{children:"P1D"}),"\n",(0,n.jsx)(r.li,{children:"P1W*"}),"\n",(0,n.jsx)(r.li,{children:"P1M"}),"\n",(0,n.jsx)(r.li,{children:"P3M"}),"\n",(0,n.jsx)(r.li,{children:"P1Y"}),"\n"]}),"\n",(0,n.jsx)(r.p,{children:"The string constant can also include any of the keywords mentioned above:"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"HOUR"})," - Same as ",(0,n.jsx)(r.code,{children:"'PT1H'"})]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"DAY"})," - Same as ",(0,n.jsx)(r.code,{children:"'P1D'"})]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"MONTH"})," - Same as ",(0,n.jsx)(r.code,{children:"'P1M'"})]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"YEAR"})," - Same as ",(0,n.jsx)(r.code,{children:"'P1Y'"})]}),"\n",(0,n.jsx)(r.li,{children:(0,n.jsx)(r.code,{children:"ALL TIME"})}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"ALL"})," - Alias for ",(0,n.jsx)(r.code,{children:"ALL TIME"})]}),"\n"]}),"\n",(0,n.jsx)(r.p,{children:"Examples:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-SQL",children:"-- Keyword\nPARTITIONED BY HOUR\n\n-- String literal\nPARTITIONED BY 'HOUR'\n\n-- ISO 8601 period\nPARTITIONED BY 'PT1H'\n\n-- TIME_FLOOR function\nPARTITIONED BY TIME_FLOOR(__time, 'PT1H')\n"})}),"\n",(0,n.jsxs)(r.p,{children:["For more information about partitioning, see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/concepts#partitioning-by-time",children:"Partitioning"}),". ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"\n*Avoid  partitioning by week, ",(0,n.jsx)(r.code,{children:"P1W"}),", because weeks don't align neatly with months and years, making it difficult to partition by coarser granularities later."]}),"\n",(0,n.jsx)(r.h3,{id:"clustered-by",children:(0,n.jsx)(r.code,{children:"CLUSTERED BY"})}),"\n",(0,n.jsxs)(r.p,{children:["The ",(0,n.jsx)(r.code,{children:"CLUSTERED BY <column list>"})," clause is optional for ",(0,n.jsx)(r.a,{href:"#insert",children:"INSERT"})," and ",(0,n.jsx)(r.a,{href:"#replace",children:"REPLACE"}),". It accepts a list of\ncolumn names or expressions."]}),"\n",(0,n.jsxs)(r.p,{children:["This column list is used for ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/ingestion/partitioning#secondary-partitioning",children:"secondary partitioning"})," of segments\nwithin a time chunk, and ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/ingestion/partitioning#sorting",children:"sorting"})," of rows within a segment. For sorting purposes,\nDruid implicitly prepends ",(0,n.jsx)(r.code,{children:"__time"})," to the ",(0,n.jsx)(r.code,{children:"CLUSTERED BY"})," column list, unless\n",(0,n.jsx)(r.a,{href:"#context",children:(0,n.jsx)(r.code,{children:"forceSegmentSortByTime"})})," is set to ",(0,n.jsx)(r.code,{children:"false"}),"\n(an experimental feature; see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/ingestion/partitioning#sorting",children:"Sorting"})," for details)."]}),"\n",(0,n.jsxs)(r.p,{children:["For more information about clustering, see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/concepts#clustering",children:"Clustering"}),"."]}),"\n",(0,n.jsx)("a",{name:"context"}),"\n",(0,n.jsx)(r.h2,{id:"context-parameters",children:"Context parameters"}),"\n",(0,n.jsxs)(r.p,{children:["In addition to the Druid SQL ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/sql-query-context",children:"context parameters"}),", the multi-stage query task engine accepts certain context parameters that are specific to it."]}),"\n",(0,n.jsx)(r.p,{children:"Use context parameters alongside your queries to customize the behavior of the query. If you're using the API, include the context parameters in the query context when you submit a query:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-json",children:'{\n  "query": "SELECT 1 + 1",\n  "context": {\n    "<key>": "<value>",\n    "maxNumTasks": 3\n  }\n}\n'})}),"\n",(0,n.jsx)(r.p,{children:"If you're using the web console, you can specify the context parameters through various UI options."}),"\n",(0,n.jsx)(r.p,{children:"The following table lists the context parameters for the MSQ task engine:"}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{children:"Parameter"}),(0,n.jsx)(r.th,{children:"Description"}),(0,n.jsx)(r.th,{children:"Default value"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"maxNumTasks"})}),(0,n.jsxs)(r.td,{children:["SELECT, INSERT, REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"The maximum total number of tasks to launch, including the controller task. The lowest possible value for this setting is 2: one controller and one worker. All tasks must be able to launch simultaneously. If they cannot, the query returns a ",(0,n.jsx)(r.code,{children:"TaskStartTimeout"})," error code after approximately 10 minutes.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"May also be provided as ",(0,n.jsx)(r.code,{children:"numTasks"}),". If both are present, ",(0,n.jsx)(r.code,{children:"maxNumTasks"})," takes priority."]}),(0,n.jsx)(r.td,{children:"2"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"taskAssignment"})}),(0,n.jsxs)(r.td,{children:["SELECT, INSERT, REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"Determines how many tasks to use. Possible values include: ",(0,n.jsxs)("ul",{children:[(0,n.jsxs)("li",{children:[(0,n.jsx)(r.code,{children:"max"}),": Uses as many tasks as possible, up to ",(0,n.jsx)(r.code,{children:"maxNumTasks"}),"."]}),(0,n.jsxs)("li",{children:[(0,n.jsx)(r.code,{children:"auto"}),": When file sizes can be determined through directory listing (for example: local files, S3, GCS, HDFS) uses as few tasks as possible without exceeding 512 MiB or 10,000 files per task, unless exceeding these limits is necessary to stay within ",(0,n.jsx)(r.code,{children:"maxNumTasks"}),". When calculating the size of files, the weighted size is used, which considers the file format and compression format used if any. When file sizes cannot be determined through directory listing (for example: http), behaves the same as ",(0,n.jsx)(r.code,{children:"max"}),"."]})]})]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"max"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"finalizeAggregations"})}),(0,n.jsxs)(r.td,{children:["SELECT, INSERT, REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"Determines the type of aggregation to return. If true, Druid finalizes the results of complex aggregations that directly appear in query results. If false, Druid returns the aggregation's intermediate type rather than finalized type. This parameter is useful during ingestion, where it enables storing sketches directly in Druid tables. For more information about aggregations, see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/sql-aggregations",children:"SQL aggregation functions"}),"."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"true"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"arrayIngestMode"})}),(0,n.jsxs)(r.td,{children:["INSERT, REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," Controls how ARRAY type values are stored in Druid segments. When set to ",(0,n.jsx)(r.code,{children:"array"})," (recommended for SQL compliance), Druid will store all ARRAY typed values in ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/arrays",children:"ARRAY typed columns"}),", and supports storing both VARCHAR and numeric typed arrays. When set to ",(0,n.jsx)(r.code,{children:"mvd"})," (the default, for backwards compatibility), Druid only supports VARCHAR typed arrays, and will store them as ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/multi-value-dimensions",children:"multi-value string columns"}),". See [",(0,n.jsx)(r.code,{children:"arrayIngestMode"}),"] in the ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/arrays",children:"Arrays"})," page for more details."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"mvd"})," (for backwards compatibility, recommended to use ",(0,n.jsx)(r.code,{children:"array"})," for SQL compliance)"]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"sqlJoinAlgorithm"})}),(0,n.jsxs)(r.td,{children:["SELECT, INSERT, REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"Algorithm to use for JOIN. Use ",(0,n.jsx)(r.code,{children:"broadcast"})," (the default) for broadcast hash join or ",(0,n.jsx)(r.code,{children:"sortMerge"})," for sort-merge join. Affects all JOIN operations in the query. This is a hint to the MSQ engine and the actual joins in the query may proceed in a different way than specified. See ",(0,n.jsx)(r.a,{href:"#joins",children:"Joins"})," for more details."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"broadcast"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"rowsInMemory"})}),(0,n.jsxs)(r.td,{children:["INSERT or REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"Maximum number of rows to store in memory at once before flushing to disk during the segment generation process. Ignored for non-INSERT queries. In most cases, use the default value. You may need to override the default if you run into one of the ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/known-issues",children:"known issues"})," around memory usage."]}),(0,n.jsx)(r.td,{children:"100,000"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"segmentSortOrder"})}),(0,n.jsxs)(r.td,{children:["INSERT or REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"Normally, Druid sorts rows in individual segments using ",(0,n.jsx)(r.code,{children:"__time"})," first, followed by the ",(0,n.jsx)(r.a,{href:"#clustered-by",children:"CLUSTERED BY"})," clause. When you set ",(0,n.jsx)(r.code,{children:"segmentSortOrder"}),", Druid uses the order from this context parameter instead. Provide the column list as comma-separated values or as a JSON array in string form.",(0,n.jsx)("br",{}),"< br/>For example, consider an INSERT query that uses ",(0,n.jsx)(r.code,{children:"CLUSTERED BY country"})," and has ",(0,n.jsx)(r.code,{children:"segmentSortOrder"})," set to ",(0,n.jsx)(r.code,{children:"__time,city,country"}),". Within each time chunk, Druid assigns rows to segments based on ",(0,n.jsx)(r.code,{children:"country"}),", and then within each of those segments, Druid sorts those rows by ",(0,n.jsx)(r.code,{children:"__time"})," first, then ",(0,n.jsx)(r.code,{children:"city"}),", then ",(0,n.jsx)(r.code,{children:"country"}),"."]}),(0,n.jsx)(r.td,{children:"empty list"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"forceSegmentSortByTime"})}),(0,n.jsxs)(r.td,{children:["INSERT or REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"When set to ",(0,n.jsx)(r.code,{children:"true"})," (the default), Druid prepends ",(0,n.jsx)(r.code,{children:"__time"})," to ",(0,n.jsx)(r.a,{href:"#clustered-by",children:"CLUSTERED BY"})," when determining the sort order for individual segments. Druid also requires that ",(0,n.jsx)(r.code,{children:"segmentSortOrder"}),", if provided, starts with ",(0,n.jsx)(r.code,{children:"__time"}),".",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"When set to ",(0,n.jsx)(r.code,{children:"false"}),", Druid uses the ",(0,n.jsx)(r.a,{href:"#clustered-by",children:"CLUSTERED BY"})," alone to determine the sort order for individual segments, and does not require that ",(0,n.jsx)(r.code,{children:"segmentSortOrder"})," begin with ",(0,n.jsx)(r.code,{children:"__time"}),". Setting this parameter to ",(0,n.jsx)(r.code,{children:"false"})," is an experimental feature; see ",(0,n.jsx)(r.a,{href:"../ingestion/partitioning#sorting",children:"Sorting"})," for details."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"true"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"maxParseExceptions"})}),(0,n.jsxs)(r.td,{children:["SELECT, INSERT, REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"Maximum number of parse exceptions that are ignored while executing the query before it stops with ",(0,n.jsx)(r.code,{children:"TooManyWarningsFault"}),". To ignore all the parse exceptions, set the value to -1."]}),(0,n.jsx)(r.td,{children:"0"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"rowsPerSegment"})}),(0,n.jsxs)(r.td,{children:["INSERT or REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"The number of rows per segment to target. The actual number of rows per segment may be somewhat higher or lower than this number. In most cases, use the default. For general information about sizing rows per segment, see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/operations/segment-optimization",children:"Segment Size Optimization"}),"."]}),(0,n.jsx)(r.td,{children:"3,000,000"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"indexSpec"})}),(0,n.jsxs)(r.td,{children:["INSERT or REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"An ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#indexspec",children:(0,n.jsx)(r.code,{children:"indexSpec"})})," to use when generating segments. May be a JSON string or object. See ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#front-coding",children:"Front coding"})," for details on configuring an ",(0,n.jsx)(r.code,{children:"indexSpec"})," with front coding."]}),(0,n.jsxs)(r.td,{children:["See ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#indexspec",children:(0,n.jsx)(r.code,{children:"indexSpec"})}),"."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"durableShuffleStorage"})}),(0,n.jsxs)(r.td,{children:["SELECT, INSERT, REPLACE ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"Whether to use durable storage for shuffle mesh. To use this feature, configure the durable storage at the server level using ",(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.enable=true"}),"). If these properties are not configured, any query with the context variable ",(0,n.jsx)(r.code,{children:"durableShuffleStorage=true"})," fails with a configuration error. ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"false"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"faultTolerance"})}),(0,n.jsxs)(r.td,{children:["SELECT, INSERT, REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," Whether to turn on fault tolerance mode or not. Failed workers are retried based on ",(0,n.jsx)(r.a,{href:"#limits",children:"Limits"}),". Cannot be used when ",(0,n.jsx)(r.code,{children:"durableShuffleStorage"})," is explicitly set to false."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"false"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"selectDestination"})}),(0,n.jsxs)(r.td,{children:["SELECT",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," Controls where the final result of the select query is written. ",(0,n.jsx)("br",{}),"Use ",(0,n.jsx)(r.code,{children:"taskReport"}),"(the default) to write select results to the task report. ",(0,n.jsx)("b",{children:" This is not scalable since task reports size explodes for large results "})," ",(0,n.jsx)("br",{}),"Use ",(0,n.jsx)(r.code,{children:"durableStorage"})," to write results to durable storage location. ",(0,n.jsxs)("b",{children:["For large results sets, its recommended to use ",(0,n.jsx)(r.code,{children:"durableStorage"})," "]}),". To configure durable storage see ",(0,n.jsx)(r.a,{href:"#durable-storage",children:(0,n.jsx)(r.code,{children:"this"})})," section."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"taskReport"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"waitUntilSegmentsLoad"})}),(0,n.jsxs)(r.td,{children:["INSERT, REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," If set, the ingest query waits for the generated segments to be loaded before exiting, else the ingest query exits without waiting. The task and live reports contain the information about the status of loading segments if this flag is set. This will ensure that any future queries made after the ingestion exits will include results from the ingestion. The drawback is that the controller task will stall till the segments are loaded."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"false"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"includeSegmentSource"})}),(0,n.jsxs)(r.td,{children:["SELECT, INSERT, REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," Controls the sources, which will be queried for results in addition to the segments present on deep storage. Can be ",(0,n.jsx)(r.code,{children:"NONE"})," or ",(0,n.jsx)(r.code,{children:"REALTIME"}),". If this value is ",(0,n.jsx)(r.code,{children:"NONE"}),", only non-realtime (published and used) segments will be downloaded from deep storage. If this value is ",(0,n.jsx)(r.code,{children:"REALTIME"}),", results will also be included from realtime tasks. ",(0,n.jsx)(r.code,{children:"REALTIME"})," cannot be used while writing data into the same datasource it is read from."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"NONE"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"rowsPerPage"})}),(0,n.jsxs)(r.td,{children:["SELECT",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"The number of rows per page to target. The actual number of rows per page may be somewhat higher or lower than this number. In most cases, use the default.",(0,n.jsx)("br",{})," This property comes into effect only when ",(0,n.jsx)(r.code,{children:"selectDestination"})," is set to ",(0,n.jsx)(r.code,{children:"durableStorage"})]}),(0,n.jsx)(r.td,{children:"100000"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"skipTypeVerification"})}),(0,n.jsxs)(r.td,{children:["INSERT or REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"During query validation, Druid validates that ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/arrays",children:"string arrays"})," and ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/multi-value-dimensions",children:"multi-value dimensions"})," are not mixed in the same column. If you are intentionally migrating from one to the other, use this context parameter to disable type validation.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"Provide the column list as comma-separated values or as a JSON array in string form."]}),(0,n.jsx)(r.td,{children:"empty list"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"failOnEmptyInsert"})}),(0,n.jsxs)(r.td,{children:["INSERT or REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," When set to false (the default), an INSERT query generating no output rows will be no-op, and a REPLACE query generating no output rows will delete all data that matches the OVERWRITE clause.  When set to true, an ingest query generating no output rows will throw an ",(0,n.jsx)(r.code,{children:"InsertCannotBeEmpty"})," fault."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"false"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"storeCompactionState"})}),(0,n.jsxs)(r.td,{children:["REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," When set to true, a REPLACE query stores as part of each segment's metadata a ",(0,n.jsx)(r.code,{children:"lastCompactionState"})," field that captures the various specs used to create the segment. Future compaction jobs skip segments whose ",(0,n.jsx)(r.code,{children:"lastCompactionState"})," matches the desired compaction state. Works the same as ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/ingestion/tasks#context-parameters",children:(0,n.jsx)(r.code,{children:"storeCompactionState"})})," task context flag."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"false"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"removeNullBytes"})}),(0,n.jsxs)(r.td,{children:["SELECT, INSERT or REPLACE",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," The MSQ engine cannot process null bytes in strings and throws ",(0,n.jsx)(r.code,{children:"InvalidNullByteFault"})," if it encounters them in the source data. If the parameter is set to true, The MSQ engine will remove the null bytes in string fields when reading the data."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"false"})})]})]})]}),"\n",(0,n.jsx)(r.h2,{id:"joins",children:"Joins"}),"\n",(0,n.jsxs)(r.p,{children:["Joins in multi-stage queries use one of two algorithms based on what you set the ",(0,n.jsx)(r.a,{href:"#context-parameters",children:"context parameter"})," ",(0,n.jsx)(r.code,{children:"sqlJoinAlgorithm"})," to:"]}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.a,{href:"#broadcast",children:(0,n.jsx)(r.code,{children:"broadcast"})})," (default)"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.a,{href:"#sort-merge",children:(0,n.jsx)(r.code,{children:"sortMerge"})}),"."]}),"\n"]}),"\n",(0,n.jsx)(r.p,{children:"If you omit this context parameter, the MSQ task engine uses broadcast since it's the default join algorithm. The context parameter applies to the entire SQL statement, so you can't mix different\njoin algorithms in the same query."}),"\n",(0,n.jsxs)(r.p,{children:[(0,n.jsx)(r.code,{children:"sqlJoinAlgorithm"})," is a hint to the planner to execute the join in the specified manner. The planner can decide to ignore\nthe hint if it deduces that the specified algorithm can be detrimental to the performance of the join beforehand. This intelligence\nis very limited as of now, and the ",(0,n.jsx)(r.code,{children:"sqlJoinAlgorithm"})," set would be respected in most cases, therefore the user should set it\nappropriately. See the advantages and the drawbacks for the ",(0,n.jsx)(r.a,{href:"#broadcast",children:"broadcast"})," and the ",(0,n.jsx)(r.a,{href:"#sort-merge",children:"sort-merge"})," join to\ndetermine which join to use beforehand."]}),"\n",(0,n.jsx)(r.h3,{id:"broadcast",children:"Broadcast"}),"\n",(0,n.jsxs)(r.p,{children:["The default join algorithm for multi-stage queries is a broadcast hash join, which is similar to how\n",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/query-execution#join",children:"joins are executed with native queries"}),"."]}),"\n",(0,n.jsxs)(r.p,{children:["To use broadcast joins, either omit the  ",(0,n.jsx)(r.code,{children:"sqlJoinAlgorithm"})," or set it to ",(0,n.jsx)(r.code,{children:"broadcast"}),"."]}),"\n",(0,n.jsx)(r.p,{children:'For a broadcast join, any adjacent joins are flattened\ninto a structure with a "base" input (the bottom-leftmost one) and other leaf inputs (the rest). Next, any subqueries\nthat are inputs the join (either base or other leafs) are planned into independent stages. Then, the non-base leaf\ninputs are all connected as broadcast inputs to the "base" stage.'}),"\n",(0,n.jsxs)(r.p,{children:["Together, all of these non-base leaf inputs must not exceed the ",(0,n.jsx)(r.a,{href:"#limits",children:"limit on broadcast table footprint"}),". There\nis no limit on the size of the base (leftmost) input."]}),"\n",(0,n.jsxs)(r.p,{children:["Only LEFT JOIN, INNER JOIN, and CROSS JOIN are supported with ",(0,n.jsx)(r.code,{children:"broadcast"}),"."]}),"\n",(0,n.jsxs)(r.p,{children:["Join conditions, if present, must be equalities. It is not necessary to include a join condition; for example,\n",(0,n.jsx)(r.code,{children:"CROSS JOIN"})," and comma join do not require join conditions."]}),"\n",(0,n.jsxs)(r.p,{children:["The following example has a single join chain where ",(0,n.jsx)(r.code,{children:"orders"})," is the base input while ",(0,n.jsx)(r.code,{children:"products"})," and\n",(0,n.jsx)(r.code,{children:"customers"})," are non-base leaf inputs. The broadcast inputs (",(0,n.jsx)(r.code,{children:"products"})," and ",(0,n.jsx)(r.code,{children:"customers"}),") must fall under the limit on broadcast table footprint, but the base ",(0,n.jsx)(r.code,{children:"orders"})," input\ncan be unlimited in size."]}),"\n",(0,n.jsxs)(r.p,{children:["The query reads ",(0,n.jsx)(r.code,{children:"products"})," and ",(0,n.jsx)(r.code,{children:"customers"})," and then broadcasts both to\nthe stage that reads ",(0,n.jsx)(r.code,{children:"orders"}),". That stage loads the broadcast inputs (",(0,n.jsx)(r.code,{children:"products"})," and ",(0,n.jsx)(r.code,{children:"customers"}),") in memory and walks\nthrough ",(0,n.jsx)(r.code,{children:"orders"})," row by row. The results are aggregated and written to the table ",(0,n.jsx)(r.code,{children:"orders_enriched"}),"."]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-sql",children:"REPLACE INTO orders_enriched\nOVERWRITE ALL\nSELECT\n  orders.__time,\n  products.name AS product_name,\n  customers.name AS customer_name,\n  SUM(orders.amount) AS amount\nFROM orders\nLEFT JOIN products ON orders.product_id = products.id\nLEFT JOIN customers ON orders.customer_id = customers.id\nGROUP BY 1, 2\nPARTITIONED BY HOUR\nCLUSTERED BY product_name\n"})}),"\n",(0,n.jsx)(r.h3,{id:"sort-merge",children:"Sort-merge"}),"\n",(0,n.jsxs)(r.p,{children:["You can use the sort-merge join algorithm to make queries more scalable at the cost of performance. If your goal is performance, consider ",(0,n.jsx)(r.a,{href:"#broadcast",children:"broadcast joins"}),".  There are various scenarios where broadcast join would return a ",(0,n.jsx)(r.a,{href:"#error-codes",children:(0,n.jsx)(r.code,{children:"BroadcastTablesTooLarge"})})," error, but a sort-merge join would succeed."]}),"\n",(0,n.jsxs)(r.p,{children:["To use the sort-merge join algorithm, set the context parameter ",(0,n.jsx)(r.code,{children:"sqlJoinAlgorithm"})," to ",(0,n.jsx)(r.code,{children:"sortMerge"}),"."]}),"\n",(0,n.jsx)(r.p,{children:"In a sort-merge join, each pairwise join is planned into its own stage with two inputs. The two inputs are partitioned and sorted using a hash partitioning on the same key."}),"\n",(0,n.jsx)(r.p,{children:"When using the sort-merge algorithm, keep the following in mind:"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:["\n",(0,n.jsx)(r.p,{children:"There is no limit on the overall size of either input, so sort-merge is a good choice for performing a join of two large inputs or for performing a self-join of a large input with itself."}),"\n"]}),"\n",(0,n.jsxs)(r.li,{children:["\n",(0,n.jsxs)(r.p,{children:["There is a limit on the amount of data associated with each individual key. If ",(0,n.jsx)(r.em,{children:"both"})," sides of the join exceed this limit, the query returns a ",(0,n.jsx)(r.a,{href:"#error-codes",children:(0,n.jsx)(r.code,{children:"TooManyRowsWithSameKey"})})," error. If only one side exceeds the limit, the query does not return this error."]}),"\n"]}),"\n",(0,n.jsxs)(r.li,{children:["\n",(0,n.jsxs)(r.p,{children:["Join conditions are optional but must be equalities if they are present. For example, ",(0,n.jsx)(r.code,{children:"CROSS JOIN"})," and comma join do not require join conditions."]}),"\n"]}),"\n",(0,n.jsxs)(r.li,{children:["\n",(0,n.jsxs)(r.p,{children:["All join types are supported with ",(0,n.jsx)(r.code,{children:"sortMerge"}),": LEFT, RIGHT, INNER, FULL, and CROSS."]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(r.p,{children:["The following example  runs using a single sort-merge join stage that receives ",(0,n.jsx)(r.code,{children:"eventstream"}),"\n(partitioned on ",(0,n.jsx)(r.code,{children:"user_id"}),") and ",(0,n.jsx)(r.code,{children:"users"})," (partitioned on ",(0,n.jsx)(r.code,{children:"id"}),") as inputs. There is no limit on the size of either input."]}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-sql",children:"REPLACE INTO eventstream_enriched\nOVERWRITE ALL\nSELECT\n  eventstream.__time,\n  eventstream.user_id,\n  eventstream.event_type,\n  eventstream.event_details,\n  users.signup_date AS user_signup_date\nFROM eventstream\nLEFT JOIN users ON eventstream.user_id = users.id\nPARTITIONED BY HOUR\nCLUSTERED BY user\n"})}),"\n",(0,n.jsxs)(r.p,{children:["The context parameter that sets ",(0,n.jsx)(r.code,{children:"sqlJoinAlgorithm"})," to ",(0,n.jsx)(r.code,{children:"sortMerge"})," is not shown in the above example."]}),"\n",(0,n.jsx)(r.h2,{id:"durable-storage",children:"Durable storage"}),"\n",(0,n.jsxs)(r.p,{children:["SQL-based ingestion supports using durable storage to store intermediate files temporarily. Enabling it can improve reliability. For more information, see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/operations/durable-storage",children:"Durable storage"}),"."]}),"\n",(0,n.jsx)(r.h3,{id:"durable-storage-configurations",children:"Durable storage configurations"}),"\n",(0,n.jsx)(r.p,{children:"Durable storage is supported on Amazon S3 storage, Microsoft's Azure Blob Storage and Google Cloud Storage.\nThere are common configurations that control the behavior regardless of which storage service you use. Apart from these common configurations, there are a few properties specific to S3 and to Azure."}),"\n",(0,n.jsx)(r.p,{children:"Common properties to configure the behavior of durable storage"}),"\n",(0,n.jsxs)(r.p,{children:["|Parameter          | Required | Description          | Default |\n|--|--|--|\n|",(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.enable"}),"  | Yes |  Whether to enable durable storage for the cluster. Set it to true to enable durable storage. For more information about enabling durable storage, see ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/operations/durable-storage",children:"Durable storage"}),". | false |\n|",(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.type"})," |  Yes | The type of storage to use. Set it to ",(0,n.jsx)(r.code,{children:"s3"})," for S3, ",(0,n.jsx)(r.code,{children:"azure"})," for Azure and ",(0,n.jsx)(r.code,{children:"google"})," for Google | n/a |\n|",(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.tempDir"}),"| Yes |  Directory path on the local disk to store temporary files required while uploading and downloading the data. If the property is not configured on the indexer or middle manager, it defaults to using the task temporary directory. | n/a |\n|",(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.maxRetry"})," |  No | Defines the max number times to attempt S3 API calls to avoid failures due to transient errors. | 10 |\n|",(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.chunkSize"})," | No | Defines the size of each chunk to temporarily store in ",(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.tempDir"}),". The chunk size must be between 5 MiB and 5 GiB. A large chunk size reduces the API calls made to the durable storage, however it requires more disk space to store the temporary chunks. Druid uses a default of 100MiB if the value is not provided.| 100MiB |"]}),"\n",(0,n.jsx)(r.p,{children:"To use S3 or Google for durable storage, you also need to configure the following properties:"}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{children:"Parameter"}),(0,n.jsx)(r.th,{children:"Required"}),(0,n.jsx)(r.th,{children:"Description"}),(0,n.jsx)(r.th,{children:"Default"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.bucket"})}),(0,n.jsx)(r.td,{children:"Yes"}),(0,n.jsx)(r.td,{children:"The S3 or Google bucket where the files are uploaded to and download from"}),(0,n.jsx)(r.td,{children:"n/a"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.prefix"})}),(0,n.jsx)(r.td,{children:"Yes"}),(0,n.jsx)(r.td,{children:"Path prepended to all the paths uploaded to the bucket to namespace the connector's files. Provide a unique value for the prefix and do not share the same prefix between different clusters. If the location includes other files or directories, then they might get cleaned up as well."}),(0,n.jsx)(r.td,{children:"n/a"})]})]})]}),"\n",(0,n.jsx)(r.p,{children:"To use Azure for durable storage, you also need to configure the following properties:"}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{children:"Parameter"}),(0,n.jsx)(r.th,{children:"Required"}),(0,n.jsx)(r.th,{children:"Description"}),(0,n.jsx)(r.th,{children:"Default"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.container"})}),(0,n.jsx)(r.td,{children:"Yes"}),(0,n.jsx)(r.td,{children:"The Azure container where the files are uploaded to and downloaded from."}),(0,n.jsx)(r.td,{children:"n/a"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.prefix"})}),(0,n.jsx)(r.td,{children:"Yes"}),(0,n.jsx)(r.td,{children:"Path prepended to all the paths uploaded to the container to namespace the connector's files. Provide a unique value for the prefix and do not share the same prefix between different clusters. If the location includes other files or directories, then they might get cleaned up as well."}),(0,n.jsx)(r.td,{children:"n/a"})]})]})]}),"\n",(0,n.jsx)(r.h3,{id:"durable-storage-cleaner-configurations",children:"Durable storage cleaner configurations"}),"\n",(0,n.jsx)(r.p,{children:"Durable storage creates files on the remote storage, and these files get cleaned up once a job no longer requires those files. However, due to failures causing abrupt exits of tasks, these files might not get cleaned up.\nYou can configure the Overlord to periodically clean up these intermediate files after a task completes and the files are no longer need. The files that get cleaned up are determined by the storage prefix you configure. Any files that match the path for the storage prefix may get cleaned up, not just intermediate files that are no longer needed."}),"\n",(0,n.jsx)(r.p,{children:"Use the following configurations to control the cleaner:"}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{children:"Parameter"}),(0,n.jsx)(r.th,{children:"Required"}),(0,n.jsx)(r.th,{children:"Description"}),(0,n.jsx)(r.th,{children:"Default"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.cleaner.enabled"})}),(0,n.jsx)(r.td,{children:"No"}),(0,n.jsx)(r.td,{children:"Whether durable storage cleaner should be enabled for the cluster."}),(0,n.jsx)(r.td,{children:"false"})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"druid.msq.intermediate.storage.cleaner.delaySeconds"})}),(0,n.jsx)(r.td,{children:"No"}),(0,n.jsx)(r.td,{children:"The delay (in seconds) after the latest run post which the durable storage cleaner cleans the up files."}),(0,n.jsx)(r.td,{children:"86400"})]})]})]}),"\n",(0,n.jsx)(r.h2,{id:"limits",children:"Limits"}),"\n",(0,n.jsxs)(r.p,{children:["Knowing the limits for the MSQ task engine can help you troubleshoot any ",(0,n.jsx)(r.a,{href:"#error-codes",children:"errors"})," that you encounter. Many of the errors occur as a result of reaching a limit."]}),"\n",(0,n.jsx)(r.p,{children:"The following table lists query limits:"}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{children:"Limit"}),(0,n.jsx)(r.th,{children:"Value"}),(0,n.jsx)(r.th,{children:"Error if exceeded"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Size of an individual row written to a frame. Row size when written to a frame may differ from the original row size."}),(0,n.jsx)(r.td,{children:"1 MB"}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.a,{href:"#error_RowTooLarge",children:(0,n.jsx)(r.code,{children:"RowTooLarge"})})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Number of segment-granular time chunks encountered during ingestion."}),(0,n.jsx)(r.td,{children:"5,000"}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.a,{href:"#error_TooManyBuckets",children:(0,n.jsx)(r.code,{children:"TooManyBuckets"})})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Number of input files/segments per worker."}),(0,n.jsx)(r.td,{children:"10,000"}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.a,{href:"#error_TooManyInputFiles",children:(0,n.jsx)(r.code,{children:"TooManyInputFiles"})})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Number of output partitions for any one stage. Number of segments generated during ingestion."}),(0,n.jsx)(r.td,{children:"25,000"}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.a,{href:"#error_TooManyPartitions",children:(0,n.jsx)(r.code,{children:"TooManyPartitions"})})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Number of output columns for any one stage."}),(0,n.jsx)(r.td,{children:"2,000"}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.a,{href:"#error_TooManyColumns",children:(0,n.jsx)(r.code,{children:"TooManyColumns"})})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Number of cluster by columns that can appear in a stage"}),(0,n.jsx)(r.td,{children:"1,500"}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.a,{href:"#error_TooManyClusteredByColumns",children:(0,n.jsx)(r.code,{children:"TooManyClusteredByColumns"})})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Number of workers for any one stage."}),(0,n.jsx)(r.td,{children:"Hard limit is 1,000. Memory-dependent soft limit may be lower."}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.a,{href:"#error_TooManyWorkers",children:(0,n.jsx)(r.code,{children:"TooManyWorkers"})})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Maximum memory occupied by broadcasted tables."}),(0,n.jsxs)(r.td,{children:["30% of each ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/multi-stage-query/concepts#memory-usage",children:"processor memory bundle"}),"."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.a,{href:"#error_BroadcastTablesTooLarge",children:(0,n.jsx)(r.code,{children:"BroadcastTablesTooLarge"})})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsxs)(r.td,{children:["Maximum memory occupied by buffered data during sort-merge join. Only relevant when ",(0,n.jsx)(r.code,{children:"sqlJoinAlgorithm"})," is ",(0,n.jsx)(r.code,{children:"sortMerge"}),"."]}),(0,n.jsx)(r.td,{children:"10 MB"}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"TooManyRowsWithSameKey"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsxs)(r.td,{children:["Maximum relaunch attempts per worker. Initial run is not a relaunch. The worker will be spawned 1 + ",(0,n.jsx)(r.code,{children:"workerRelaunchLimit"})," times before the job fails."]}),(0,n.jsx)(r.td,{children:"2"}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"TooManyAttemptsForWorker"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:"Maximum relaunch attempts for a job across all workers."}),(0,n.jsx)(r.td,{children:"100"}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"TooManyAttemptsForJob"})})]})]})]}),"\n",(0,n.jsx)("a",{name:"errors"}),"\n",(0,n.jsx)(r.h2,{id:"error-codes",children:"Error codes"}),"\n",(0,n.jsxs)(r.p,{children:["The following table describes error codes you may encounter in the ",(0,n.jsx)(r.code,{children:"multiStageQuery.payload.status.errorReport.error.errorCode"})," field:"]}),"\n",(0,n.jsxs)(r.table,{children:[(0,n.jsx)(r.thead,{children:(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.th,{children:"Code"}),(0,n.jsx)(r.th,{children:"Meaning"}),(0,n.jsx)(r.th,{children:"Additional fields"})]})}),(0,n.jsxs)(r.tbody,{children:[(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_BroadcastTablesTooLarge",children:(0,n.jsx)(r.code,{children:"BroadcastTablesTooLarge"})})}),(0,n.jsxs)(r.td,{children:["The size of the broadcast tables used in the right hand side of the join exceeded the memory reserved for them in a worker task.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"Try increasing the peon memory or reducing the size of the broadcast tables."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"maxBroadcastTablesSize"}),": Memory reserved for the broadcast tables, measured in bytes."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_Canceled",children:(0,n.jsx)(r.code,{children:"Canceled"})})}),(0,n.jsxs)(r.td,{children:["The query was canceled. Common reasons for cancellation:",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsxs)("ul",{children:[(0,n.jsxs)("li",{children:["User-initiated shutdown of the controller task via the ",(0,n.jsx)(r.code,{children:"/druid/indexer/v1/task/{taskId}/shutdown"})," API."]}),(0,n.jsx)("li",{children:"Restart or failure of the server process that was running the controller task."})]})]}),(0,n.jsx)(r.td,{})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_CannotParseExternalData",children:(0,n.jsx)(r.code,{children:"CannotParseExternalData"})})}),(0,n.jsx)(r.td,{children:"A worker task could not parse data from an external datasource."}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"errorMessage"}),": More details on why parsing failed."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_ColumnNameRestricted",children:(0,n.jsx)(r.code,{children:"ColumnNameRestricted"})})}),(0,n.jsx)(r.td,{children:"The query uses a restricted column name."}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"columnName"}),": The restricted column name."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_ColumnTypeNotSupported",children:(0,n.jsx)(r.code,{children:"ColumnTypeNotSupported"})})}),(0,n.jsxs)(r.td,{children:["The column type is not supported. This can be because:",(0,n.jsx)("br",{})," ",(0,n.jsx)("br",{}),(0,n.jsxs)("ul",{children:[(0,n.jsx)("li",{children:"Support for writing or reading from a particular column type is not supported."}),(0,n.jsx)("li",{children:"The query attempted to use a column type that is not supported by the frame format. This occurs with ARRAY types, which are not yet implemented for frames."})]})]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"columnName"}),": The column name with an unsupported type.",(0,n.jsx)("br",{})," ",(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"columnType"}),": The unknown column type."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_InsertCannotAllocateSegment",children:(0,n.jsx)(r.code,{children:"InsertCannotAllocateSegment"})})}),(0,n.jsxs)(r.td,{children:["The controller task could not allocate a new segment ID due to conflict with existing segments or pending segments. Common reasons for such conflicts:",(0,n.jsx)("br",{})," ",(0,n.jsx)("br",{}),(0,n.jsxs)("ul",{children:[(0,n.jsx)("li",{children:"Attempting to mix different granularities in the same intervals of the same datasource."}),(0,n.jsx)("li",{children:"Prior ingestions that used non-extendable shard specs."})]})," ",(0,n.jsx)("br",{})," ",(0,n.jsx)("br",{})," Use REPLACE to overwrite the existing data or if the error contains the ",(0,n.jsx)(r.code,{children:"allocatedInterval"})," then alternatively rerun the INSERT job with the mentioned granularity to append to existing data. Note that it might not always be possible to append to the existing data using INSERT and can only be done if ",(0,n.jsx)(r.code,{children:"allocatedInterval"})," is present."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"dataSource"}),(0,n.jsx)("br",{})," ",(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"interval"}),": The interval for the attempted new segment allocation. ",(0,n.jsx)("br",{})," ",(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"allocatedInterval"}),": The incorrect interval allocated by the overlord. It can be null"]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_InsertCannotBeEmpty",children:(0,n.jsx)(r.code,{children:"InsertCannotBeEmpty"})})}),(0,n.jsxs)(r.td,{children:["An INSERT or REPLACE query did not generate any output rows when ",(0,n.jsx)(r.code,{children:"failOnEmptyInsert"})," query context is set to true. ",(0,n.jsx)(r.code,{children:"failOnEmptyInsert"})," defaults to false, so an INSERT query generating no output rows will be no-op, and a REPLACE query generating no output rows will delete all data that matches the OVERWRITE clause."]}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"dataSource"})})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_InsertLockPreempted",children:(0,n.jsx)(r.code,{children:"InsertLockPreempted"})})}),(0,n.jsx)(r.td,{children:"An INSERT or REPLACE query was canceled by a higher-priority ingestion job, such as a real-time ingestion task."}),(0,n.jsx)(r.td,{})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_InsertTimeNull",children:(0,n.jsx)(r.code,{children:"InsertTimeNull"})})}),(0,n.jsxs)(r.td,{children:["An INSERT or REPLACE query encountered a null timestamp in the ",(0,n.jsx)(r.code,{children:"__time"})," field.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"This can happen due to using an expression like ",(0,n.jsx)(r.code,{children:"TIME_PARSE(timestamp) AS __time"})," with a timestamp that cannot be parsed. (",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/sql-scalar#date-and-time-functions",children:(0,n.jsx)(r.code,{children:"TIME_PARSE"})})," returns null when it cannot parse a timestamp.) In this case, try parsing your timestamps using a different function or pattern. Or, if your timestamps may genuinely be null, consider using ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/sql-scalar#other-scalar-functions",children:(0,n.jsx)(r.code,{children:"COALESCE"})})," to provide a default value. One option is ",(0,n.jsx)(r.a,{href:"/docs/33.0.0/querying/sql-scalar#date-and-time-functions",children:(0,n.jsx)(r.code,{children:"CURRENT_TIMESTAMP"})}),", which represents the start time of the job."]}),(0,n.jsx)(r.td,{})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_InsertTimeOutOfBounds",children:(0,n.jsx)(r.code,{children:"InsertTimeOutOfBounds"})})}),(0,n.jsxs)(r.td,{children:["A REPLACE query generated a timestamp outside the bounds of the TIMESTAMP parameter for your OVERWRITE WHERE clause.",(0,n.jsx)("br",{})," ",(0,n.jsx)("br",{}),"To avoid this error, verify that the you specified is valid."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"interval"}),": time chunk interval corresponding to the out-of-bounds timestamp"]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_InvalidField",children:(0,n.jsx)(r.code,{children:"InvalidField"})})}),(0,n.jsx)(r.td,{children:"An error was encountered while writing a field."}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"error"}),": Encountered error. ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"source"}),": Source for the error. ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"rowNumber"}),": Row number (1-indexed) for the error. ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"column"}),": Column for the error."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_InvalidNullByte",children:(0,n.jsx)(r.code,{children:"InvalidNullByte"})})}),(0,n.jsx)(r.td,{children:"A string column included a null byte. Null bytes in strings are not permitted."}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"source"}),": The source that included the null byte ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"rowNumber"}),": The row number (1-indexed) that included the null byte ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"column"}),": The column that included the null byte ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"value"}),": Actual string containing the null byte ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"position"}),": Position (1-indexed) of occurrence of null byte"]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_QueryNotSupported",children:(0,n.jsx)(r.code,{children:"QueryNotSupported"})})}),(0,n.jsxs)(r.td,{children:["QueryKit could not translate the provided native query to a multi-stage query.",(0,n.jsx)("br",{})," ",(0,n.jsx)("br",{}),"This can happen if the query uses features that aren't supported, like GROUPING SETS."]}),(0,n.jsx)(r.td,{})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_QueryRuntimeError",children:(0,n.jsx)(r.code,{children:"QueryRuntimeError"})})}),(0,n.jsxs)(r.td,{children:["MSQ uses the native query engine to run the leaf stages. This error tells MSQ that error is in native query runtime.",(0,n.jsx)("br",{})," ",(0,n.jsx)("br",{})," Since this is a generic error, the user needs to look at logs for the error message and stack trace to figure out the next course of action. If the user is stuck, consider raising a ",(0,n.jsx)(r.code,{children:"github"})," issue for assistance."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"baseErrorMessage"})," error message from the native query runtime."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_RowTooLarge",children:(0,n.jsx)(r.code,{children:"RowTooLarge"})})}),(0,n.jsxs)(r.td,{children:["The query tried to process a row that was too large to write to a single frame. See the ",(0,n.jsx)(r.a,{href:"#limits",children:"Limits"})," table for specific limits on frame size. Note that the effective maximum row size is smaller than the maximum frame size due to alignment considerations during frame writing."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"maxFrameSize"}),": The limit on the frame size."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_TaskStartTimeout",children:(0,n.jsx)(r.code,{children:"TaskStartTimeout"})})}),(0,n.jsxs)(r.td,{children:["Unable to launch ",(0,n.jsx)(r.code,{children:"pendingTasks"})," worker out of total ",(0,n.jsx)(r.code,{children:"totalTasks"})," workers tasks within ",(0,n.jsx)(r.code,{children:"timeout"})," seconds of the last successful worker launch.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"There may be insufficient available slots to start all the worker tasks simultaneously. Try splitting up your query into smaller chunks using a smaller value of ",(0,n.jsx)(r.a,{href:"#context-parameters",children:(0,n.jsx)(r.code,{children:"maxNumTasks"})}),". Another option is to increase capacity."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"pendingTasks"}),": Number of tasks not yet started.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"totalTasks"}),": The number of tasks attempted to launch.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"timeout"}),": Timeout, in milliseconds, that was exceeded."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_TooManyAttemptsForJob",children:(0,n.jsx)(r.code,{children:"TooManyAttemptsForJob"})})}),(0,n.jsxs)(r.td,{children:["Total relaunch attempt count across all workers exceeded max relaunch attempt limit. See the ",(0,n.jsx)(r.a,{href:"#limits",children:"Limits"})," table for the specific limit."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"maxRelaunchCount"}),": Max number of relaunches across all the workers defined in the ",(0,n.jsx)(r.a,{href:"#limits",children:"Limits"})," section. ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"currentRelaunchCount"}),": current relaunch counter for the job across all workers. ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"taskId"}),": Latest task id which failed ",(0,n.jsx)("br",{})," ",(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"rootErrorMessage"}),": Error message of the latest failed task."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_TooManyAttemptsForWorker",children:(0,n.jsx)(r.code,{children:"TooManyAttemptsForWorker"})})}),(0,n.jsxs)(r.td,{children:["Worker exceeded maximum relaunch attempt count as defined in the ",(0,n.jsx)(r.a,{href:"#limits",children:"Limits"})," section."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"maxPerWorkerRelaunchCount"}),": Max number of relaunches allowed per worker as defined in the ",(0,n.jsx)(r.a,{href:"#limits",children:"Limits"})," section. ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"workerNumber"}),": the worker number for which the task failed ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"taskId"}),": Latest task id which failed ",(0,n.jsx)("br",{})," ",(0,n.jsx)("br",{})," ",(0,n.jsx)(r.code,{children:"rootErrorMessage"}),": Error message of the latest failed task."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_TooManyBuckets",children:(0,n.jsx)(r.code,{children:"TooManyBuckets"})})}),(0,n.jsxs)(r.td,{children:["Exceeded the maximum number of partition buckets for a stage (5,000 partition buckets).",(0,n.jsx)("br",{}),"< br />Partition buckets are created for each ",(0,n.jsx)(r.a,{href:"#partitioned-by",children:(0,n.jsx)(r.code,{children:"PARTITIONED BY"})})," time chunk for INSERT and REPLACE queries. The most common reason for this error is that your ",(0,n.jsx)(r.code,{children:"PARTITIONED BY"})," is too narrow relative to your data."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"maxBuckets"}),": The limit on partition buckets."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_TooManyInputFiles",children:(0,n.jsx)(r.code,{children:"TooManyInputFiles"})})}),(0,n.jsxs)(r.td,{children:["Exceeded the maximum number of input files or segments per worker (10,000 files or segments).",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"If you encounter this limit, consider adding more workers, or breaking up your query into smaller queries that process fewer files or segments per query."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"numInputFiles"}),": The total number of input files/segments for the stage.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"maxInputFiles"}),": The maximum number of input files/segments per worker per stage.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"minNumWorker"}),": The minimum number of workers required for a successful run."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_TooManyPartitions",children:(0,n.jsx)(r.code,{children:"TooManyPartitions"})})}),(0,n.jsxs)(r.td,{children:["Exceeded the maximum number of partitions for a stage (25,000 partitions).",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"This can occur with INSERT or REPLACE statements that generate large numbers of segments, since each segment is associated with a partition. If you encounter this limit, consider breaking up your INSERT or REPLACE statement into smaller statements that process less data per statement."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"maxPartitions"}),": The limit on partitions which was exceeded"]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_TooManyClusteredByColumns",children:(0,n.jsx)(r.code,{children:"TooManyClusteredByColumns"})})}),(0,n.jsxs)(r.td,{children:["Exceeded the maximum number of clustering columns for a stage (1,500 columns).",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),"This can occur with ",(0,n.jsx)(r.code,{children:"CLUSTERED BY"}),", ",(0,n.jsx)(r.code,{children:"ORDER BY"}),", or ",(0,n.jsx)(r.code,{children:"GROUP BY"})," with a large number of columns."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"numColumns"}),": The number of columns requested.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"maxColumns"}),": The limit on columns which was exceeded.",(0,n.jsx)(r.code,{children:"stage"}),": The stage number exceeding the limit",(0,n.jsx)("br",{}),(0,n.jsx)("br",{})]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_TooManyRowsWithSameKey",children:(0,n.jsx)(r.code,{children:"TooManyRowsWithSameKey"})})}),(0,n.jsxs)(r.td,{children:["The number of rows for a given key exceeded the maximum number of buffered bytes on both sides of a join. See the ",(0,n.jsx)(r.a,{href:"#limits",children:"Limits"})," table for the specific limit. Only occurs when join is executed via the sort-merge join algorithm."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"key"}),": The key that had a large number of rows.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"numBytes"}),": Number of bytes buffered, which may include other keys.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"maxBytes"}),": Maximum number of bytes buffered."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_TooManyColumns",children:(0,n.jsx)(r.code,{children:"TooManyColumns"})})}),(0,n.jsx)(r.td,{children:"Exceeded the maximum number of columns for a stage (2,000 columns)."}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"numColumns"}),": The number of columns requested.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"maxColumns"}),": The limit on columns which was exceeded."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_TooManyWarnings",children:(0,n.jsx)(r.code,{children:"TooManyWarnings"})})}),(0,n.jsx)(r.td,{children:"Exceeded the maximum allowed number of warnings of a particular type."}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"rootErrorCode"}),": The error code corresponding to the exception that exceeded the required limit. ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"maxWarnings"}),": Maximum number of warnings that are allowed for the corresponding ",(0,n.jsx)(r.code,{children:"rootErrorCode"}),"."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_TooManyWorkers",children:(0,n.jsx)(r.code,{children:"TooManyWorkers"})})}),(0,n.jsxs)(r.td,{children:["Exceeded the maximum number of simultaneously-running workers. See the ",(0,n.jsx)(r.a,{href:"#limits",children:"Limits"})," table for more details."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"workers"}),": The number of simultaneously running workers that exceeded a hard or soft limit. This may be larger than the number of workers in any one stage if multiple stages are running simultaneously. ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"maxWorkers"}),": The hard or soft limit on workers that was exceeded. If this is lower than the hard limit (1,000 workers), then you can increase the limit by adding more memory to each task."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_NotEnoughMemory",children:(0,n.jsx)(r.code,{children:"NotEnoughMemory"})})}),(0,n.jsx)(r.td,{children:"Insufficient memory to launch a stage."}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"suggestedServerMemory"}),": Suggested number of bytes of memory to allocate to a given process. ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"serverMemory"}),": The number of bytes of memory available to a single process.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"usableMemory"}),": The number of usable bytes of memory for a single process.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"serverWorkers"}),": The number of workers running in a single process.",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"serverThreads"}),": The number of threads in a single process."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_NotEnoughTemporaryStorage",children:(0,n.jsx)(r.code,{children:"NotEnoughTemporaryStorage"})})}),(0,n.jsxs)(r.td,{children:["Insufficient temporary storage configured to launch a stage. This limit is set by the property ",(0,n.jsx)(r.code,{children:"druid.indexer.task.tmpStorageBytesPerTask"}),". This property should be increased to the minimum suggested limit to resolve this."]}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"suggestedMinimumStorage"}),": Suggested number of bytes of temporary storage space to allocate to a given process. ",(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"configuredTemporaryStorage"}),": The number of bytes of storage currently configured."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_WorkerFailed",children:(0,n.jsx)(r.code,{children:"WorkerFailed"})})}),(0,n.jsx)(r.td,{children:"A worker task failed unexpectedly."}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"errorMsg"}),(0,n.jsx)("br",{}),(0,n.jsx)("br",{}),(0,n.jsx)(r.code,{children:"workerTaskId"}),": The ID of the worker task."]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_WorkerRpcFailed",children:(0,n.jsx)(r.code,{children:"WorkerRpcFailed"})})}),(0,n.jsx)(r.td,{children:"A remote procedure call to a worker task failed and could not recover."}),(0,n.jsxs)(r.td,{children:[(0,n.jsx)(r.code,{children:"workerTaskId"}),": the id of the worker task"]})]}),(0,n.jsxs)(r.tr,{children:[(0,n.jsx)(r.td,{children:(0,n.jsx)("a",{name:"error_UnknownError",children:(0,n.jsx)(r.code,{children:"UnknownError"})})}),(0,n.jsx)(r.td,{children:"All other errors."}),(0,n.jsx)(r.td,{children:(0,n.jsx)(r.code,{children:"message"})})]})]})]})]})}function h(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}}}]);