"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[6611],{28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>r});var s=i(96540);const t={},a=s.createContext(t);function o(e){const n=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),s.createElement(a.Provider,{value:n},e.children)}},61335:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"ingestion/schema-design","title":"Schema design tips","description":"\x3c!--","source":"@site/docs/33.0.0/ingestion/schema-design.md","sourceDirName":"ingestion","slug":"/ingestion/schema-design","permalink":"/docs/33.0.0/ingestion/schema-design","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"schema-design","title":"Schema design tips"},"sidebar":"docs","previous":{"title":"Ingestion spec reference","permalink":"/docs/33.0.0/ingestion/ingestion-spec"},"next":{"title":"Concurrent append and replace","permalink":"/docs/33.0.0/ingestion/concurrent-append-replace"}}');var t=i(74848),a=i(28453);const o={id:"schema-design",title:"Schema design tips"},r=void 0,d={},l=[{value:"Druid&#39;s data model",id:"druids-data-model",level:2},{value:"If you&#39;re coming from a",id:"if-youre-coming-from-a",level:2},{value:"Relational model",id:"relational-model",level:3},{value:"Time series model",id:"time-series-model",level:3},{value:"Log aggregation model",id:"log-aggregation-model",level:3},{value:"General tips and best practices",id:"general-tips-and-best-practices",level:2},{value:"Rollup",id:"rollup",level:3},{value:"Partitioning and sorting",id:"partitioning-and-sorting",level:3},{value:"Sketches for high cardinality columns",id:"sketches-for-high-cardinality-columns",level:3},{value:"String vs numeric dimensions",id:"string-vs-numeric-dimensions",level:3},{value:"Secondary timestamps",id:"secondary-timestamps",level:3},{value:"Nested dimensions",id:"nested-dimensions",level:3},{value:"Counting the number of ingested events",id:"counting-the-number-of-ingested-events",level:3},{value:"Schema auto-discovery for dimensions",id:"schema-auto-discovery-for-dimensions",level:3},{value:"Type-aware schema discovery",id:"type-aware-schema-discovery",level:4},{value:"String-based schema discovery",id:"string-based-schema-discovery",level:4},{value:"Migrating to type-aware schema discovery",id:"migrating-to-type-aware-schema-discovery",level:4},{value:"Including the same column as a dimension and a metric",id:"including-the-same-column-as-a-dimension-and-a-metric",level:3}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"druids-data-model",children:"Druid's data model"}),"\n",(0,t.jsxs)(n.p,{children:["For general information, check out the documentation on ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/schema-model",children:"Druid schema model"})," on the main\ningestion overview page. The rest of this page discusses tips for users coming from other kinds of systems, as well as\ngeneral tips and common practices."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Druid data is stored in ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/schema-model",children:"datasources"}),", which are similar to tables in a traditional RDBMS."]}),"\n",(0,t.jsxs)(n.li,{children:["Druid datasources can be ingested with or without ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/rollup",children:"rollup"}),". With rollup enabled, Druid partially aggregates your data during ingestion, potentially reducing its row count, decreasing storage footprint, and improving query performance. With rollup disabled, Druid stores one row for each row in your input data, without any pre-aggregation."]}),"\n",(0,t.jsx)(n.li,{children:"Every row in Druid must have a timestamp. Data is always partitioned by time, and every query has a time filter. Query results can also be broken down by time buckets like minutes, hours, days, and so on."}),"\n",(0,t.jsxs)(n.li,{children:["All columns in Druid datasources, other than the timestamp column, are either dimensions or metrics. This follows the ",(0,t.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Online_analytical_processing#Overview_of_OLAP_systems",children:"standard naming convention"})," of OLAP data."]}),"\n",(0,t.jsx)(n.li,{children:"Typical production datasources have tens to hundreds of columns."}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/schema-model#dimensions",children:"Dimension columns"})," are stored as-is, so they can be filtered on, grouped by, or aggregated at query time. They are always single Strings, ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/multi-value-dimensions",children:"arrays of Strings"}),", single Longs, single Doubles or single Floats."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/schema-model#metrics",children:"Metric columns"})," are stored ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/aggregations",children:"pre-aggregated"}),", so they can only be aggregated at query time (not filtered or grouped by). They are often stored as numbers (integers or floats) but can also be stored as complex objects like ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/aggregations#approximate-aggregations",children:"HyperLogLog sketches or approximate quantile sketches"}),". Metrics can be configured at ingestion time even when rollup is disabled, but are most useful when rollup is enabled."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"if-youre-coming-from-a",children:"If you're coming from a"}),"\n",(0,t.jsx)(n.h3,{id:"relational-model",children:"Relational model"}),"\n",(0,t.jsx)(n.p,{children:"(Like Hive or PostgreSQL.)"}),"\n",(0,t.jsxs)(n.p,{children:["Druid datasources are generally equivalent to tables in a relational database. Druid ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/lookups",children:"lookups"}),"\ncan act similarly to data-warehouse-style dimension tables, but as you'll see below, denormalization is often\nrecommended if you can get away with it."]}),"\n",(0,t.jsxs)(n.p,{children:["Common practice for relational data modeling involves ",(0,t.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Database_normalization",children:"normalization"}),':\nthe idea of splitting up data into multiple tables such that data redundancy is reduced or eliminated. For example, in a\n"sales" table, best-practices relational modeling calls for a "product id" column that is a foreign key into a separate\n"products" table, which in turn has "product id", "product name", and "product category" columns. This prevents the\nproduct name and category from needing to be repeated on different rows in the "sales" table that refer to the same\nproduct.']}),"\n",(0,t.jsxs)(n.p,{children:['In Druid, on the other hand, it is common to use totally flat datasources that do not require joins at query time. In\nthe example of the "sales" table, in Druid it would be typical to store "product_id", "product_name", and\n"product_category" as dimensions directly in a Druid "sales" datasource, without using a separate "products" table.\nTotally flat schemas substantially increase performance, since the need for joins is eliminated at query time. As an\nadded speed boost, this also allows Druid\'s query layer to operate directly on compressed dictionary-encoded data.\nPerhaps counter-intuitively, this does ',(0,t.jsx)(n.em,{children:"not"})," substantially increase storage footprint relative to normalized schemas,\nsince Druid uses dictionary encoding to effectively store just a single integer per row for string columns."]}),"\n",(0,t.jsxs)(n.p,{children:["If necessary, Druid datasources can be partially normalized through the use of ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/lookups",children:"lookups"}),",\nwhich are the rough equivalent of dimension tables in a relational database. At query time, you would use Druid's SQL\n",(0,t.jsx)(n.code,{children:"LOOKUP"})," function, or native lookup extraction functions, instead of using the JOIN keyword like you would in a\nrelational database. Since lookup tables impose an increase in memory footprint and incur more computational overhead\nat query time, it is only recommended to do this if you need the ability to update a lookup table and have the changes\nreflected immediately for already-ingested rows in your main table."]}),"\n",(0,t.jsx)(n.p,{children:"Tips for modeling relational data in Druid:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Druid datasources do not have primary or unique keys, so skip those."}),"\n",(0,t.jsxs)(n.li,{children:["Denormalize if possible. If you need to be able to update dimension / lookup tables periodically and have those\nchanges reflected in already-ingested data, consider partial normalization with ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/lookups",children:"lookups"}),"."]}),"\n",(0,t.jsx)(n.li,{children:"If you need to join two large distributed tables with each other, you must do this before loading the data into Druid.\nDruid does not support query-time joins of two datasources. Lookups do not help here, since a full copy of each lookup\ntable is stored on each Druid server, so they are not a good choice for large tables."}),"\n",(0,t.jsxs)(n.li,{children:["Consider whether you want to enable ",(0,t.jsx)(n.a,{href:"#rollup",children:"rollup"})," for pre-aggregation, or whether you want to disable\nrollup and load your existing data as-is. Rollup in Druid is similar to creating a summary table in a relational model."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"time-series-model",children:"Time series model"}),"\n",(0,t.jsx)(n.p,{children:"(Like OpenTSDB or InfluxDB.)"}),"\n",(0,t.jsx)(n.p,{children:"Similar to time series databases, Druid's data model requires a timestamp. Druid is not a timeseries database, but\nit is a natural choice for storing timeseries data. Its flexible data model allows it to store both timeseries and\nnon-timeseries data, even in the same datasource."}),"\n",(0,t.jsxs)(n.p,{children:["To achieve best-case compression and query performance in Druid for timeseries data, it is important to partition and\nsort by metric name, like timeseries databases often do. See ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/partitioning",children:"Partitioning and sorting"})," for more details."]}),"\n",(0,t.jsx)(n.p,{children:"Tips for modeling timeseries data in Druid:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Druid does not think of data points as being part of a "time series". Instead, Druid treats each point separately\nfor ingestion and aggregation.'}),"\n",(0,t.jsxs)(n.li,{children:['Create a dimension that indicates the name of the series that a data point belongs to. This dimension is often called\n"metric" or "name". Do not get the dimension named "metric" confused with the concept of Druid metrics. Place this\nfirst in the list of dimensions in your "dimensionsSpec" for best performance (this helps because it improves locality;\nsee ',(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/partitioning",children:"partitioning and sorting"})," below for details)."]}),"\n",(0,t.jsx)(n.li,{children:'Create other dimensions for attributes attached to your data points. These are often called "tags" in timeseries\ndatabase systems.'}),"\n",(0,t.jsxs)(n.li,{children:["Create ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/aggregations",children:"metrics"}),' corresponding to the types of aggregations that you want to be able\nto query. Typically, this includes "sum", "min", and "max" (in one of the long, float, or double flavors). If you want the ability\nto compute percentiles or quantiles, use Druid\'s ',(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/aggregations#approximate-aggregations",children:"approximate aggregators"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Consider enabling ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/rollup",children:"rollup"}),", which will allow Druid to potentially combine multiple points into one\nrow in your Druid datasource. This can be useful if you want to store data at a different time granularity than it is\nnaturally emitted. It is also useful if you want to combine timeseries and non-timeseries data in the same datasource."]}),"\n",(0,t.jsxs)(n.li,{children:["If you don't know ahead of time what columns you'll want to ingest, use an empty dimensions list to trigger\n",(0,t.jsx)(n.a,{href:"#schema-auto-discovery-for-dimensions",children:"automatic detection of dimension columns"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"log-aggregation-model",children:"Log aggregation model"}),"\n",(0,t.jsx)(n.p,{children:"(Like Elasticsearch or Splunk.)"}),"\n",(0,t.jsx)(n.p,{children:"Similar to log aggregation systems, Druid offers inverted indexes for fast searching and filtering. Druid's search\ncapabilities are generally less developed than these systems, and its analytical capabilities are generally more\ndeveloped. The main data modeling differences between Druid and these systems are that when ingesting data into Druid,\nyou must be more explicit. Druid columns have types specific upfront."}),"\n",(0,t.jsx)(n.p,{children:"Tips for modeling log data in Druid:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["If you don't know ahead of time what columns to ingest, you can have Druid perform ",(0,t.jsx)(n.a,{href:"#schema-auto-discovery-for-dimensions",children:"schema auto-discovery"}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["If you have nested data, you can ingest it using the ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/nested-columns",children:"nested columns"})," feature or flatten it using a ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#flattenspec",children:(0,t.jsx)(n.code,{children:"flattenSpec"})}),"."]}),"\n",(0,t.jsxs)(n.li,{children:["Consider enabling ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/rollup",children:"rollup"})," if you have mainly analytical use cases for your log data. This will\nmean you lose the ability to retrieve individual events from Druid, but you potentially gain substantial compression and\nquery performance boosts."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"general-tips-and-best-practices",children:"General tips and best practices"}),"\n",(0,t.jsx)(n.h3,{id:"rollup",children:"Rollup"}),"\n",(0,t.jsxs)(n.p,{children:["Druid can roll up data as it is ingested to minimize the amount of raw data that needs to be stored. This is a form\nof summarization or pre-aggregation. For more details, see the ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/rollup",children:"Rollup"})," section of the ingestion\ndocumentation."]}),"\n",(0,t.jsx)(n.h3,{id:"partitioning-and-sorting",children:"Partitioning and sorting"}),"\n",(0,t.jsxs)(n.p,{children:["Optimally partitioning and sorting your data can have substantial impact on footprint and performance. For more details,\nsee the ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/partitioning",children:"Partitioning"})," section of the ingestion documentation."]}),"\n",(0,t.jsx)("a",{name:"sketches"}),"\n",(0,t.jsx)(n.h3,{id:"sketches-for-high-cardinality-columns",children:"Sketches for high cardinality columns"}),"\n",(0,t.jsx)(n.p,{children:'When dealing with high cardinality columns like user IDs or other unique IDs, consider using sketches for approximate\nanalysis rather than operating on the actual values. When you ingest data using a sketch, Druid does not store the\noriginal raw data, but instead stores a "sketch" of it that it can feed into a later computation at query time. Popular\nuse cases for sketches include count-distinct and quantile computation. Each sketch is designed for just one particular\nkind of computation.'}),"\n",(0,t.jsx)(n.p,{children:"In general using sketches serves two main purposes: improving rollup, and reducing memory footprint at\nquery time."}),"\n",(0,t.jsx)(n.p,{children:"Sketches improve rollup ratios because they allow you to collapse multiple distinct values into the same sketch. For\nexample, if you have two rows that are identical except for a user ID (perhaps two users did the same action at the\nsame time), storing them in a count-distinct sketch instead of as-is means you can store the data in one row instead of\ntwo. You won't be able to retrieve the user IDs or compute exact distinct counts, but you'll still be able to compute\napproximate distinct counts, and you'll reduce your storage footprint."}),"\n",(0,t.jsx)(n.p,{children:"Sketches reduce memory footprint at query time because they limit the amount of data that needs to be shuffled between\nservers. For example, in a quantile computation, instead of needing to send all data points to a central location\nso that they can be sorted and the quantile can be computed, Druid instead only needs to send a sketch of the points. This\ncan reduce data transfer needs to mere kilobytes."}),"\n",(0,t.jsxs)(n.p,{children:["For details about the sketches available in Druid, see the\n",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/aggregations#approximate-aggregations",children:"approximate aggregators"})," page."]}),"\n",(0,t.jsxs)(n.p,{children:["If you prefer videos, take a look at ",(0,t.jsx)(n.a,{href:"https://www.youtube.com/watch?v=Hpd3f_MLdXo",children:"Not exactly!"}),", a conference talk\nabout sketches in Druid."]}),"\n",(0,t.jsx)(n.h3,{id:"string-vs-numeric-dimensions",children:"String vs numeric dimensions"}),"\n",(0,t.jsxs)(n.p,{children:["If the user wishes to ingest a column as a numeric-typed dimension (Long, Double or Float), it is necessary to specify the type of the column in the ",(0,t.jsx)(n.code,{children:"dimensions"})," section of the ",(0,t.jsx)(n.code,{children:"dimensionsSpec"}),". If the type is omitted, Druid will ingest a column as the default String type."]}),"\n",(0,t.jsx)(n.p,{children:"There are performance tradeoffs between string and numeric columns. Numeric columns are generally faster to group on\nthan string columns. But unlike string columns, numeric columns don't have indexes, so they can be slower to filter on.\nYou may want to experiment to find the optimal choice for your use case."}),"\n",(0,t.jsxs)(n.p,{children:["For details about how to configure numeric dimensions, see the ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#dimensionsspec",children:(0,t.jsx)(n.code,{children:"dimensionsSpec"})})," documentation."]}),"\n",(0,t.jsx)(n.h3,{id:"secondary-timestamps",children:"Secondary timestamps"}),"\n",(0,t.jsxs)(n.p,{children:["Druid schemas must always include a primary timestamp. The primary timestamp is used for\n",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/partitioning",children:"partitioning and sorting"})," your data, so it should be the timestamp that you will most often filter on.\nDruid is able to rapidly identify and retrieve data corresponding to time ranges of the primary timestamp column."]}),"\n",(0,t.jsxs)(n.p,{children:["If your data has more than one timestamp, you can ingest the others as secondary timestamps. The best way to do this\nis to ingest them as ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#dimensionsspec",children:"long-typed dimensions"})," in milliseconds format.\nIf necessary, you can get them into this format using a ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#transformspec",children:(0,t.jsx)(n.code,{children:"transformSpec"})})," and\n",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/math-expr",children:"expressions"})," like ",(0,t.jsx)(n.code,{children:"timestamp_parse"}),", which returns millisecond timestamps."]}),"\n",(0,t.jsxs)(n.p,{children:["At query time, you can query secondary timestamps with ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/sql-scalar#date-and-time-functions",children:"SQL time functions"}),"\nlike ",(0,t.jsx)(n.code,{children:"MILLIS_TO_TIMESTAMP"}),", ",(0,t.jsx)(n.code,{children:"TIME_FLOOR"}),", and others. If you're using native Druid queries, you can use\n",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/math-expr",children:"expressions"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"nested-dimensions",children:"Nested dimensions"}),"\n",(0,t.jsxs)(n.p,{children:["You can ingest and store nested data in a Druid column as a ",(0,t.jsx)(n.code,{children:"COMPLEX<json>"})," data type. See ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/nested-columns",children:"Nested columns"})," for more information."]}),"\n",(0,t.jsxs)(n.p,{children:["If you want to ingest nested data in a format unsupported by the nested columns feature, you  must use the ",(0,t.jsx)(n.code,{children:"flattenSpec"})," object to flatten it. For example, if you have data of the following form:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{ "foo": { "bar": 3 } }\n'})}),"\n",(0,t.jsx)(n.p,{children:"then before indexing it, you should transform it to:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{ "foo_bar": 3 }\n'})}),"\n",(0,t.jsxs)(n.p,{children:["See the ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#flattenspec",children:(0,t.jsx)(n.code,{children:"flattenSpec"})})," documentation for more details."]}),"\n",(0,t.jsx)("a",{name:"counting"}),"\n",(0,t.jsx)(n.h3,{id:"counting-the-number-of-ingested-events",children:"Counting the number of ingested events"}),"\n",(0,t.jsx)(n.p,{children:"When rollup is enabled, count aggregators at query time do not actually tell you the number of rows that have been\ningested. They tell you the number of rows in the Druid datasource, which may be smaller than the number of rows\ningested."}),"\n",(0,t.jsxs)(n.p,{children:["In this case, a count aggregator at ",(0,t.jsx)(n.em,{children:"ingestion"})," time can be used to count the number of events. However, it is important to note\nthat when you query for this metric, you should use a ",(0,t.jsx)(n.code,{children:"longSum"})," aggregator. A ",(0,t.jsx)(n.code,{children:"count"})," aggregator at query time will return\nthe number of Druid rows for the time interval, which can be used to determine what the roll-up ratio was."]}),"\n",(0,t.jsx)(n.p,{children:"To clarify with an example, if your ingestion spec contains:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'"metricsSpec": [\n    { "type": "count", "name": "count" }\n]\n'})}),"\n",(0,t.jsx)(n.p,{children:"You should query for the number of ingested rows with:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'"aggregations": [\n    { "type": "longSum", "name": "numIngestedEvents", "fieldName": "count" }\n]\n'})}),"\n",(0,t.jsx)(n.h3,{id:"schema-auto-discovery-for-dimensions",children:"Schema auto-discovery for dimensions"}),"\n",(0,t.jsx)(n.p,{children:"Druid can infer the schema for your data in one of two ways:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#type-aware-schema-discovery",children:"Type-aware schema discovery"})," where Druid infers the schema and type for your data. Type-aware schema discovery is available for native batch and streaming ingestion."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"#string-based-schema-discovery",children:"String-based schema discovery"})," where all the discovered columns are typed as either native string or multi-value string columns."]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"type-aware-schema-discovery",children:"Type-aware schema discovery"}),"\n",(0,t.jsx)(n.admonition,{type:"info",children:(0,t.jsx)(n.p,{children:"Note that using type-aware schema discovery can impact downstream BI tools depending on how they handle ARRAY typed columns."})}),"\n",(0,t.jsxs)(n.p,{children:["You can have Druid infer the schema and types for your data partially or fully by setting ",(0,t.jsx)(n.code,{children:"dimensionsSpec.useSchemaDiscovery"})," to ",(0,t.jsx)(n.code,{children:"true"})," and defining some or no dimensions in the dimensions list."]}),"\n",(0,t.jsxs)(n.p,{children:["When performing type-aware schema discovery, Druid can discover all the columns of your input data (that are not present in\nthe exclusion list). Druid automatically chooses the most appropriate native Druid type among ",(0,t.jsx)(n.code,{children:"STRING"}),", ",(0,t.jsx)(n.code,{children:"LONG"}),",\n",(0,t.jsx)(n.code,{children:"DOUBLE"}),", ",(0,t.jsx)(n.code,{children:"ARRAY<STRING>"}),", ",(0,t.jsx)(n.code,{children:"ARRAY<LONG>"}),", ",(0,t.jsx)(n.code,{children:"ARRAY<DOUBLE>"}),", or ",(0,t.jsx)(n.code,{children:"COMPLEX<json>"})," for nested data. For input formats with\nnative boolean types, Druid ingests these values as longs. Array typed columns can be queried using\nthe ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/sql-array-functions",children:"array functions"})," or ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/sql#unnest",children:"UNNEST"}),". Nested\ncolumns can be queried with the ",(0,t.jsx)(n.a,{href:"/docs/33.0.0/querying/sql-json-functions",children:"JSON functions"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Mixed type columns follow the same rules for schema differences between segments, and present as the ",(0,t.jsx)(n.em,{children:"least"})," restrictive\ntype that can represent all values in the column. For example:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Mixed numeric columns are ",(0,t.jsx)(n.code,{children:"DOUBLE"})]}),"\n",(0,t.jsxs)(n.li,{children:["If there are any strings present, then the column is a ",(0,t.jsx)(n.code,{children:"STRING"})]}),"\n",(0,t.jsx)(n.li,{children:"If there are arrays, then the column becomes an array with the least restrictive element type"}),"\n",(0,t.jsxs)(n.li,{children:["Any nested data or arrays of nested data become ",(0,t.jsx)(n.code,{children:"COMPLEX<json>"})," nested columns."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Grouping, filtering, and aggregating mixed type values will handle these columns as if all values are represented as the\nleast restrictive type. The exception to this is the scan query, which will return the values in their original mixed\ntypes, but any downstream operations on these values will still coerce them to the common type."}),"\n",(0,t.jsxs)(n.p,{children:["If you're already using string-based schema discovery and want to migrate, see ",(0,t.jsx)(n.a,{href:"#migrating-to-type-aware-schema-discovery",children:"Migrating to type-aware schema discovery"}),"."]}),"\n",(0,t.jsx)(n.h4,{id:"string-based-schema-discovery",children:"String-based schema discovery"}),"\n",(0,t.jsxs)(n.p,{children:["If you do not set ",(0,t.jsx)(n.code,{children:"dimensionsSpec.useSchemaDiscovery"})," to ",(0,t.jsx)(n.code,{children:"true"}),", Druid can still use the string-based schema discovery for ingestion if any of the following conditions are met:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The dimension list is empty"}),"\n",(0,t.jsxs)(n.li,{children:["You set ",(0,t.jsx)(n.code,{children:"includeAllDimensions"})," to ",(0,t.jsx)(n.code,{children:"true"})]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Druid coerces primitives and arrays of primitive types into the native Druid string type. Nested data structures and arrays of nested data structures are ignored and not ingested."}),"\n",(0,t.jsx)(n.h4,{id:"migrating-to-type-aware-schema-discovery",children:"Migrating to type-aware schema discovery"}),"\n",(0,t.jsx)(n.p,{children:"If you previously used string-based schema discovery and want to migrate to type-aware schema discovery, do the following:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Update any queries that use multi-value dimensions (MVDs) to use UNNEST in conjunction with other functions so that no MVD behavior is being relied upon. Type-aware schema discovery generates ARRAY typed columns instead of MVDs, so queries that use any MVD features will fail."}),"\n",(0,t.jsx)(n.li,{children:"Be aware of mixed typed inputs and test how type-aware schema discovery handles them. Druid attempts to cast them as the least restrictive type."}),"\n",(0,t.jsx)(n.li,{children:"If you notice issues with numeric types, you may need to explicitly cast them. Generally, Druid handles the coercion for you."}),"\n",(0,t.jsx)(n.li,{children:"Update your dimension exclusion list and add any nested columns if you want to continue to exclude them. String-based schema discovery automatically ignores nested columns, but type-aware schema discovery will ingest them."}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"including-the-same-column-as-a-dimension-and-a-metric",children:"Including the same column as a dimension and a metric"}),"\n",(0,t.jsxs)(n.p,{children:["One workflow with unique IDs is to be able to filter on a particular ID, while still being able to do fast unique counts on the ID column.\nIf you are not using schema-less dimensions, this use case is supported by setting the ",(0,t.jsx)(n.code,{children:"name"})," of the metric to something different from the dimension.\nIf you are using schema-less dimensions, the best practice here is to include the same column twice, once as a dimension, and as a ",(0,t.jsx)(n.code,{children:"hyperUnique"})," metric. This may involve\nsome work at ETL time."]}),"\n",(0,t.jsx)(n.p,{children:"As an example, for schema-less dimensions, repeat the same column:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{ "device_id_dim": 123, "device_id_met": 123 }\n'})}),"\n",(0,t.jsxs)(n.p,{children:["and in your ",(0,t.jsx)(n.code,{children:"metricsSpec"}),", include:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-json",children:'{ "type": "hyperUnique", "name": "devices", "fieldName": "device_id_met" }\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"device_id_dim"})," should automatically get picked up as a dimension."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);