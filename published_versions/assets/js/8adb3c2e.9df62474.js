"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[4393],{28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>c});var i=t(96540);const o={},s=i.createContext(o);function r(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(s.Provider,{value:n},e.children)}},37170:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"development/extensions-core/orc","title":"ORC Extension","description":"\x3c!--","source":"@site/docs/33.0.0/development/extensions-core/orc.md","sourceDirName":"development/extensions-core","slug":"/development/extensions-core/orc","permalink":"/docs/33.0.0/development/extensions-core/orc","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"orc","title":"ORC Extension"}}');var o=t(74848),s=t(28453);const r={id:"orc",title:"ORC Extension"},c=void 0,a={},d=[{value:"ORC extension",id:"orc-extension",level:2},{value:"Migration from &#39;contrib&#39; extension",id:"migration-from-contrib-extension",level:3}];function l(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"orc-extension",children:"ORC extension"}),"\n",(0,o.jsx)(n.p,{children:"This Apache Druid extension enables Druid to ingest and understand the Apache ORC data format."}),"\n",(0,o.jsxs)(n.p,{children:["The extension provides the ",(0,o.jsx)(n.a,{href:"/docs/33.0.0/ingestion/data-formats#orc",children:"ORC input format"})," and the ",(0,o.jsx)(n.a,{href:"/docs/33.0.0/ingestion/data-formats#orc-hadoop-parser",children:"ORC Hadoop parser"}),"\nfor ",(0,o.jsx)(n.a,{href:"/docs/33.0.0/ingestion/native-batch",children:"native batch ingestion"})," and ",(0,o.jsx)(n.a,{href:"/docs/33.0.0/ingestion/hadoop",children:"Hadoop batch ingestion"}),", respectively.\nPlease see corresponding docs for details."]}),"\n",(0,o.jsxs)(n.p,{children:["To use this extension, make sure to ",(0,o.jsx)(n.a,{href:"/docs/33.0.0/configuration/extensions#loading-extensions",children:"include"})," ",(0,o.jsx)(n.code,{children:"druid-orc-extensions"})," in the extensions load list."]}),"\n",(0,o.jsx)(n.h3,{id:"migration-from-contrib-extension",children:"Migration from 'contrib' extension"}),"\n",(0,o.jsxs)(n.p,{children:["This extension, first available in version 0.15.0, replaces the previous 'contrib' extension which was available until\n0.14.0-incubating. While this extension can index any data the 'contrib' extension could, the JSON spec for the\ningestion task is ",(0,o.jsx)(n.em,{children:"incompatible"}),", and will need modified to work with the newer 'core' extension."]}),"\n",(0,o.jsx)(n.p,{children:"To migrate to 0.15.0+:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["In ",(0,o.jsx)(n.code,{children:"inputSpec"})," of ",(0,o.jsx)(n.code,{children:"ioConfig"}),", ",(0,o.jsx)(n.code,{children:"inputFormat"})," must be changed from ",(0,o.jsx)(n.code,{children:'"org.apache.hadoop.hive.ql.io.orc.OrcNewInputFormat"'})," to\n",(0,o.jsx)(n.code,{children:'"org.apache.orc.mapreduce.OrcInputFormat"'})]}),"\n",(0,o.jsxs)(n.li,{children:["The 'contrib' extension supported a ",(0,o.jsx)(n.code,{children:"typeString"})," property, which provided the schema of the\nORC file, of which was essentially required to have the types correct, but notably ",(0,o.jsx)(n.em,{children:"not"})," the column names, which\nfacilitated column renaming. In the 'core' extension, column renaming can be achieved with\n",(0,o.jsx)(n.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#flattenspec",children:(0,o.jsx)(n.code,{children:"flattenSpec"})}),". For example, ",(0,o.jsx)(n.code,{children:'"typeString":"struct<time:string,name:string>"'}),"\nwith the actual schema ",(0,o.jsx)(n.code,{children:"struct<_col0:string,_col1:string>"}),", to preserve Druid schema would need replaced with:"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'"flattenSpec": {\n  "fields": [\n    {\n      "type": "path",\n      "name": "time",\n      "expr": "$._col0"\n    },\n    {\n      "type": "path",\n      "name": "name",\n      "expr": "$._col1"\n    }\n  ]\n  ...\n}\n'})}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["The 'contrib' extension supported a ",(0,o.jsx)(n.code,{children:"mapFieldNameFormat"})," property, which provided a way to specify a dimension to\nflatten ",(0,o.jsx)(n.code,{children:"OrcMap"})," columns with primitive types. This functionality has also been replaced with\n",(0,o.jsx)(n.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#flattenspec",children:(0,o.jsx)(n.code,{children:"flattenSpec"})}),". For example: ",(0,o.jsx)(n.code,{children:'"mapFieldNameFormat": "<PARENT>_<CHILD>"'}),"\nfor a dimension ",(0,o.jsx)(n.code,{children:"nestedData_dim1"}),", to preserve Druid schema could be replaced with"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-json",children:'"flattenSpec": {\n "fields": [\n   {\n     "type": "path",\n     "name": "nestedData_dim1",\n     "expr": "$.nestedData.dim1"\n   }\n ]\n ...\n}\n'})})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}}}]);