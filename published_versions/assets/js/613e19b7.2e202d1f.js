"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[8135],{28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(96540);const i={},d=s.createContext(i);function r(e){const n=s.useContext(d);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(d.Provider,{value:n},e.children)}},97571:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"ingestion/native-batch-simple-task","title":"JSON-based batch simple task indexing","description":"\x3c!--","source":"@site/docs/latest/ingestion/native-batch-simple-task.md","sourceDirName":"ingestion","slug":"/ingestion/native-batch-simple-task","permalink":"/docs/latest/ingestion/native-batch-simple-task","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"native-batch-simple-task","title":"JSON-based batch simple task indexing","sidebar_label":"JSON-based batch (simple)"}}');var i=t(74848),d=t(28453);const r={id:"native-batch-simple-task",title:"JSON-based batch simple task indexing",sidebar_label:"JSON-based batch (simple)"},a=void 0,o={},l=[{value:"Simple task example",id:"simple-task-example",level:2},{value:"Simple task configuration",id:"simple-task-configuration",level:2},{value:"<code>dataSchema</code>",id:"dataschema",level:3},{value:"<code>ioConfig</code>",id:"ioconfig",level:3},{value:"<code>tuningConfig</code>",id:"tuningconfig",level:3},{value:"<code>partitionsSpec</code>",id:"partitionsspec",level:3},{value:"Segment pushing modes",id:"segment-pushing-modes",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,d.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsxs)(n.p,{children:["This page describes native batch ingestion using ",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/ingestion-spec",children:"ingestion specs"}),". Refer to the ",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/#batch",children:"ingestion\nmethods"})," table to determine which ingestion method is right for you."]})}),"\n",(0,i.jsxs)(n.p,{children:["The simple task (",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/tasks",children:"task type"})," ",(0,i.jsx)(n.code,{children:"index"}),") executes single-threaded as a single task within the indexing service. For parallel, scalable options consider using ",(0,i.jsxs)(n.a,{href:"/docs/latest/ingestion/native-batch",children:[(0,i.jsx)(n.code,{children:"index_parallel"})," tasks"]})," or ",(0,i.jsx)(n.a,{href:"/docs/latest/multi-stage-query/",children:"SQL-based batch ingestion"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"simple-task-example",children:"Simple task example"}),"\n",(0,i.jsx)(n.p,{children:"A sample task is shown below:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-json",children:'{\n  "type" : "index",\n  "spec" : {\n    "dataSchema" : {\n      "dataSource" : "wikipedia",\n      "timestampSpec" : {\n        "column" : "timestamp",\n        "format" : "auto"\n      },\n      "dimensionsSpec" : {\n        "dimensions": ["country", "page","language","user","unpatrolled","newPage","robot","anonymous","namespace","continent","region","city"],\n        "dimensionExclusions" : []\n      },\n      "metricsSpec" : [\n        {\n          "type" : "count",\n          "name" : "count"\n        },\n        {\n          "type" : "doubleSum",\n          "name" : "added",\n          "fieldName" : "added"\n        },\n        {\n          "type" : "doubleSum",\n          "name" : "deleted",\n          "fieldName" : "deleted"\n        },\n        {\n          "type" : "doubleSum",\n          "name" : "delta",\n          "fieldName" : "delta"\n        }\n      ],\n      "granularitySpec" : {\n        "type" : "uniform",\n        "segmentGranularity" : "DAY",\n        "queryGranularity" : "NONE",\n        "intervals" : [ "2013-08-31/2013-09-01" ]\n      }\n    },\n    "ioConfig" : {\n      "type" : "index",\n      "inputSource" : {\n        "type" : "local",\n        "baseDir" : "examples/indexing/",\n        "filter" : "wikipedia_data.json"\n       },\n       "inputFormat": {\n         "type": "json"\n       }\n    },\n    "tuningConfig" : {\n      "type" : "index",\n      "partitionsSpec": {\n        "type": "hashed",\n        "partitionDimensions": ["country"],\n        "targetRowsPerSegment": 5000000\n      }\n    }\n  }\n}\n'})}),"\n",(0,i.jsx)(n.h2,{id:"simple-task-configuration",children:"Simple task configuration"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"property"}),(0,i.jsx)(n.th,{children:"description"}),(0,i.jsx)(n.th,{children:"required?"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"type"}),(0,i.jsxs)(n.td,{children:["The task type, this should always be ",(0,i.jsx)(n.code,{children:"index"}),"."]}),(0,i.jsx)(n.td,{children:"yes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"id"}),(0,i.jsx)(n.td,{children:"The task ID. If this is not explicitly specified, Druid generates the task ID using task type, data source name, interval, and date-time stamp."}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"spec"}),(0,i.jsxs)(n.td,{children:["The ingestion spec including the ",(0,i.jsx)(n.a,{href:"#dataschema",children:"data schema"}),", ",(0,i.jsx)(n.a,{href:"#ioconfig",children:"IO config"}),", and ",(0,i.jsx)(n.a,{href:"#tuningconfig",children:"tuning config"}),"."]}),(0,i.jsx)(n.td,{children:"yes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"context"}),(0,i.jsxs)(n.td,{children:["Context to specify various task configuration parameters. See ",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/tasks#context-parameters",children:"Task context parameters"})," for more details."]}),(0,i.jsx)(n.td,{children:"no"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"dataschema",children:(0,i.jsx)(n.code,{children:"dataSchema"})}),"\n",(0,i.jsx)(n.p,{children:"This field is required."}),"\n",(0,i.jsxs)(n.p,{children:["See the ",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/ingestion-spec#dataschema",children:(0,i.jsx)(n.code,{children:"dataSchema"})})," section of the ingestion docs for details."]}),"\n",(0,i.jsxs)(n.p,{children:["If you do not specify ",(0,i.jsx)(n.code,{children:"intervals"})," explicitly in your dataSchema's granularitySpec, the Local Index Task will do an extra\npass over the data to determine the range to lock when it starts up.  If you specify ",(0,i.jsx)(n.code,{children:"intervals"})," explicitly, any rows\noutside the specified intervals will be thrown away. We recommend setting ",(0,i.jsx)(n.code,{children:"intervals"})," explicitly if you know the time\nrange of the data because it allows the task to skip the extra pass, and so that you don't accidentally replace data outside\nthat range if there's some stray data with unexpected timestamps."]}),"\n",(0,i.jsx)(n.h3,{id:"ioconfig",children:(0,i.jsx)(n.code,{children:"ioConfig"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"property"}),(0,i.jsx)(n.th,{children:"description"}),(0,i.jsx)(n.th,{children:"default"}),(0,i.jsx)(n.th,{children:"required?"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"type"}),(0,i.jsx)(n.td,{children:'The task type, this should always be "index".'}),(0,i.jsx)(n.td,{children:"none"}),(0,i.jsx)(n.td,{children:"yes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"inputFormat"}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/data-formats#input-format",children:(0,i.jsx)(n.code,{children:"inputFormat"})})," to specify how to parse input data."]}),(0,i.jsx)(n.td,{children:"none"}),(0,i.jsx)(n.td,{children:"yes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"appendToExisting"}),(0,i.jsxs)(n.td,{children:["Creates segments as additional shards of the latest version, effectively appending to the segment set instead of replacing it. This means that you can append new segments to any datasource regardless of its original partitioning scheme. You must use the ",(0,i.jsx)(n.code,{children:"dynamic"})," partitioning type for the appended segments. If you specify a different partitioning type, the task fails with an error."]}),(0,i.jsx)(n.td,{children:"false"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"dropExisting"}),(0,i.jsxs)(n.td,{children:["If this setting is ",(0,i.jsx)(n.code,{children:"false"})," then ingestion proceeds as usual. Set this to ",(0,i.jsx)(n.code,{children:"true"})," and ",(0,i.jsx)(n.code,{children:"appendToExisting"})," to ",(0,i.jsx)(n.code,{children:"false"}),' to enforce true "replace" functionality as described next. If ',(0,i.jsx)(n.code,{children:"true"})," and ",(0,i.jsx)(n.code,{children:"appendToExisting"})," is ",(0,i.jsx)(n.code,{children:"false"})," and the ",(0,i.jsx)(n.code,{children:"granularitySpec"})," contains at least one",(0,i.jsx)(n.code,{children:"interval"}),", then the ingestion task will create regular segments for time chunk intervals with input data and ",(0,i.jsx)(n.code,{children:"tombstones"}),' for all other time chunks with no data. The task will publish the data segments and the tombstone segments together when the it publishes new segments. The net effect of the data segments and the tombstones is to completely adhere to a "replace" semantics where the  input data contained in the ',(0,i.jsx)(n.code,{children:"granularitySpec"})," intervals replaces all existing data in the intervals even for time chunks that would be empty in the case that no input data was associated with them. In the extreme case when the input data set that falls in the ",(0,i.jsx)(n.code,{children:"granularitySpec"})," intervals is empty all existing data in the interval will be replaced with an empty data set (i.e. with nothing -- all existing data will be covered by ",(0,i.jsx)(n.code,{children:"tombstones"}),"). If ingestion fails, no segments and tombstones will be published. The following two combinations are not supported and will make the ingestion fail with an error: ",(0,i.jsx)(n.code,{children:"dropExisting"})," is ",(0,i.jsx)(n.code,{children:"true"})," and ",(0,i.jsx)(n.code,{children:"interval"})," is not specified in ",(0,i.jsx)(n.code,{children:"granularitySpec"})," or  ",(0,i.jsx)(n.code,{children:"appendToExisting"})," is true and ",(0,i.jsx)(n.code,{children:"dropExisting"})," is ",(0,i.jsx)(n.code,{children:"true"}),". WARNING: this functionality is still in beta and even though we are not aware of any bugs, use with caution."]}),(0,i.jsx)(n.td,{children:"false"}),(0,i.jsx)(n.td,{children:"no"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"tuningconfig",children:(0,i.jsx)(n.code,{children:"tuningConfig"})}),"\n",(0,i.jsx)(n.p,{children:"The tuningConfig is optional and default parameters will be used if no tuningConfig is specified. See below for more details."}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"property"}),(0,i.jsx)(n.th,{children:"description"}),(0,i.jsx)(n.th,{children:"default"}),(0,i.jsx)(n.th,{children:"required?"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"type"}),(0,i.jsx)(n.td,{children:'The task type, this should always be "index".'}),(0,i.jsx)(n.td,{children:"none"}),(0,i.jsx)(n.td,{children:"yes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"maxRowsInMemory"}),(0,i.jsx)(n.td,{children:"Used in determining when intermediate persists to disk should occur. Normally user does not need to set this, but depending on the nature of data, if rows are short in terms of bytes, user may not want to store a million rows in memory and this value should be set."}),(0,i.jsx)(n.td,{children:"1000000"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"maxBytesInMemory"}),(0,i.jsxs)(n.td,{children:["Used in determining when intermediate persists to disk should occur. Normally this is computed internally and user does not need to set it. This value represents number of bytes to aggregate in heap memory before persisting. This is based on a rough estimate of memory usage and not actual usage. The maximum heap memory usage for indexing is maxBytesInMemory * (2 + maxPendingPersists). Note that ",(0,i.jsx)(n.code,{children:"maxBytesInMemory"})," also includes heap usage of artifacts created from intermediary persists. This means that after every persist, the amount of ",(0,i.jsx)(n.code,{children:"maxBytesInMemory"})," until next persist will decreases, and task will fail when the sum of bytes of all intermediary persisted artifacts exceeds ",(0,i.jsx)(n.code,{children:"maxBytesInMemory"}),"."]}),(0,i.jsx)(n.td,{children:"1/6 of max JVM memory"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"maxTotalRows"}),(0,i.jsxs)(n.td,{children:["Deprecated. Use ",(0,i.jsx)(n.code,{children:"partitionsSpec"})," instead. Total number of rows in segments waiting for being pushed. Used in determining when intermediate pushing should occur."]}),(0,i.jsx)(n.td,{children:"20000000"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"numShards"}),(0,i.jsxs)(n.td,{children:["Deprecated. Use ",(0,i.jsx)(n.code,{children:"partitionsSpec"})," instead. Directly specify the number of shards to create. If this is specified and ",(0,i.jsx)(n.code,{children:"intervals"})," is specified in the ",(0,i.jsx)(n.code,{children:"granularitySpec"}),", the index task can skip the determine intervals/partitions pass through the data."]}),(0,i.jsx)(n.td,{children:"null"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"partitionDimensions"}),(0,i.jsxs)(n.td,{children:["Deprecated. Use ",(0,i.jsx)(n.code,{children:"partitionsSpec"})," instead. The dimensions to partition on. Leave blank to select all dimensions. Only used with ",(0,i.jsx)(n.code,{children:"forceGuaranteedRollup"})," = true, will be ignored otherwise."]}),(0,i.jsx)(n.td,{children:"null"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"partitionsSpec"}),(0,i.jsxs)(n.td,{children:["Defines how to partition data in each timeChunk, see ",(0,i.jsx)(n.a,{href:"#partitionsspec",children:"PartitionsSpec"})]}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"dynamic"})," if ",(0,i.jsx)(n.code,{children:"forceGuaranteedRollup"})," = false, ",(0,i.jsx)(n.code,{children:"hashed"})," if ",(0,i.jsx)(n.code,{children:"forceGuaranteedRollup"})," = true"]}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"indexSpec"}),(0,i.jsxs)(n.td,{children:["Defines segment storage format options to be used at indexing time, see ",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/ingestion-spec#indexspec",children:"IndexSpec"})]}),(0,i.jsx)(n.td,{children:"null"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"indexSpecForIntermediatePersists"}),(0,i.jsxs)(n.td,{children:["Defines segment storage format options to be used at indexing time for intermediate persisted temporary segments. This can be used to disable dimension/metric compression on intermediate segments to reduce memory required for final merging. However, disabling compression on intermediate segments might increase page cache use while they are used before getting merged into final segment published, see ",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/ingestion-spec#indexspec",children:"IndexSpec"})," for possible values."]}),(0,i.jsx)(n.td,{children:"same as indexSpec"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"maxPendingPersists"}),(0,i.jsx)(n.td,{children:"Maximum number of persists that can be pending but not started. If this limit would be exceeded by a new intermediate persist, ingestion will block until the currently-running persist finishes. Maximum heap memory usage for indexing scales with maxRowsInMemory * (2 + maxPendingPersists)."}),(0,i.jsx)(n.td,{children:"0 (meaning one persist can be running concurrently with ingestion, and none can be queued up)"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"forceGuaranteedRollup"}),(0,i.jsxs)(n.td,{children:["Forces guaranteeing the ",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/rollup",children:"perfect rollup"}),". The perfect rollup optimizes the total size of generated segments and querying time while indexing time will be increased. If this is set to true, the index task will read the entire input data twice: one for finding the optimal number of partitions per time chunk and one for generating segments. Note that the result segments would be hash-partitioned. This flag cannot be used with ",(0,i.jsx)(n.code,{children:"appendToExisting"})," of IOConfig. For more details, see the below ",(0,i.jsx)(n.strong,{children:"Segment pushing modes"})," section."]}),(0,i.jsx)(n.td,{children:"false"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"reportParseExceptions"}),(0,i.jsxs)(n.td,{children:["DEPRECATED. If true, exceptions encountered during parsing will be thrown and will halt ingestion; if false, unparseable rows and fields will be skipped. Setting ",(0,i.jsx)(n.code,{children:"reportParseExceptions"})," to true will override existing configurations for ",(0,i.jsx)(n.code,{children:"maxParseExceptions"})," and ",(0,i.jsx)(n.code,{children:"maxSavedParseExceptions"}),", setting ",(0,i.jsx)(n.code,{children:"maxParseExceptions"})," to 0 and limiting ",(0,i.jsx)(n.code,{children:"maxSavedParseExceptions"})," to no more than 1."]}),(0,i.jsx)(n.td,{children:"false"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"pushTimeout"}),(0,i.jsx)(n.td,{children:"Milliseconds to wait for pushing segments. It must be >= 0, where 0 means to wait forever."}),(0,i.jsx)(n.td,{children:"0"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"segmentWriteOutMediumFactory"}),(0,i.jsxs)(n.td,{children:["Segment write-out medium to use when creating segments. See ",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/native-batch#segmentwriteoutmediumfactory",children:"SegmentWriteOutMediumFactory"}),"."]}),(0,i.jsxs)(n.td,{children:["Not specified, the value from ",(0,i.jsx)(n.code,{children:"druid.peon.defaultSegmentWriteOutMediumFactory.type"})," is used"]}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"logParseExceptions"}),(0,i.jsx)(n.td,{children:"If true, log an error message when a parsing exception occurs, containing information about the row where the error occurred."}),(0,i.jsx)(n.td,{children:"false"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"maxParseExceptions"}),(0,i.jsxs)(n.td,{children:["The maximum number of parse exceptions that can occur before the task halts ingestion and fails. Overridden if ",(0,i.jsx)(n.code,{children:"reportParseExceptions"})," is set."]}),(0,i.jsx)(n.td,{children:"unlimited"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"maxSavedParseExceptions"}),(0,i.jsxs)(n.td,{children:['When a parse exception occurs, Druid can keep track of the most recent parse exceptions. "maxSavedParseExceptions" limits how many exception instances will be saved. These saved exceptions will be made available after the task finishes in the ',(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/tasks#task-reports",children:"task completion report"}),". Overridden if ",(0,i.jsx)(n.code,{children:"reportParseExceptions"})," is set."]}),(0,i.jsx)(n.td,{children:"0"}),(0,i.jsx)(n.td,{children:"no"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"partitionsspec",children:(0,i.jsx)(n.code,{children:"partitionsSpec"})}),"\n",(0,i.jsxs)(n.p,{children:["PartitionsSpec is to describe the secondary partitioning method.\nYou should use different partitionsSpec depending on the ",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/rollup",children:"rollup mode"})," you want.\nFor perfect rollup, you should use ",(0,i.jsx)(n.code,{children:"hashed"}),"."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"property"}),(0,i.jsx)(n.th,{children:"description"}),(0,i.jsx)(n.th,{children:"default"}),(0,i.jsx)(n.th,{children:"required?"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"type"}),(0,i.jsxs)(n.td,{children:["This should always be ",(0,i.jsx)(n.code,{children:"hashed"})]}),(0,i.jsx)(n.td,{children:"none"}),(0,i.jsx)(n.td,{children:"yes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"maxRowsPerSegment"}),(0,i.jsx)(n.td,{children:"Used in sharding. Determines how many rows are in each segment."}),(0,i.jsx)(n.td,{children:"5000000"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"numShards"}),(0,i.jsxs)(n.td,{children:["Directly specify the number of shards to create. If this is specified and ",(0,i.jsx)(n.code,{children:"intervals"})," is specified in the ",(0,i.jsx)(n.code,{children:"granularitySpec"}),", the index task can skip the determine intervals/partitions pass through the data. ",(0,i.jsx)(n.code,{children:"numShards"})," cannot be specified if ",(0,i.jsx)(n.code,{children:"maxRowsPerSegment"})," is set."]}),(0,i.jsx)(n.td,{children:"null"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"partitionDimensions"}),(0,i.jsx)(n.td,{children:"The dimensions to partition on. Leave blank to select all dimensions."}),(0,i.jsx)(n.td,{children:"null"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"partitionFunction"}),(0,i.jsxs)(n.td,{children:["A function to compute hash of partition dimensions. See ",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/native-batch#hash-partition-function",children:"Hash partition function"})]}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"murmur3_32_abs"})}),(0,i.jsx)(n.td,{children:"no"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:["For best-effort rollup, you should use ",(0,i.jsx)(n.code,{children:"dynamic"}),"."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"property"}),(0,i.jsx)(n.th,{children:"description"}),(0,i.jsx)(n.th,{children:"default"}),(0,i.jsx)(n.th,{children:"required?"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"type"}),(0,i.jsxs)(n.td,{children:["This should always be ",(0,i.jsx)(n.code,{children:"dynamic"})]}),(0,i.jsx)(n.td,{children:"none"}),(0,i.jsx)(n.td,{children:"yes"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"maxRowsPerSegment"}),(0,i.jsx)(n.td,{children:"Used in sharding. Determines how many rows are in each segment."}),(0,i.jsx)(n.td,{children:"5000000"}),(0,i.jsx)(n.td,{children:"no"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"maxTotalRows"}),(0,i.jsx)(n.td,{children:"Total number of rows in segments waiting for being pushed."}),(0,i.jsx)(n.td,{children:"20000000"}),(0,i.jsx)(n.td,{children:"no"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"segment-pushing-modes",children:"Segment pushing modes"}),"\n",(0,i.jsxs)(n.p,{children:["While ingesting data using the simple task indexing, Druid creates segments from the input data and pushes them. For segment pushing,\nthe simple task index supports the following segment pushing modes based upon your type of ",(0,i.jsx)(n.a,{href:"/docs/latest/ingestion/rollup",children:"rollup"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Bulk pushing mode: Used for perfect rollup. Druid pushes every segment at the very end of the index task. Until then, Druid stores created segments in memory and local storage of the service running the index task. This mode can cause problems if you have limited storage capacity, and is not recommended to use in production.\nTo enable bulk pushing mode, set ",(0,i.jsx)(n.code,{children:"forceGuaranteedRollup"})," in your TuningConfig. You can not use bulk pushing with ",(0,i.jsx)(n.code,{children:"appendToExisting"})," in your IOConfig."]}),"\n",(0,i.jsxs)(n.li,{children:["Incremental pushing mode: Used for best-effort rollup. Druid pushes segments are incrementally during the course of the indexing task. The index task collects data and stores created segments in the memory and disks of the services running the task until the total number of collected rows exceeds ",(0,i.jsx)(n.code,{children:"maxTotalRows"}),". At that point the index task immediately pushes all segments created up until that moment, cleans up pushed segments, and continues to ingest the remaining data."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,d.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}}}]);