"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[9271],{1769:(e,r,s)=>{s.r(r),s.d(r,{assets:()=>a,contentTitle:()=>t,default:()=>p,frontMatter:()=>n,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"tutorials/tutorial-kerberos-hadoop","title":"Configure Apache Druid to use Kerberized Apache Hadoop as deep storage","description":"\x3c!--","source":"@site/docs/32.0.0/tutorials/tutorial-kerberos-hadoop.md","sourceDirName":"tutorials","slug":"/tutorials/tutorial-kerberos-hadoop","permalink":"/docs/32.0.0/tutorials/tutorial-kerberos-hadoop","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"tutorial-kerberos-hadoop","title":"Configure Apache Druid to use Kerberized Apache Hadoop as deep storage","sidebar_label":"Kerberized HDFS deep storage"},"sidebar":"docs","previous":{"title":"Load from Apache Hadoop","permalink":"/docs/32.0.0/tutorials/tutorial-batch-hadoop"},"next":{"title":"Architecture","permalink":"/docs/32.0.0/design/architecture"}}');var d=s(74848),i=s(28453);const n={id:"tutorial-kerberos-hadoop",title:"Configure Apache Druid to use Kerberized Apache Hadoop as deep storage",sidebar_label:"Kerberized HDFS deep storage"},t=void 0,a={},l=[{value:"Hadoop Setup",id:"hadoop-setup",level:2},{value:"HDFS Folders and permissions",id:"hdfs-folders-and-permissions",level:3},{value:"Druid Setup",id:"druid-setup",level:2},{value:"common.runtime.properties",id:"commonruntimeproperties",level:3},{value:"Hadoop Jars",id:"hadoop-jars",level:3},{value:"Kerberos setup",id:"kerberos-setup",level:3},{value:"Restart Druid Services",id:"restart-druid-services",level:3}];function c(e){const r={code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",...(0,i.R)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(r.h2,{id:"hadoop-setup",children:"Hadoop Setup"}),"\n",(0,d.jsx)(r.p,{children:"Following are the configurations files required to be copied over to Druid conf folders:"}),"\n",(0,d.jsxs)(r.ol,{children:["\n",(0,d.jsx)(r.li,{children:"For HDFS as a deep storage, hdfs-site.xml, core-site.xml"}),"\n",(0,d.jsx)(r.li,{children:"For ingestion, mapred-site.xml, yarn-site.xml"}),"\n"]}),"\n",(0,d.jsx)(r.h3,{id:"hdfs-folders-and-permissions",children:"HDFS Folders and permissions"}),"\n",(0,d.jsxs)(r.ol,{children:["\n",(0,d.jsxs)(r.li,{children:["\n",(0,d.jsx)(r.p,{children:"Choose any folder name for the druid deep storage, for example 'druid'"}),"\n"]}),"\n",(0,d.jsxs)(r.li,{children:["\n",(0,d.jsxs)(r.p,{children:["Create the folder in hdfs under the required parent folder. For example,\n",(0,d.jsx)(r.code,{children:"hdfs dfs -mkdir /druid"}),"\nOR\n",(0,d.jsx)(r.code,{children:"hdfs dfs -mkdir /apps/druid"})]}),"\n"]}),"\n",(0,d.jsxs)(r.li,{children:["\n",(0,d.jsx)(r.p,{children:"Give druid processes appropriate permissions for the druid processes to access this folder. This would ensure that druid is able to create necessary folders like data and indexing_log in HDFS.\nFor example, if druid processes run as user 'root', then"}),"\n",(0,d.jsx)(r.p,{children:(0,d.jsx)(r.code,{children:"hdfs dfs -chown root:root /apps/druid"})}),"\n",(0,d.jsx)(r.p,{children:"OR"}),"\n",(0,d.jsx)(r.p,{children:(0,d.jsx)(r.code,{children:"hdfs dfs -chmod 777 /apps/druid"})}),"\n"]}),"\n"]}),"\n",(0,d.jsx)(r.p,{children:"Druid creates necessary sub-folders to store data and index under this newly created folder."}),"\n",(0,d.jsx)(r.h2,{id:"druid-setup",children:"Druid Setup"}),"\n",(0,d.jsx)(r.p,{children:"Edit common.runtime.properties at conf/druid/_common/common.runtime.properties to include the HDFS properties. Folders used for the location are same as the ones used for example above."}),"\n",(0,d.jsx)(r.h3,{id:"commonruntimeproperties",children:"common.runtime.properties"}),"\n",(0,d.jsx)(r.pre,{children:(0,d.jsx)(r.code,{className:"language-properties",children:"# Deep storage\n#\n# For HDFS:\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n# OR\n# druid.storage.storageDirectory=/apps/druid/segments\n\n#\n# Indexing service logs\n#\n\n# For HDFS:\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n# OR\n# druid.storage.storageDirectory=/apps/druid/indexing-logs\n"})}),"\n",(0,d.jsx)(r.p,{children:"Note: Comment out Local storage and S3 Storage parameters in the file"}),"\n",(0,d.jsxs)(r.p,{children:["Also include hdfs-storage core extension to ",(0,d.jsx)(r.code,{children:"conf/druid/_common/common.runtime.properties"})]}),"\n",(0,d.jsx)(r.pre,{children:(0,d.jsx)(r.code,{className:"language-properties",children:'#\n# Extensions\n#\n\ndruid.extensions.directory=dist/druid/extensions\ndruid.extensions.hadoopDependenciesDir=dist/druid/hadoop-dependencies\ndruid.extensions.loadList=["mysql-metadata-storage", "druid-hdfs-storage", "druid-kerberos"]\n'})}),"\n",(0,d.jsx)(r.h3,{id:"hadoop-jars",children:"Hadoop Jars"}),"\n",(0,d.jsx)(r.p,{children:"Ensure that Druid has necessary jars to support the Hadoop version."}),"\n",(0,d.jsxs)(r.p,{children:["Find the hadoop version using command, ",(0,d.jsx)(r.code,{children:"hadoop version"})]}),"\n",(0,d.jsxs)(r.p,{children:["In case there is other software used with hadoop, like ",(0,d.jsx)(r.code,{children:"WanDisco"}),", ensure that"]}),"\n",(0,d.jsxs)(r.ol,{children:["\n",(0,d.jsx)(r.li,{children:"the necessary libraries are available"}),"\n",(0,d.jsxs)(r.li,{children:["add the requisite extensions to ",(0,d.jsx)(r.code,{children:"druid.extensions.loadlist"})," in ",(0,d.jsx)(r.code,{children:"conf/druid/_common/common.runtime.properties"})]}),"\n"]}),"\n",(0,d.jsx)(r.h3,{id:"kerberos-setup",children:"Kerberos setup"}),"\n",(0,d.jsx)(r.p,{children:"Create a headless keytab which would have access to the druid data and index."}),"\n",(0,d.jsx)(r.p,{children:"Edit conf/druid/_common/common.runtime.properties and add the following properties:"}),"\n",(0,d.jsx)(r.pre,{children:(0,d.jsx)(r.code,{className:"language-properties",children:"druid.hadoop.security.kerberos.principal\ndruid.hadoop.security.kerberos.keytab\n"})}),"\n",(0,d.jsx)(r.p,{children:"For example"}),"\n",(0,d.jsx)(r.pre,{children:(0,d.jsx)(r.code,{className:"language-properties",children:"druid.hadoop.security.kerberos.principal=hdfs-test@EXAMPLE.IO\ndruid.hadoop.security.kerberos.keytab=/etc/security/keytabs/hdfs.headless.keytab\n"})}),"\n",(0,d.jsx)(r.h3,{id:"restart-druid-services",children:"Restart Druid Services"}),"\n",(0,d.jsx)(r.p,{children:"With the above changes, restart Druid. This would ensure that Druid works with Kerberized Hadoop"})]})}function p(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,d.jsx)(r,{...e,children:(0,d.jsx)(c,{...e})}):c(e)}},28453:(e,r,s)=>{s.d(r,{R:()=>n,x:()=>t});var o=s(96540);const d={},i=o.createContext(d);function n(e){const r=o.useContext(i);return o.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function t(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:n(e.components),o.createElement(i.Provider,{value:r},e.children)}}}]);