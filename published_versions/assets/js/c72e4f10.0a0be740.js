"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[8235],{28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>s});var t=i(96540);const r={},o=t.createContext(r);function a(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),t.createElement(o.Provider,{value:n},e.children)}},40420:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"development/overview","title":"Developing on Apache Druid","description":"\x3c!--","source":"@site/docs/33.0.0/development/overview.md","sourceDirName":"development","slug":"/development/overview","permalink":"/docs/33.0.0/development/overview","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"overview","title":"Developing on Apache Druid","sidebar_label":"Developing on Druid"},"sidebar":"docs","previous":{"title":"Content for build.sbt","permalink":"/docs/33.0.0/operations/use_sbt_to_build_fat_jar"},"next":{"title":"Creating extensions","permalink":"/docs/33.0.0/development/modules"}}');var r=i(74848),o=i(28453);const a={id:"overview",title:"Developing on Apache Druid",sidebar_label:"Developing on Druid"},s=void 0,d={},l=[{value:"Storage format",id:"storage-format",level:2},{value:"Segment creation",id:"segment-creation",level:2},{value:"Storage engine",id:"storage-engine",level:2},{value:"Query engine",id:"query-engine",level:2},{value:"Coordination",id:"coordination",level:2},{value:"Real-time Ingestion",id:"real-time-ingestion",level:2},{value:"Native Batch Ingestion",id:"native-batch-ingestion",level:2},{value:"Hadoop-based Batch Ingestion",id:"hadoop-based-batch-ingestion",level:2},{value:"Internal UIs",id:"internal-uis",level:2},{value:"Client libraries",id:"client-libraries",level:2}];function c(e){const n={a:"a",code:"code",h2:"h2",p:"p",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Druid's codebase consists of several major components. For developers interested in learning the code, this document provides\na high level overview of the main components that make up Druid and the relevant classes to start from to learn the code."}),"\n",(0,r.jsx)(n.h2,{id:"storage-format",children:"Storage format"}),"\n",(0,r.jsxs)(n.p,{children:["Data in Druid is stored in a custom column format known as a ",(0,r.jsx)(n.a,{href:"/docs/33.0.0/design/segments",children:"segment"}),". Segments are composed of\ndifferent types of columns. ",(0,r.jsx)(n.code,{children:"Column.java"})," and the classes that extend it is a great place to looking into the storage format."]}),"\n",(0,r.jsx)(n.h2,{id:"segment-creation",children:"Segment creation"}),"\n",(0,r.jsxs)(n.p,{children:["Raw data is ingested in ",(0,r.jsx)(n.code,{children:"IncrementalIndex.java"}),", and segments are created in ",(0,r.jsx)(n.code,{children:"IndexMerger.java"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"storage-engine",children:"Storage engine"}),"\n",(0,r.jsxs)(n.p,{children:["Druid segments are memory mapped in ",(0,r.jsx)(n.code,{children:"IndexIO.java"})," to be exposed for querying."]}),"\n",(0,r.jsx)(n.h2,{id:"query-engine",children:"Query engine"}),"\n",(0,r.jsxs)(n.p,{children:["Most of the logic related to Druid queries can be found in the Query* classes. Druid leverages query runners to run queries.\nQuery runners often embed other query runners and each query runner adds on a layer of logic. A good starting point to trace\nthe query logic is to start from ",(0,r.jsx)(n.code,{children:"QueryResource.java"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"coordination",children:"Coordination"}),"\n",(0,r.jsxs)(n.p,{children:["Most of the coordination logic for Historical processes is on the Druid Coordinator. The starting point here is ",(0,r.jsx)(n.code,{children:"DruidCoordinator.java"}),".\nMost of the coordination logic for (real-time) ingestion is in the Druid indexing service. The starting point here is ",(0,r.jsx)(n.code,{children:"OverlordResource.java"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"real-time-ingestion",children:"Real-time Ingestion"}),"\n",(0,r.jsxs)(n.p,{children:["Druid streaming tasks are based on the 'seekable stream' classes such as ",(0,r.jsx)(n.code,{children:"SeekableStreamSupervisor.java"}),",\n",(0,r.jsx)(n.code,{children:"SeekableStreamIndexTask.java"}),", and ",(0,r.jsx)(n.code,{children:"SeekableStreamIndexTaskRunner.java"}),". The data processing happens through\n",(0,r.jsx)(n.code,{children:"StreamAppenderator.java"}),", and the persist and hand-off logic is in ",(0,r.jsx)(n.code,{children:"StreamAppenderatorDriver.java"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"native-batch-ingestion",children:"Native Batch Ingestion"}),"\n",(0,r.jsxs)(n.p,{children:["Druid native batch ingestion main task types are based on ",(0,r.jsx)(n.code,{children:"AbstractBatchTask.java"})," and ",(0,r.jsx)(n.code,{children:"AbstractBatchSubtask.java"}),".\nParallel processing uses ",(0,r.jsx)(n.code,{children:"ParallelIndexSupervisorTask.java"}),", which spawns subtasks to perform various operations such\nas data analysis and partitioning depending on the task specification. Segment generation happens in\n",(0,r.jsx)(n.code,{children:"SinglePhaseSubTask.java"}),", ",(0,r.jsx)(n.code,{children:"PartialHashSegmentGenerateTask.java"}),", or ",(0,r.jsx)(n.code,{children:"PartialRangeSegmentGenerateTask.java"})," through\n",(0,r.jsx)(n.code,{children:"BatchAppenderator"}),", and the persist and hand-off logic is in ",(0,r.jsx)(n.code,{children:"BatchAppenderatorDriver.java"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"hadoop-based-batch-ingestion",children:"Hadoop-based Batch Ingestion"}),"\n",(0,r.jsxs)(n.p,{children:["The two main Hadoop indexing classes are ",(0,r.jsx)(n.code,{children:"HadoopDruidDetermineConfigurationJob.java"})," for the job to determine how many Druid\nsegments to create, and ",(0,r.jsx)(n.code,{children:"HadoopDruidIndexerJob.java"}),", which creates Druid segments."]}),"\n",(0,r.jsx)(n.p,{children:"At some point in the future, we may move the Hadoop ingestion code out of core Druid."}),"\n",(0,r.jsx)(n.h2,{id:"internal-uis",children:"Internal UIs"}),"\n",(0,r.jsx)(n.p,{children:"Druid currently has two internal UIs. One is for the Coordinator and one is for the Overlord."}),"\n",(0,r.jsx)(n.p,{children:"At some point in the future, we will likely move the internal UI code out of core Druid."}),"\n",(0,r.jsx)(n.h2,{id:"client-libraries",children:"Client libraries"}),"\n",(0,r.jsxs)(n.p,{children:["We welcome contributions for new client libraries to interact with Druid. See the\n",(0,r.jsx)(n.a,{href:"https://druid.apache.org/libraries.html",children:"Community and third-party libraries"})," page for links to existing client\nlibraries."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}}}]);