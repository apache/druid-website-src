"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[5120],{26587:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"ingestion/tasks","title":"Task reference","description":"\x3c!--","source":"@site/docs/33.0.0/ingestion/tasks.md","sourceDirName":"ingestion","slug":"/ingestion/tasks","permalink":"/docs/33.0.0/ingestion/tasks","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"tasks","title":"Task reference","sidebar_label":"Task reference"},"sidebar":"docs","previous":{"title":"Partitioning","permalink":"/docs/33.0.0/ingestion/partitioning"},"next":{"title":"SQL-based ingestion","permalink":"/docs/33.0.0/multi-stage-query/"}}');var i=n(74848),r=n(28453);const o={id:"tasks",title:"Task reference",sidebar_label:"Task reference"},a=void 0,d={},l=[{value:"Task API",id:"task-api",level:2},{value:"Task reports",id:"task-reports",level:2},{value:"Completion report",id:"completion-report",level:3},{value:"Segment Availability Fields",id:"segment-availability-fields",level:4},{value:"Compaction task segment info fields",id:"compaction-task-segment-info-fields",level:4},{value:"Live report",id:"live-report",level:3},{value:"Live reports",id:"live-reports",level:2},{value:"Row stats",id:"row-stats",level:3},{value:"Unparseable events",id:"unparseable-events",level:3},{value:"Task lock system",id:"task-lock-system",level:2},{value:"&quot;Overshadowing&quot; between segments",id:"overshadowing-between-segments",level:3},{value:"Locking",id:"locking",level:3},{value:"Lock priority",id:"lock-priority",level:3},{value:"Task actions",id:"task-actions",level:2},{value:"Batching <code>segmentAllocate</code> actions",id:"batching-segmentallocate-actions",level:3},{value:"Context parameters",id:"context-parameters",level:2},{value:"Task logs",id:"task-logs",level:2},{value:"Configuring task storage sizes",id:"configuring-task-storage-sizes",level:2},{value:"All task types",id:"all-task-types",level:2},{value:"<code>index_parallel</code>",id:"index_parallel",level:3},{value:"<code>index_hadoop</code>",id:"index_hadoop",level:3},{value:"<code>index_kafka</code>",id:"index_kafka",level:3},{value:"<code>index_kinesis</code>",id:"index_kinesis",level:3},{value:"<code>compact</code>",id:"compact",level:3},{value:"<code>kill</code>",id:"kill",level:3}];function c(e){const s={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(s.p,{children:["Tasks do all ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/",children:"ingestion"}),"-related work in Druid."]}),"\n",(0,i.jsxs)(s.p,{children:["For batch ingestion, you will generally submit tasks directly to Druid using the\n",(0,i.jsx)(s.a,{href:"/docs/33.0.0/api-reference/tasks-api",children:"Tasks APIs"}),". For streaming ingestion, tasks are generally submitted for you by a\nsupervisor."]}),"\n",(0,i.jsx)(s.h2,{id:"task-api",children:"Task API"}),"\n",(0,i.jsx)(s.p,{children:"Task APIs are available in two main places:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["The ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/design/overlord",children:"Overlord"})," process offers HTTP APIs to submit tasks, cancel tasks, check their status,\nreview logs and reports, and more. Refer to the ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/api-reference/tasks-api",children:"Tasks API reference"})," for a\nfull list."]}),"\n",(0,i.jsxs)(s.li,{children:["Druid SQL includes a ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/querying/sql-metadata-tables#tasks-table",children:(0,i.jsx)(s.code,{children:"sys.tasks"})})," table that provides information about active\nand recently completed tasks. This table is read-only and has a subset of the full task report available through\nthe Overlord APIs."]}),"\n"]}),"\n",(0,i.jsx)("a",{name:"reports"}),"\n",(0,i.jsx)(s.h2,{id:"task-reports",children:"Task reports"}),"\n",(0,i.jsx)(s.p,{children:"A report containing information about the number of rows ingested, and any parse exceptions that occurred is available for both completed tasks and running tasks."}),"\n",(0,i.jsxs)(s.p,{children:["The reporting feature is supported by ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/native-batch",children:"native batch tasks"}),", the Hadoop batch task, and Kafka and Kinesis ingestion tasks."]}),"\n",(0,i.jsx)(s.h3,{id:"completion-report",children:"Completion report"}),"\n",(0,i.jsx)(s.p,{children:"After a task completes, if it supports reports, its report can be retrieved at:"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"http://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/task/{taskId}/reports\n"})}),"\n",(0,i.jsx)(s.p,{children:"An example output is shown below:"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-json",children:'{\n  "ingestionStatsAndErrors": {\n    "taskId": "compact_twitter_2018-09-24T18:24:23.920Z",\n    "payload": {\n      "ingestionState": "COMPLETED",\n      "unparseableEvents": {},\n      "rowStats": {\n        "determinePartitions": {\n          "processed": 0,\n          "processedBytes": 0,\n          "processedWithError": 0,\n          "thrownAway": 0,\n          "unparseable": 0\n        },\n        "buildSegments": {\n          "processed": 5390324,\n          "processedBytes": 5109573212,\n          "processedWithError": 0,\n          "thrownAway": 0,\n          "unparseable": 0\n        }\n      },\n      "segmentAvailabilityConfirmed": false,\n      "segmentAvailabilityWaitTimeMs": 0,\n      "recordsProcessed": {\n        "partition-a": 5789\n      },\n      "errorMsg": null\n    },\n    "type": "ingestionStatsAndErrors"\n  },\n  "taskContext": {\n    "type": "taskContext",\n    "taskId": "compact_twitter_2018-09-24T18:24:23.920Z",\n    "payload": {\n      "forceTimeChunkLock": true,\n      "useLineageBasedSegmentAllocation": true\n    }\n  }\n}\n'})}),"\n",(0,i.jsx)(s.p,{children:"Compaction tasks can generate multiple sets of segment output reports based on how the input interval is split. So the overall report contains mappings from each split to each report.\nExample report could be:"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-json",children:'{\n  "ingestionStatsAndErrors_0": {\n    "taskId": "compact_twitter_2018-09-24T18:24:23.920Z",\n    "payload": {\n      "ingestionState": "COMPLETED",\n      "unparseableEvents": {},\n      "rowStats": {\n        "buildSegments": {\n          "processed": 5390324,\n          "processedBytes": 5109573212,\n          "processedWithError": 0,\n          "thrownAway": 0,\n          "unparseable": 0\n        }\n      },\n      "segmentAvailabilityConfirmed": false,\n      "segmentAvailabilityWaitTimeMs": 0,\n      "recordsProcessed": null,\n      "errorMsg": null\n    },\n    "type": "ingestionStatsAndErrors"\n  },\n  "ingestionStatsAndErrors_1": {\n   "taskId": "compact_twitter_2018-09-25T18:24:23.920Z",\n   "payload": {\n    "ingestionState": "COMPLETED",\n    "unparseableEvents": {},\n    "rowStats": {\n     "buildSegments": {\n      "processed": 12345,\n      "processedBytes": 132456789,\n      "processedWithError": 0,\n      "thrownAway": 0,\n      "unparseable": 0\n     }\n    },\n    "segmentAvailabilityConfirmed": false,\n    "segmentAvailabilityWaitTimeMs": 0,\n    "recordsProcessed": null,\n    "errorMsg": null\n   },\n   "type": "ingestionStatsAndErrors"\n  }\n}\n'})}),"\n",(0,i.jsx)(s.h4,{id:"segment-availability-fields",children:"Segment Availability Fields"}),"\n",(0,i.jsxs)(s.p,{children:["For some task types, the indexing task can wait for the newly ingested segments to become available for queries after ingestion completes. The below fields inform the end user regarding the duration and result of the availability wait. For batch ingestion task types, refer to ",(0,i.jsx)(s.code,{children:"tuningConfig"})," docs to see if the task supports an availability waiting period."]}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{children:"Field"}),(0,i.jsx)(s.th,{children:"Description"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"segmentAvailabilityConfirmed"})}),(0,i.jsx)(s.td,{children:"Whether all segments generated by this ingestion task had been confirmed as available for queries in the cluster before the task completed."})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"segmentAvailabilityWaitTimeMs"})}),(0,i.jsx)(s.td,{children:"Milliseconds waited by the ingestion task for the newly ingested segments to be available for query after completing ingestion was completed."})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"recordsProcessed"})}),(0,i.jsx)(s.td,{children:"Partitions that were processed by an ingestion task and includes count of records processed from each partition."})]})]})]}),"\n",(0,i.jsx)(s.h4,{id:"compaction-task-segment-info-fields",children:"Compaction task segment info fields"}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{children:"Field"}),(0,i.jsx)(s.th,{children:"Description"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"segmentsRead"})}),(0,i.jsx)(s.td,{children:"Number of segments read by compaction task."})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"segmentsPublished"})}),(0,i.jsx)(s.td,{children:"Number of segments published by compaction task."})]})]})]}),"\n",(0,i.jsx)(s.h3,{id:"live-report",children:"Live report"}),"\n",(0,i.jsx)(s.p,{children:"When a task is running, a live report containing ingestion state, unparseable events and moving average for number of events processed for 1 min, 5 min, 15 min time window can be retrieved at:"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"http://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/task/{taskId}/reports\n"})}),"\n",(0,i.jsx)(s.p,{children:"An example output is shown below:"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-json",children:'{\n  "ingestionStatsAndErrors": {\n    "taskId": "compact_twitter_2018-09-24T18:24:23.920Z",\n    "payload": {\n      "ingestionState": "RUNNING",\n      "unparseableEvents": {},\n      "rowStats": {\n        "movingAverages": {\n          "buildSegments": {\n            "5m": {\n              "processed": 3.392158326408501,\n              "processedBytes": 627.5492903856,\n              "unparseable": 0,\n              "thrownAway": 0,\n              "processedWithError": 0\n            },\n            "15m": {\n              "processed": 1.736165476881023,\n              "processedBytes": 321.1906130223,\n              "unparseable": 0,\n              "thrownAway": 0,\n              "processedWithError": 0\n            },\n            "1m": {\n              "processed": 4.206417693750045,\n              "processedBytes": 778.1872733438,\n              "unparseable": 0,\n              "thrownAway": 0,\n              "processedWithError": 0\n            }\n          }\n        },\n        "totals": {\n          "buildSegments": {\n            "processed": 1994,\n            "processedBytes": 3425110,\n            "processedWithError": 0,\n            "thrownAway": 0,\n            "unparseable": 0\n          }\n        }\n      },\n      "errorMsg": null\n    },\n    "type": "ingestionStatsAndErrors"\n  }\n}\n'})}),"\n",(0,i.jsx)(s.p,{children:"A description of the fields:"}),"\n",(0,i.jsxs)(s.p,{children:["The ",(0,i.jsx)(s.code,{children:"ingestionStatsAndErrors"})," report provides information about row counts and errors."]}),"\n",(0,i.jsxs)(s.p,{children:["The ",(0,i.jsx)(s.code,{children:"ingestionState"})," shows what step of ingestion the task reached. Possible states include:"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"NOT_STARTED"}),": The task has not begun reading any rows"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"DETERMINE_PARTITIONS"}),": The task is processing rows to determine partitioning"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"BUILD_SEGMENTS"}),": The task is processing rows to construct segments"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"SEGMENT_AVAILABILITY_WAIT"}),": The task has published its segments and is waiting for them to become available."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"COMPLETED"}),": The task has finished its work."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Only batch tasks have the DETERMINE_PARTITIONS phase. Realtime tasks such as those created by the Kafka Indexing Service do not have a DETERMINE_PARTITIONS phase."}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.code,{children:"unparseableEvents"})," contains lists of exception messages that were caused by unparseable inputs. This can help with identifying problematic input rows. There will be one list each for the DETERMINE_PARTITIONS and BUILD_SEGMENTS phases. Note that the Hadoop batch task does not support saving of unparseable events."]}),"\n",(0,i.jsxs)(s.p,{children:["the ",(0,i.jsx)(s.code,{children:"rowStats"})," map contains information about row counts. There is one entry for each ingestion phase. The definitions of the different row counts are shown below:"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"processed"}),": Number of rows successfully ingested without parsing errors"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"processedBytes"}),": Total number of uncompressed bytes processed by the task. This reports the total byte size of all rows i.e. even those that are included in ",(0,i.jsx)(s.code,{children:"processedWithError"}),", ",(0,i.jsx)(s.code,{children:"unparseable"})," or ",(0,i.jsx)(s.code,{children:"thrownAway"}),"."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"processedWithError"}),": Number of rows that were ingested, but contained a parsing error within one or more columns. This typically occurs where input rows have a parseable structure but invalid types for columns, such as passing in a non-numeric String value for a numeric column."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"thrownAway"}),": Number of rows skipped. This includes rows with timestamps that were outside of the ingestion task's defined time interval and rows that were filtered out with a ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#transformspec",children:(0,i.jsx)(s.code,{children:"transformSpec"})}),", but doesn't include the rows skipped by explicit user configurations. For example, the rows skipped by ",(0,i.jsx)(s.code,{children:"skipHeaderRows"})," or ",(0,i.jsx)(s.code,{children:"hasHeaderRow"})," in the CSV format are not counted."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"unparseable"}),": Number of rows that could not be parsed at all and were discarded. This tracks input rows without a parseable structure, such as passing in non-JSON data when using a JSON parser."]}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["The ",(0,i.jsx)(s.code,{children:"errorMsg"})," field shows a message describing the error that caused a task to fail. It will be null if the task was successful."]}),"\n",(0,i.jsx)(s.h2,{id:"live-reports",children:"Live reports"}),"\n",(0,i.jsx)(s.h3,{id:"row-stats",children:"Row stats"}),"\n",(0,i.jsxs)(s.p,{children:["The ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/native-batch",children:"native batch task"}),", the Hadoop batch task, and Kafka and Kinesis ingestion tasks support retrieval of row stats while the task is running."]}),"\n",(0,i.jsx)(s.p,{children:"The live report can be accessed with a GET to the following URL on a Peon running a task:"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"http://<middlemanager-host>:<worker-port>/druid/worker/v1/chat/{taskId}/rowStats\n"})}),"\n",(0,i.jsxs)(s.p,{children:["An example report is shown below. The ",(0,i.jsx)(s.code,{children:"movingAverages"})," section contains 1 minute, 5 minute, and 15 minute moving averages of increases to the four row counters, which have the same definitions as those in the completion report. The ",(0,i.jsx)(s.code,{children:"totals"})," section shows the current totals."]}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-json",children:'{\n  "movingAverages": {\n    "buildSegments": {\n      "5m": {\n        "processed": 3.392158326408501,\n        "processedBytes": 627.5492903856,\n        "unparseable": 0,\n        "thrownAway": 0,\n        "processedWithError": 0\n      },\n      "15m": {\n        "processed": 1.736165476881023,\n        "processedBytes": 321.1906130223,\n        "unparseable": 0,\n        "thrownAway": 0,\n        "processedWithError": 0\n      },\n      "1m": {\n        "processed": 4.206417693750045,\n        "processedBytes": 778.1872733438,\n        "unparseable": 0,\n        "thrownAway": 0,\n        "processedWithError": 0\n      }\n    }\n  },\n  "totals": {\n    "buildSegments": {\n      "processed": 1994,\n      "processedBytes": 3425110,\n      "processedWithError": 0,\n      "thrownAway": 0,\n      "unparseable": 0\n    }\n  }\n}\n'})}),"\n",(0,i.jsx)(s.p,{children:"For the Kafka Indexing Service, a GET to the following Overlord API will retrieve live row stat reports from each task being managed by the supervisor and provide a combined report."}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"http://<OVERLORD-HOST>:<OVERLORD-PORT>/druid/indexer/v1/supervisor/{supervisorId}/stats\n"})}),"\n",(0,i.jsx)(s.h3,{id:"unparseable-events",children:"Unparseable events"}),"\n",(0,i.jsx)(s.p,{children:"Lists of recently-encountered unparseable events can be retrieved from a running task with a GET to the following Peon API:"}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{children:"http://<middlemanager-host>:<worker-port>/druid/worker/v1/chat/{taskId}/unparseableEvents\n"})}),"\n",(0,i.jsxs)(s.p,{children:["Note that this functionality is not supported by all task types. Currently, it is only supported by the\nnon-parallel ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/native-batch",children:"native batch task"})," (type ",(0,i.jsx)(s.code,{children:"index"}),") and the tasks created by the Kafka\nand Kinesis indexing services."]}),"\n",(0,i.jsx)("a",{name:"locks"}),"\n",(0,i.jsx)(s.h2,{id:"task-lock-system",children:"Task lock system"}),"\n",(0,i.jsx)(s.p,{children:"This section explains the task locking system in Druid. Druid's locking system\nand versioning system are tightly coupled with each other to guarantee the correctness of ingested data."}),"\n",(0,i.jsx)(s.h3,{id:"overshadowing-between-segments",children:'"Overshadowing" between segments'}),"\n",(0,i.jsxs)(s.p,{children:["You can run a task to overwrite existing data. The segments created by an overwriting task ",(0,i.jsx)(s.em,{children:"overshadows"})," existing segments.\nNote that the overshadow relation holds only for the same time chunk and the same data source.\nThese overshadowed segments are not considered in query processing to filter out stale data."]}),"\n",(0,i.jsxs)(s.p,{children:["Each segment has a ",(0,i.jsx)(s.em,{children:"major"})," version and a ",(0,i.jsx)(s.em,{children:"minor"})," version. The major version is\nrepresented as a timestamp in the format of ",(0,i.jsx)(s.a,{href:"https://www.joda.org/joda-time/apidocs/org/joda/time/format/DateTimeFormat",children:(0,i.jsx)(s.code,{children:"\"yyyy-MM-dd'T'hh:mm:ss\""})}),"\nwhile the minor version is an integer number. These major and minor versions\nare used to determine the overshadow relation between segments as seen below."]}),"\n",(0,i.jsxs)(s.p,{children:["A segment ",(0,i.jsx)(s.code,{children:"s1"})," overshadows another ",(0,i.jsx)(s.code,{children:"s2"})," if"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"s1"})," has a higher major version than ",(0,i.jsx)(s.code,{children:"s2"}),", or"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"s1"})," has the same major version and a higher minor version than ",(0,i.jsx)(s.code,{children:"s2"}),"."]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"Here are some examples."}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:["A segment of the major version of ",(0,i.jsx)(s.code,{children:"2019-01-01T00:00:00.000Z"})," and the minor version of ",(0,i.jsx)(s.code,{children:"0"})," overshadows\nanother of the major version of ",(0,i.jsx)(s.code,{children:"2018-01-01T00:00:00.000Z"})," and the minor version of ",(0,i.jsx)(s.code,{children:"1"}),"."]}),"\n",(0,i.jsxs)(s.li,{children:["A segment of the major version of ",(0,i.jsx)(s.code,{children:"2019-01-01T00:00:00.000Z"})," and the minor version of ",(0,i.jsx)(s.code,{children:"1"})," overshadows\nanother of the major version of ",(0,i.jsx)(s.code,{children:"2019-01-01T00:00:00.000Z"})," and the minor version of ",(0,i.jsx)(s.code,{children:"0"}),"."]}),"\n"]}),"\n",(0,i.jsx)(s.h3,{id:"locking",children:"Locking"}),"\n",(0,i.jsxs)(s.p,{children:["If you are running two or more ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/tasks",children:"Druid tasks"})," which generate segments for the same data source and the same time chunk,\nthe generated segments could potentially overshadow each other, which could lead to incorrect query results."]}),"\n",(0,i.jsxs)(s.p,{children:["To avoid this problem, tasks will attempt to get locks prior to creating any segment in Druid.\nThere are two types of locks, i.e., ",(0,i.jsx)(s.em,{children:"time chunk lock"})," and ",(0,i.jsx)(s.em,{children:"segment lock"}),"."]}),"\n",(0,i.jsxs)(s.p,{children:["When the time chunk lock is used, a task locks the entire time chunk of a data source where generated segments will be written.\nFor example, suppose we have a task ingesting data into the time chunk of ",(0,i.jsx)(s.code,{children:"2019-01-01T00:00:00.000Z/2019-01-02T00:00:00.000Z"})," of the ",(0,i.jsx)(s.code,{children:"wikipedia"})," data source.\nWith the time chunk locking, this task will lock the entire time chunk of ",(0,i.jsx)(s.code,{children:"2019-01-01T00:00:00.000Z/2019-01-02T00:00:00.000Z"})," of the ",(0,i.jsx)(s.code,{children:"wikipedia"})," data source\nbefore it creates any segments. As long as it holds the lock, any other tasks will be unable to create segments for the same time chunk of the same data source.\nThe segments created with the time chunk locking have a ",(0,i.jsx)(s.em,{children:"higher"})," major version than existing segments. Their minor version is always ",(0,i.jsx)(s.code,{children:"0"}),"."]}),"\n",(0,i.jsxs)(s.p,{children:["When the segment lock is used, a task locks individual segments instead of the entire time chunk.\nAs a result, two or more tasks can create segments for the same time chunk of the same data source simultaneously\nif they are reading different segments.\nFor example, a Kafka indexing task and a compaction task can always write segments into the same time chunk of the same data source simultaneously.\nThe reason for this is because a Kafka indexing task always appends new segments, while a compaction task always overwrites existing segments.\nThe segments created with the segment locking have the ",(0,i.jsx)(s.em,{children:"same"})," major version and a ",(0,i.jsx)(s.em,{children:"higher"})," minor version."]}),"\n",(0,i.jsx)(s.admonition,{type:"info",children:(0,i.jsx)(s.p,{children:"The segment locking is still experimental. It could have unknown bugs which potentially lead to incorrect query results."})}),"\n",(0,i.jsxs)(s.p,{children:["To enable segment locking, you may need to set ",(0,i.jsx)(s.code,{children:"forceTimeChunkLock"})," to ",(0,i.jsx)(s.code,{children:"false"})," in the ",(0,i.jsx)(s.a,{href:"#context",children:"task context"}),".\nOnce ",(0,i.jsx)(s.code,{children:"forceTimeChunkLock"})," is unset, the task will choose a proper lock type to use automatically.\nPlease note that segment lock is not always available. The most common use case where time chunk lock is enforced is\nwhen an overwriting task changes the segment granularity.\nAlso, the segment locking is supported by only native indexing tasks and Kafka/Kinesis indexing tasks.\nHadoop indexing tasks don't support it."]}),"\n",(0,i.jsxs)(s.p,{children:[(0,i.jsx)(s.code,{children:"forceTimeChunkLock"})," in the task context is only applied to individual tasks.\nIf you want to unset it for all tasks, you would want to set ",(0,i.jsx)(s.code,{children:"druid.indexer.tasklock.forceTimeChunkLock"})," to false in the ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/configuration/#overlord-operations",children:"overlord configuration"}),"."]}),"\n",(0,i.jsx)(s.p,{children:"Lock requests can conflict with each other if two or more tasks try to get locks for the overlapped time chunks of the same data source.\nNote that the lock conflict can happen between different locks types."}),"\n",(0,i.jsxs)(s.p,{children:["The behavior on lock conflicts depends on the ",(0,i.jsx)(s.a,{href:"#lock-priority",children:"task priority"}),".\nIf all tasks of conflicting lock requests have the same priority, then the task who requested first will get the lock.\nOther tasks will wait for the task to release the lock."]}),"\n",(0,i.jsxs)(s.p,{children:["If a task of a lower priority asks a lock later than another of a higher priority,\nthis task will also wait for the task of a higher priority to release the lock.\nIf a task of a higher priority asks a lock later than another of a lower priority,\nthen this task will ",(0,i.jsx)(s.em,{children:"preempt"})," the other task of a lower priority. The lock\nof the lower-prioritized task will be revoked and the higher-prioritized task will acquire a new lock."]}),"\n",(0,i.jsxs)(s.p,{children:["This lock preemption can happen at any time while a task is running except\nwhen it is ",(0,i.jsx)(s.em,{children:"publishing segments"})," in a critical section. Its locks become preemptible again once publishing segments is finished."]}),"\n",(0,i.jsx)(s.p,{children:"Note that locks are shared by the tasks of the same groupId.\nFor example, Kafka indexing tasks of the same supervisor have the same groupId and share all locks with each other."}),"\n",(0,i.jsx)("a",{name:"priority"}),"\n",(0,i.jsx)(s.h3,{id:"lock-priority",children:"Lock priority"}),"\n",(0,i.jsx)(s.p,{children:"Each task type has a different default lock priority. The below table shows the default priorities of different task types. Higher the number, higher the priority."}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{children:"task type"}),(0,i.jsx)(s.th,{children:"default priority"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"Realtime index task"}),(0,i.jsx)(s.td,{children:"75"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsxs)(s.td,{children:["Batch index tasks, including ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/native-batch",children:"native batch"}),", ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/multi-stage-query/",children:"SQL"}),", and ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/hadoop",children:"Hadoop-based"})]}),(0,i.jsx)(s.td,{children:"50"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"Merge/Append/Compaction task"}),(0,i.jsx)(s.td,{children:"25"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:"Other tasks"}),(0,i.jsx)(s.td,{children:"0"})]})]})]}),"\n",(0,i.jsx)(s.p,{children:"You can override the task priority by setting your priority in the task context as below."}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-json",children:'"context" : {\n  "priority" : 100\n}\n'})}),"\n",(0,i.jsx)("a",{name:"actions"}),"\n",(0,i.jsx)(s.h2,{id:"task-actions",children:"Task actions"}),"\n",(0,i.jsx)(s.p,{children:"Task actions are overlord actions performed by tasks during their lifecycle. Some typical task actions are:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"lockAcquire"}),": acquires a time-chunk lock on an interval for the task"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"lockRelease"}),": releases a lock acquired by the task on an interval"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"segmentTransactionalInsert"}),": publishes new segments created by a task and optionally overwrites and/or drops existing segments in a single transaction"]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"segmentAllocate"}),": allocates pending segments to a task to write rows"]}),"\n"]}),"\n",(0,i.jsxs)(s.h3,{id:"batching-segmentallocate-actions",children:["Batching ",(0,i.jsx)(s.code,{children:"segmentAllocate"})," actions"]}),"\n",(0,i.jsxs)(s.p,{children:["In a cluster with several concurrent tasks, ",(0,i.jsx)(s.code,{children:"segmentAllocate"})," actions on the overlord can take a long time to finish, causing spikes in the ",(0,i.jsx)(s.code,{children:"task/action/run/time"}),". This can result in ingestion lag building up while a task waits for a segment to be allocated.\nThe root cause of such spikes is likely to be one or more of the following:"]}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"several concurrent tasks trying to allocate segments for the same datasource and interval"}),"\n",(0,i.jsx)(s.li,{children:"large number of metadata calls made to the segments and pending segments tables"}),"\n",(0,i.jsx)(s.li,{children:"concurrency limitations while acquiring a task lock required for allocating a segment"}),"\n"]}),"\n",(0,i.jsxs)(s.p,{children:["Since the contention typically arises from tasks allocating segments for the same datasource and interval, you can improve the run times by batching the actions together.\nTo enable batched segment allocation on the overlord, set  ",(0,i.jsx)(s.code,{children:"druid.indexer.tasklock.batchSegmentAllocation"})," to ",(0,i.jsx)(s.code,{children:"true"}),". See ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/configuration/#overlord-operations",children:"overlord configuration"})," for more details."]}),"\n",(0,i.jsx)("a",{name:"context"}),"\n",(0,i.jsx)(s.h2,{id:"context-parameters",children:"Context parameters"}),"\n",(0,i.jsxs)(s.p,{children:["The task context is used for various individual task configuration.\nSpecify task context configurations in the ",(0,i.jsx)(s.code,{children:"context"})," field of the ingestion spec.\nWhen configuring ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/data-management/automatic-compaction",children:"automatic compaction"}),", set the task context configurations in ",(0,i.jsx)(s.code,{children:"taskContext"})," rather than in ",(0,i.jsx)(s.code,{children:"context"}),".\nThe settings get passed into the ",(0,i.jsx)(s.code,{children:"context"})," field of the compaction tasks issued to Middle Managers."]}),"\n",(0,i.jsx)(s.p,{children:"The following parameters apply to all task types."}),"\n",(0,i.jsxs)(s.table,{children:[(0,i.jsx)(s.thead,{children:(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.th,{children:"Property"}),(0,i.jsx)(s.th,{children:"Description"}),(0,i.jsx)(s.th,{children:"Default"})]})}),(0,i.jsxs)(s.tbody,{children:[(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"forceTimeChunkLock"})}),(0,i.jsxs)(s.td,{children:[(0,i.jsx)(s.em,{children:"Setting this to false is still experimental."}),(0,i.jsx)("br",{})," Force to use time chunk lock. When ",(0,i.jsx)(s.code,{children:"true"}),", this parameter overrides the overlord runtime property ",(0,i.jsx)(s.code,{children:"druid.indexer.tasklock.forceTimeChunkLock"})," ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/configuration/#overlord-operations",children:"configuration for the overlord"}),". If neither this parameter nor the runtime property is ",(0,i.jsx)(s.code,{children:"true"}),", each task automatically chooses a lock type to use. See ",(0,i.jsx)(s.a,{href:"#locking",children:"Locking"})," for more details."]}),(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"true"})})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"priority"})}),(0,i.jsx)(s.td,{children:"Task priority"}),(0,i.jsxs)(s.td,{children:["Depends on the task type. See ",(0,i.jsx)(s.a,{href:"#priority",children:"Priority"})," for more details."]})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"storeCompactionState"})}),(0,i.jsxs)(s.td,{children:["Enables the task to store the compaction state of created segments in the metadata store. When ",(0,i.jsx)(s.code,{children:"true"}),", the segments created by the task fill ",(0,i.jsx)(s.code,{children:"lastCompactionState"})," in the segment metadata. This parameter is set automatically on compaction tasks."]}),(0,i.jsxs)(s.td,{children:[(0,i.jsx)(s.code,{children:"true"})," for compaction tasks, ",(0,i.jsx)(s.code,{children:"false"})," for other task types"]})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"storeEmptyColumns"})}),(0,i.jsxs)(s.td,{children:["Enables the task to store empty columns during ingestion. When ",(0,i.jsx)(s.code,{children:"true"}),", Druid stores every column specified in the ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#dimensionsspec",children:(0,i.jsx)(s.code,{children:"dimensionsSpec"})}),". When ",(0,i.jsx)(s.code,{children:"false"}),", Druid SQL queries referencing empty columns will fail. If you intend to leave ",(0,i.jsx)(s.code,{children:"storeEmptyColumns"})," disabled, you should either ingest dummy data for empty columns or else not query on empty columns.",(0,i.jsx)("br",{}),(0,i.jsx)("br",{}),"When set in the task context, ",(0,i.jsx)(s.code,{children:"storeEmptyColumns"})," overrides the system property ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/configuration/#additional-peon-configuration",children:(0,i.jsx)(s.code,{children:"druid.indexer.task.storeEmptyColumns"})}),"."]}),(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"true"})})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"taskLockTimeout"})}),(0,i.jsxs)(s.td,{children:["Task lock timeout in milliseconds. For more details, see ",(0,i.jsx)(s.a,{href:"#locking",children:"Locking"}),".",(0,i.jsx)("br",{}),(0,i.jsx)("br",{}),"When a task acquires a lock, it sends a request via HTTP and awaits until it receives a response containing the lock acquisition result. As a result, an HTTP timeout error can occur if ",(0,i.jsx)(s.code,{children:"taskLockTimeout"})," is greater than ",(0,i.jsx)(s.code,{children:"druid.server.http.maxIdleTime"})," of Overlords."]}),(0,i.jsx)(s.td,{children:"300000"})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"useLineageBasedSegmentAllocation"})}),(0,i.jsxs)(s.td,{children:["Enables the new lineage-based segment allocation protocol for the native Parallel task with dynamic partitioning. This option should be off during the replacing rolling upgrade from one of the Druid versions between 0.19 and 0.21 to Druid 0.22 or higher. Once the upgrade is done, it must be set to ",(0,i.jsx)(s.code,{children:"true"})," to ensure data correctness."]}),(0,i.jsxs)(s.td,{children:[(0,i.jsx)(s.code,{children:"false"})," in 0.21 or earlier, ",(0,i.jsx)(s.code,{children:"true"})," in 0.22 or later"]})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"lookupLoadingMode"})}),(0,i.jsxs)(s.td,{children:["Controls the lookup loading behavior in tasks. This property supports three values: ",(0,i.jsx)(s.code,{children:"ALL"})," mode loads all the lookups, ",(0,i.jsx)(s.code,{children:"NONE"})," mode does not load any lookups and ",(0,i.jsx)(s.code,{children:"ONLY_REQUIRED"})," mode loads the lookups specified with context key ",(0,i.jsx)(s.code,{children:"lookupsToLoad"}),". This property must not be specified for ",(0,i.jsx)(s.code,{children:"MSQ"})," and ",(0,i.jsx)(s.code,{children:"kill"})," tasks as the task engine enforces ",(0,i.jsx)(s.code,{children:"ONLY_REQUIRED"})," mode for ",(0,i.jsx)(s.code,{children:"MSQWorkerTask"})," and ",(0,i.jsx)(s.code,{children:"NONE"})," mode for ",(0,i.jsx)(s.code,{children:"MSQControllerTask"})," and ",(0,i.jsx)(s.code,{children:"kill"})," tasks."]}),(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"ALL"})})]}),(0,i.jsxs)(s.tr,{children:[(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"lookupsToLoad"})}),(0,i.jsxs)(s.td,{children:["List of lookup names to load in tasks. This property is required only if the ",(0,i.jsx)(s.code,{children:"lookupLoadingMode"})," is set to ",(0,i.jsx)(s.code,{children:"ONLY_REQUIRED"}),". For ",(0,i.jsx)(s.code,{children:"MSQWorkerTask"})," type, the lookup names to load are identified by the controller task by parsing the SQL."]}),(0,i.jsx)(s.td,{children:(0,i.jsx)(s.code,{children:"null"})})]})]})]}),"\n",(0,i.jsx)(s.h2,{id:"task-logs",children:"Task logs"}),"\n",(0,i.jsx)(s.p,{children:"Logs are created by ingestion tasks as they run. You can configure Druid to push these into a repository for long-term storage after they complete."}),"\n",(0,i.jsxs)(s.p,{children:["Once the task has been submitted to the Overlord it remains ",(0,i.jsx)(s.code,{children:"WAITING"})," for locks to be acquired. Worker slot allocation is then ",(0,i.jsx)(s.code,{children:"PENDING"})," until the task can actually start executing."]}),"\n",(0,i.jsxs)(s.p,{children:["The task then starts creating logs in a local directory of the middle manager (or indexer) in a ",(0,i.jsx)(s.code,{children:"log"})," directory for the specific ",(0,i.jsx)(s.code,{children:"taskId"})," at ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/configuration/#middle-manager-configuration",children:(0,i.jsx)(s.code,{children:"druid.worker.baseTaskDirs"})}),"."]}),"\n",(0,i.jsxs)(s.p,{children:["When the task completes - whether it succeeds or fails - the middle manager (or indexer) will push the task log file into the location specified in ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/configuration/#task-logging",children:(0,i.jsx)(s.code,{children:"druid.indexer.logs"})}),"."]}),"\n",(0,i.jsxs)(s.p,{children:["Task logs on the Druid web console are retrieved via an ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/api-reference/service-status-api#overlord",children:"API"})," on the Overlord. It automatically detects where the log file is, either in the Middle Manager / indexer or in long-term storage, and passes it back."]}),"\n",(0,i.jsx)(s.p,{children:"If you don't see the log file in long-term storage, it means either:"}),"\n",(0,i.jsxs)(s.ul,{children:["\n",(0,i.jsx)(s.li,{children:"the Middle Manager / indexer failed to push the log file to deep storage or"}),"\n",(0,i.jsx)(s.li,{children:"the task did not complete."}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"You can check the Middle Manager / indexer logs locally to see if there was a push failure. If there was not, check the Overlord's own process logs to see why the task failed before it started."}),"\n",(0,i.jsx)(s.admonition,{type:"info",children:(0,i.jsx)(s.p,{children:"If you are running the indexing service in remote mode, the task logs must be stored in S3, Azure Blob Store, Google Cloud Storage or HDFS."})}),"\n",(0,i.jsxs)(s.p,{children:["You can configure retention periods for logs in milliseconds by setting ",(0,i.jsx)(s.code,{children:"druid.indexer.logs.kill"})," properties in ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/configuration/#task-logging",children:"configuration"}),".  The Overlord will then automatically manage task logs in log directories along with entries in task-related metadata storage tables."]}),"\n",(0,i.jsx)(s.admonition,{type:"info",children:(0,i.jsx)(s.p,{children:"Automatic log file deletion typically works based on the log file's 'modified' timestamp in the back-end store. Large clock skews between Druid processes and the long-term store might result in unintended behavior."})}),"\n",(0,i.jsx)(s.h2,{id:"configuring-task-storage-sizes",children:"Configuring task storage sizes"}),"\n",(0,i.jsx)(s.p,{children:"Tasks sometimes need to use local disk for storage of things while the task is active.  For example, for realtime ingestion tasks to accept broadcast segments for broadcast joins.  Or intermediate data sets for Multi-stage Query jobs"}),"\n",(0,i.jsx)(s.p,{children:"Task storage sizes are configured through a combination of three properties:"}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"druid.worker.capacity"}),' - i.e. the "number of task slots"']}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"druid.worker.baseTaskDirs"})," - i.e. the list of directories to use for task storage."]}),"\n",(0,i.jsxs)(s.li,{children:[(0,i.jsx)(s.code,{children:"druid.worker.baseTaskDirSize"})," - i.e. the amount of storage to use on each storage location"]}),"\n"]}),"\n",(0,i.jsx)(s.p,{children:"While it seems like one task might use multiple directories, only one directory from the list of base directories will be used for any given task, as such, each task is only given a singular directory for scratch space."}),"\n",(0,i.jsx)(s.p,{children:"The actual amount of memory assigned to any given task is computed by determining the largest size that enables all task slots to be given an equivalent amount of disk storage. For example, with 5 slots, 2 directories (A and B) and a size of 300 GB, 3 slots would be given to directory A, 2 slots to directory B and each slot would be allowed 100 GB"}),"\n",(0,i.jsx)(s.h2,{id:"all-task-types",children:"All task types"}),"\n",(0,i.jsx)(s.h3,{id:"index_parallel",children:(0,i.jsx)(s.code,{children:"index_parallel"})}),"\n",(0,i.jsxs)(s.p,{children:["See ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/native-batch",children:"Native batch ingestion (parallel task)"}),"."]}),"\n",(0,i.jsx)(s.h3,{id:"index_hadoop",children:(0,i.jsx)(s.code,{children:"index_hadoop"})}),"\n",(0,i.jsxs)(s.p,{children:["See ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/hadoop",children:"Hadoop-based ingestion"}),"."]}),"\n",(0,i.jsx)(s.h3,{id:"index_kafka",children:(0,i.jsx)(s.code,{children:"index_kafka"})}),"\n",(0,i.jsxs)(s.p,{children:["Submitted automatically, on your behalf, by a\n",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/kafka-ingestion",children:"Kafka-based ingestion supervisor"}),"."]}),"\n",(0,i.jsx)(s.h3,{id:"index_kinesis",children:(0,i.jsx)(s.code,{children:"index_kinesis"})}),"\n",(0,i.jsxs)(s.p,{children:["Submitted automatically, on your behalf, by a\n",(0,i.jsx)(s.a,{href:"/docs/33.0.0/ingestion/kinesis-ingestion",children:"Kinesis-based ingestion supervisor"}),"."]}),"\n",(0,i.jsx)(s.h3,{id:"compact",children:(0,i.jsx)(s.code,{children:"compact"})}),"\n",(0,i.jsxs)(s.p,{children:["Compaction tasks merge all segments of the given interval. See the documentation on\n",(0,i.jsx)(s.a,{href:"/docs/33.0.0/data-management/compaction",children:"compaction"})," for details."]}),"\n",(0,i.jsx)(s.h3,{id:"kill",children:(0,i.jsx)(s.code,{children:"kill"})}),"\n",(0,i.jsxs)(s.p,{children:["Kill tasks delete all metadata about certain segments and removes them from deep storage.\nSee the documentation on ",(0,i.jsx)(s.a,{href:"/docs/33.0.0/data-management/delete",children:"deleting data"})," for details."]})]})}function h(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},28453:(e,s,n)=>{n.d(s,{R:()=>o,x:()=>a});var t=n(96540);const i={},r=t.createContext(i);function o(e){const s=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(r.Provider,{value:s},e.children)}}}]);