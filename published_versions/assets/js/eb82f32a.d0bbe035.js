"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[6570],{4434:(e,o,t)=>{t.r(o),t.d(o,{assets:()=>s,contentTitle:()=>i,default:()=>h,frontMatter:()=>d,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"tutorials/tutorial-batch-hadoop","title":"Load batch data using Apache Hadoop","description":"\x3c!--","source":"@site/docs/33.0.0/tutorials/tutorial-batch-hadoop.md","sourceDirName":"tutorials","slug":"/tutorials/tutorial-batch-hadoop","permalink":"/docs/33.0.0/tutorials/tutorial-batch-hadoop","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"tutorial-batch-hadoop","title":"Load batch data using Apache Hadoop","sidebar_label":"Load from Apache Hadoop"},"sidebar":"docs","previous":{"title":"JDBC connector tutorial","permalink":"/docs/33.0.0/tutorials/tutorial-jdbc"},"next":{"title":"Kerberized HDFS deep storage","permalink":"/docs/33.0.0/tutorials/tutorial-kerberos-hadoop"}}');var n=t(74848),a=t(28453);const d={id:"tutorial-batch-hadoop",title:"Load batch data using Apache Hadoop",sidebar_label:"Load from Apache Hadoop"},i=void 0,s={},l=[{value:"Install Docker",id:"install-docker",level:2},{value:"Build the Hadoop docker image",id:"build-the-hadoop-docker-image",level:2},{value:"Setup the Hadoop docker cluster",id:"setup-the-hadoop-docker-cluster",level:2},{value:"Create temporary shared directory",id:"create-temporary-shared-directory",level:3},{value:"Configure /etc/hosts",id:"configure-etchosts",level:3},{value:"Start the Hadoop container",id:"start-the-hadoop-container",level:3},{value:"Accessing the Hadoop container shell",id:"accessing-the-hadoop-container-shell",level:4},{value:"Test data",id:"test-data",level:3},{value:"Configure Druid to use Hadoop",id:"configure-druid-to-use-hadoop",level:2},{value:"Provide Hadoop configuration for Druid",id:"provide-hadoop-configuration-for-druid",level:3},{value:"Update Druid segment and log storage",id:"update-druid-segment-and-log-storage",level:3},{value:"Disable local deep storage and enable HDFS deep storage",id:"disable-local-deep-storage-and-enable-hdfs-deep-storage",level:4},{value:"Disable local log storage and enable HDFS log storage",id:"disable-local-log-storage-and-enable-hdfs-log-storage",level:4},{value:"Restart Druid cluster",id:"restart-druid-cluster",level:3},{value:"Load batch data",id:"load-batch-data",level:2},{value:"Querying your data",id:"querying-your-data",level:2},{value:"Cleanup",id:"cleanup",level:2},{value:"Further reading",id:"further-reading",level:2}];function c(e){const o={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",ul:"ul",...(0,a.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(o.p,{children:"This tutorial shows you how to load data files into Apache Druid using a remote Hadoop cluster."}),"\n",(0,n.jsxs)(o.p,{children:["For this tutorial, we'll assume that you've already completed the previous\n",(0,n.jsx)(o.a,{href:"/docs/33.0.0/tutorials/tutorial-batch",children:"batch ingestion tutorial"})," using Druid's native batch ingestion system and are using the\nautomatic single-machine configuration as described in the ",(0,n.jsx)(o.a,{href:"/docs/33.0.0/operations/single-server",children:"quickstart"}),"."]}),"\n",(0,n.jsx)(o.h2,{id:"install-docker",children:"Install Docker"}),"\n",(0,n.jsxs)(o.p,{children:["This tutorial requires ",(0,n.jsx)(o.a,{href:"https://docs.docker.com/install/",children:"Docker"})," to be installed on the tutorial machine."]}),"\n",(0,n.jsx)(o.p,{children:"Once the Docker install is complete, please proceed to the next steps in the tutorial."}),"\n",(0,n.jsx)(o.h2,{id:"build-the-hadoop-docker-image",children:"Build the Hadoop docker image"}),"\n",(0,n.jsx)(o.p,{children:"For this tutorial, we've provided a Dockerfile for a Hadoop 3.3.6 cluster, which we'll use to run the batch indexing task."}),"\n",(0,n.jsxs)(o.p,{children:["This Dockerfile and related files are located at ",(0,n.jsx)(o.code,{children:"quickstart/tutorial/hadoop/docker"}),"."]}),"\n",(0,n.jsxs)(o.p,{children:["From the ",(0,n.jsx)(o.code,{children:"apache-druid-33.0.0"}),' package root, run the following commands to build a Docker image named "druid-hadoop-demo" with version tag "3.3.6":']}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{className:"language-bash",children:"cd quickstart/tutorial/hadoop/docker\ndocker build -t druid-hadoop-demo:3.3.6 .\n"})}),"\n",(0,n.jsxs)(o.p,{children:["This will start building the Hadoop image. Once the image build is done, you should see the message ",(0,n.jsx)(o.code,{children:"Successfully tagged druid-hadoop-demo:3.3.6"})," printed to the console."]}),"\n",(0,n.jsx)(o.h2,{id:"setup-the-hadoop-docker-cluster",children:"Setup the Hadoop docker cluster"}),"\n",(0,n.jsx)(o.h3,{id:"create-temporary-shared-directory",children:"Create temporary shared directory"}),"\n",(0,n.jsx)(o.p,{children:"We'll need a shared folder between the host and the Hadoop container for transferring some files."}),"\n",(0,n.jsxs)(o.p,{children:["Let's create some folders under ",(0,n.jsx)(o.code,{children:"/tmp"}),", we will use these later when starting the Hadoop container:"]}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{className:"language-bash",children:"mkdir -p /tmp/shared\n"})}),"\n",(0,n.jsx)(o.h3,{id:"configure-etchosts",children:"Configure /etc/hosts"}),"\n",(0,n.jsxs)(o.p,{children:["On the host machine, add the following entry to ",(0,n.jsx)(o.code,{children:"/etc/hosts"}),":"]}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{children:"127.0.0.1 druid-hadoop-demo\n"})}),"\n",(0,n.jsx)(o.h3,{id:"start-the-hadoop-container",children:"Start the Hadoop container"}),"\n",(0,n.jsxs)(o.p,{children:["Once the ",(0,n.jsx)(o.code,{children:"/tmp/shared"})," folder has been created and the ",(0,n.jsx)(o.code,{children:"etc/hosts"})," entry has been added, run the following command to start the Hadoop container."]}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{className:"language-bash",children:"docker run -it  -h druid-hadoop-demo --name druid-hadoop-demo -p 2049:2049 -p 2122:2122 -p 8020-8042:8020-8042 -p 8088:8088 -p 8443:8443 -p 9000:9000 -p 9820:9820 -p 9860-9880:9860-9880 -p 10020:10020 -p 19888:19888 -p 34455:34455 -p 49707:49707 -p 50010:50010 -p 50020:50020 -p 50030:50030 -p 50060:50060 -p 50070:50070 -p 50075:50075 -p 50090:50090 -p 51111:51111 -v /tmp/shared:/shared druid-hadoop-demo:3.3.6 /etc/bootstrap.sh -bash\n"})}),"\n",(0,n.jsx)(o.p,{children:"Once the container is started, your terminal will attach to a bash shell running inside the container:"}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{className:"language-bash",children:'Starting namenodes on [druid-hadoop-demo]\nStarting datanodes\nStarting secondary namenodes [druid-hadoop-demo]\nWARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\nWARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\nStarting resourcemanager\nWARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\nStarting nodemanagers\nWARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\nlocalhost: WARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\nWARNING: YARN_CONF_DIR has been replaced by HADOOP_CONF_DIR. Using value of YARN_CONF_DIR.\nWARNING: YARN_OPTS has been replaced by HADOOP_OPTS. Using value of YARN_OPTS.\nWARNING: Use of this script to start the MR JobHistory daemon is deprecated.\nWARNING: Attempting to execute replacement "mapred --daemon start" instead.\n * initialize hdfs for first run\nbash-4.1#\n'})}),"\n",(0,n.jsxs)(o.p,{children:["The ",(0,n.jsx)(o.code,{children:"Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"})," warning messages can be safely ignored."]}),"\n",(0,n.jsx)(o.h4,{id:"accessing-the-hadoop-container-shell",children:"Accessing the Hadoop container shell"}),"\n",(0,n.jsx)(o.p,{children:"To open another shell to the Hadoop container, run the following command:"}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{children:"docker exec -it druid-hadoop-demo bash\n"})}),"\n",(0,n.jsx)(o.h3,{id:"test-data",children:"Test data"}),"\n",(0,n.jsxs)(o.p,{children:["The startup script ",(0,n.jsx)(o.code,{children:"bootstrap.sh"}),":"]}),"\n",(0,n.jsxs)(o.ul,{children:["\n",(0,n.jsx)(o.li,{children:"creates the necessary directories"}),"\n",(0,n.jsx)(o.li,{children:"loads an input file to HDFS"}),"\n",(0,n.jsxs)(o.li,{children:["places the hadoop configuration into the shared volume as ",(0,n.jsx)(o.code,{children:"hadoop-conf.tgz"})]}),"\n"]}),"\n",(0,n.jsx)(o.h2,{id:"configure-druid-to-use-hadoop",children:"Configure Druid to use Hadoop"}),"\n",(0,n.jsx)(o.p,{children:"Some additional steps are needed to configure the Druid cluster for Hadoop batch indexing."}),"\n",(0,n.jsx)(o.h3,{id:"provide-hadoop-configuration-for-druid",children:"Provide Hadoop configuration for Druid"}),"\n",(0,n.jsx)(o.p,{children:"From the Hadoop container's shell, run the following command to copy the Hadoop .xml configuration files to the shared folder:"}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{className:"language-bash",children:"cp /usr/local/hadoop/etc/hadoop/*.xml /shared/hadoop_xml\n"})}),"\n",(0,n.jsxs)(o.p,{children:["From the host machine, run the following, where ",(0,n.jsx)(o.code,{children:"PATH_TO_DRUID"})," is replaced by the path to the Druid package."]}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{className:"language-bash",children:"cd $PATH_TO_DRUID\nmkdir -p conf/druid/single-server/micro-quickstart/_common/hadoop-xml\ntar xzf /tmp/shared/hadoop-conf.tgz -C conf/druid/single-server/micro-quickstart/_common/hadoop-xml\n"})}),"\n",(0,n.jsx)(o.h3,{id:"update-druid-segment-and-log-storage",children:"Update Druid segment and log storage"}),"\n",(0,n.jsxs)(o.p,{children:["In your favorite text editor, open ",(0,n.jsx)(o.code,{children:"conf/druid/single-server/micro-quickstart/_common/common.runtime.properties"}),", and make the following edits:"]}),"\n",(0,n.jsx)(o.h4,{id:"disable-local-deep-storage-and-enable-hdfs-deep-storage",children:"Disable local deep storage and enable HDFS deep storage"}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{children:"#\n# Deep storage\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.storage.type=local\n#druid.storage.storageDirectory=var/druid/segments\n\n# For HDFS:\ndruid.storage.type=hdfs\ndruid.storage.storageDirectory=/druid/segments\n"})}),"\n",(0,n.jsx)(o.h4,{id:"disable-local-log-storage-and-enable-hdfs-log-storage",children:"Disable local log storage and enable HDFS log storage"}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{children:"#\n# Indexing service logs\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\n#druid.indexer.logs.type=file\n#druid.indexer.logs.directory=var/druid/indexing-logs\n\n# For HDFS:\ndruid.indexer.logs.type=hdfs\ndruid.indexer.logs.directory=/druid/indexing-logs\n\n"})}),"\n",(0,n.jsx)(o.h3,{id:"restart-druid-cluster",children:"Restart Druid cluster"}),"\n",(0,n.jsx)(o.p,{children:"Once the Hadoop .xml files have been copied to the Druid cluster and the segment/log storage configuration has been updated to use HDFS, the Druid cluster needs to be restarted for the new configurations to take effect."}),"\n",(0,n.jsx)(o.p,{children:"If the cluster is still running, CTRL-C to terminate it if running - and start it with:"}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{children:"bin/start-druid -c conf/druid/single-server/micro-quickstart\n"})}),"\n",(0,n.jsx)(o.h2,{id:"load-batch-data",children:"Load batch data"}),"\n",(0,n.jsx)(o.p,{children:"We've included a sample of Wikipedia edits from September 12, 2015 to get you started."}),"\n",(0,n.jsxs)(o.p,{children:["To load this data into Druid, you can submit an ",(0,n.jsx)(o.em,{children:"ingestion task"})," pointing to the file. We've included\na task that loads the ",(0,n.jsx)(o.code,{children:"wikiticker-2015-09-12-sampled.json.gz"})," file included in the archive."]}),"\n",(0,n.jsxs)(o.p,{children:["Let's submit the ",(0,n.jsx)(o.code,{children:"wikipedia-index-hadoop3.json"})," task:"]}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{className:"language-bash",children:"bin/post-index-task --file quickstart/tutorial/wikipedia-index-hadoop3.json --url http://localhost:8081\n"})}),"\n",(0,n.jsx)(o.h2,{id:"querying-your-data",children:"Querying your data"}),"\n",(0,n.jsxs)(o.p,{children:["After the data load is complete, please follow the ",(0,n.jsx)(o.a,{href:"/docs/33.0.0/tutorials/tutorial-query",children:"query tutorial"})," to run some example queries on the newly loaded data."]}),"\n",(0,n.jsx)(o.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,n.jsxs)(o.p,{children:["This tutorial is only meant to be used together with the ",(0,n.jsx)(o.a,{href:"/docs/33.0.0/tutorials/tutorial-query",children:"query tutorial"}),"."]}),"\n",(0,n.jsx)(o.p,{children:"If you wish to go through any of the other tutorials, you will need to:"}),"\n",(0,n.jsxs)(o.ul,{children:["\n",(0,n.jsxs)(o.li,{children:["Shut down the cluster and reset the cluster state by removing the contents of the ",(0,n.jsx)(o.code,{children:"var"})," directory under the druid package."]}),"\n",(0,n.jsxs)(o.li,{children:["Revert the deep storage and task storage config back to local types in ",(0,n.jsx)(o.code,{children:"conf/druid/single-server/micro-quickstart/_common/common.runtime.properties"})]}),"\n",(0,n.jsx)(o.li,{children:"Restart the cluster"}),"\n"]}),"\n",(0,n.jsx)(o.p,{children:'This is necessary because the other ingestion tutorials will write to the same "wikipedia" datasource, and later tutorials expect the cluster to use local deep storage.'}),"\n",(0,n.jsx)(o.p,{children:"Example reverted config:"}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{children:"#\n# Deep storage\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\ndruid.storage.type=local\ndruid.storage.storageDirectory=var/druid/segments\n\n# For HDFS:\n#druid.storage.type=hdfs\n#druid.storage.storageDirectory=/druid/segments\n\n#\n# Indexing service logs\n#\n\n# For local disk (only viable in a cluster if this is a network mount):\ndruid.indexer.logs.type=file\ndruid.indexer.logs.directory=var/druid/indexing-logs\n\n# For HDFS:\n#druid.indexer.logs.type=hdfs\n#druid.indexer.logs.directory=/druid/indexing-logs\n\n"})}),"\n",(0,n.jsx)(o.h2,{id:"further-reading",children:"Further reading"}),"\n",(0,n.jsxs)(o.p,{children:["For more information on loading batch data with Hadoop, please see ",(0,n.jsx)(o.a,{href:"/docs/33.0.0/ingestion/hadoop",children:"the Hadoop batch ingestion documentation"}),"."]})]})}function h(e={}){const{wrapper:o}={...(0,a.R)(),...e.components};return o?(0,n.jsx)(o,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},28453:(e,o,t)=>{t.d(o,{R:()=>d,x:()=>i});var r=t(96540);const n={},a=r.createContext(n);function d(e){const o=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(o):{...o,...e}}),[o,e])}function i(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:d(e.components),r.createElement(a.Provider,{value:o},e.children)}}}]);