"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[3039],{28453:(e,r,n)=>{n.d(r,{R:()=>o,x:()=>d});var i=n(96540);const s={},t=i.createContext(s);function o(e){const r=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(r):{...r,...e}}),[r,e])}function d(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(t.Provider,{value:r},e.children)}},28579:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>a,contentTitle:()=>d,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"querying/groupbyquery","title":"GroupBy queries","description":"\x3c!--","source":"@site/docs/32.0.0/querying/groupbyquery.md","sourceDirName":"querying","slug":"/querying/groupbyquery","permalink":"/docs/32.0.0/querying/groupbyquery","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"groupbyquery","title":"GroupBy queries","sidebar_label":"GroupBy"},"sidebar":"docs","previous":{"title":"TopN","permalink":"/docs/32.0.0/querying/topnquery"},"next":{"title":"Scan","permalink":"/docs/32.0.0/querying/scan-query"}}');var s=n(74848),t=n(28453);const o={id:"groupbyquery",title:"GroupBy queries",sidebar_label:"GroupBy"},d=void 0,a={},l=[{value:"Behavior on multi-value dimensions",id:"behavior-on-multi-value-dimensions",level:2},{value:"More on subtotalsSpec",id:"more-on-subtotalsspec",level:2},{value:"Implementation details",id:"implementation-details",level:2},{value:"Memory tuning and resource limits",id:"memory-tuning-and-resource-limits",level:3},{value:"Performance tuning for groupBy",id:"performance-tuning-for-groupby",level:3},{value:"Limit pushdown optimization",id:"limit-pushdown-optimization",level:4},{value:"Optimizing hash table",id:"optimizing-hash-table",level:4},{value:"Parallel combine",id:"parallel-combine",level:4},{value:"Alternatives",id:"alternatives",level:3},{value:"Nested groupBys",id:"nested-groupbys",level:3},{value:"Configurations",id:"configurations",level:3},{value:"Advanced configurations",id:"advanced-configurations",level:3},{value:"Array based result rows",id:"array-based-result-rows",level:4}];function c(e){const r={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.admonition,{type:"info",children:(0,s.jsxs)(r.p,{children:["Apache Druid supports two query languages: ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/sql",children:"Druid SQL"})," and ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/",children:"native queries"}),".\nThis document describes a query\ntype in the native language. For information about when Druid SQL will use this query type, refer to the\n",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/sql-translation#query-types",children:"SQL documentation"}),"."]})}),"\n",(0,s.jsx)(r.p,{children:"These types of Apache Druid queries take a groupBy query object and return an array of JSON objects where each object represents a\ngrouping asked for by the query."}),"\n",(0,s.jsx)(r.admonition,{type:"info",children:(0,s.jsxs)(r.p,{children:["Note: If you are doing aggregations with time as your only grouping, or an ordered groupBy over a single dimension,\nconsider ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/timeseriesquery",children:"Timeseries"})," and ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/topnquery",children:"TopN"})," queries as well as\ngroupBy. Their performance may be better in some cases. See ",(0,s.jsx)(r.a,{href:"#alternatives",children:"Alternatives"})," below for more details."]})}),"\n",(0,s.jsx)(r.p,{children:"An example groupBy query object is shown below:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-json",children:'{\n  "queryType": "groupBy",\n  "dataSource": "sample_datasource",\n  "granularity": "day",\n  "dimensions": ["country", "device"],\n  "limitSpec": { "type": "default", "limit": 5000, "columns": ["country", "data_transfer"] },\n  "filter": {\n    "type": "and",\n    "fields": [\n      { "type": "selector", "dimension": "carrier", "value": "AT&T" },\n      { "type": "or",\n        "fields": [\n          { "type": "selector", "dimension": "make", "value": "Apple" },\n          { "type": "selector", "dimension": "make", "value": "Samsung" }\n        ]\n      }\n    ]\n  },\n  "aggregations": [\n    { "type": "longSum", "name": "total_usage", "fieldName": "user_count" },\n    { "type": "doubleSum", "name": "data_transfer", "fieldName": "data_transfer" }\n  ],\n  "postAggregations": [\n    { "type": "arithmetic",\n      "name": "avg_usage",\n      "fn": "/",\n      "fields": [\n        { "type": "fieldAccess", "fieldName": "data_transfer" },\n        { "type": "fieldAccess", "fieldName": "total_usage" }\n      ]\n    }\n  ],\n  "intervals": [ "2012-01-01T00:00:00.000/2012-01-03T00:00:00.000" ],\n  "having": {\n    "type": "greaterThan",\n    "aggregation": "total_usage",\n    "value": 100\n  }\n}\n'})}),"\n",(0,s.jsx)(r.p,{children:"Following are main parts to a groupBy query:"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"property"}),(0,s.jsx)(r.th,{children:"description"}),(0,s.jsx)(r.th,{children:"required?"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"queryType"}),(0,s.jsx)(r.td,{children:'This String should always be "groupBy"; this is the first thing Druid looks at to figure out how to interpret the query'}),(0,s.jsx)(r.td,{children:"yes"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"dataSource"}),(0,s.jsxs)(r.td,{children:["A String or Object defining the data source to query, very similar to a table in a relational database. See ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/datasource",children:"DataSource"})," for more information."]}),(0,s.jsx)(r.td,{children:"yes"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"dimensions"}),(0,s.jsxs)(r.td,{children:["A JSON list of dimensions to do the groupBy over; or see ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/dimensionspecs",children:"DimensionSpec"})," for ways to extract dimensions."]}),(0,s.jsx)(r.td,{children:"yes"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"virtualColumns"}),(0,s.jsxs)(r.td,{children:["A JSON list of ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/virtual-columns",children:"virtual columns"}),". You can reference the virtual columns in ",(0,s.jsx)(r.code,{children:"dimensions"}),", ",(0,s.jsx)(r.code,{children:"aggregations"}),", or ",(0,s.jsx)(r.code,{children:"postAggregations"}),"."]}),(0,s.jsx)(r.td,{children:"no (default none)"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"limitSpec"}),(0,s.jsxs)(r.td,{children:["See ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/limitspec",children:"LimitSpec"}),"."]}),(0,s.jsx)(r.td,{children:"no"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"having"}),(0,s.jsxs)(r.td,{children:["See ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/having",children:"Having"}),"."]}),(0,s.jsx)(r.td,{children:"no"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"granularity"}),(0,s.jsxs)(r.td,{children:["Defines the granularity of the query. See ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/granularities",children:"Granularities"})]}),(0,s.jsx)(r.td,{children:"yes"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"filter"}),(0,s.jsxs)(r.td,{children:["See ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/filters",children:"Filters"})]}),(0,s.jsx)(r.td,{children:"no"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"aggregations"}),(0,s.jsxs)(r.td,{children:["See ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/aggregations",children:"Aggregations"})]}),(0,s.jsx)(r.td,{children:"no"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"postAggregations"}),(0,s.jsxs)(r.td,{children:["See ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/post-aggregations",children:"Post Aggregations"})]}),(0,s.jsx)(r.td,{children:"no"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"intervals"}),(0,s.jsx)(r.td,{children:"A JSON Object representing ISO-8601 Intervals. This defines the time ranges to run the query over."}),(0,s.jsx)(r.td,{children:"yes"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"subtotalsSpec"}),(0,s.jsxs)(r.td,{children:["A JSON array of arrays to return additional result sets for groupings of subsets of top level ",(0,s.jsx)(r.code,{children:"dimensions"}),". It is ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/groupbyquery#more-on-subtotalsspec",children:"described later"})," in more detail."]}),(0,s.jsx)(r.td,{children:"no"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:"context"}),(0,s.jsx)(r.td,{children:"An additional JSON Object which can be used to specify certain flags."}),(0,s.jsx)(r.td,{children:"no"})]})]})]}),"\n",(0,s.jsxs)(r.p,{children:["To pull it all together, the above query would return ",(0,s.jsx)(r.em,{children:"n*m"})," data points, up to a maximum of 5000 points, where n is the cardinality of the ",(0,s.jsx)(r.code,{children:"country"})," dimension, m is the cardinality of the ",(0,s.jsx)(r.code,{children:"device"})," dimension, each day between 2012-01-01 and 2012-01-03, from the ",(0,s.jsx)(r.code,{children:"sample_datasource"})," table. Each data point contains the (long) sum of ",(0,s.jsx)(r.code,{children:"total_usage"})," if the value of the data point is greater than 100, the (double) sum of ",(0,s.jsx)(r.code,{children:"data_transfer"})," and the (double) result of ",(0,s.jsx)(r.code,{children:"total_usage"})," divided by ",(0,s.jsx)(r.code,{children:"data_transfer"})," for the filter set for a particular grouping of ",(0,s.jsx)(r.code,{children:"country"})," and ",(0,s.jsx)(r.code,{children:"device"}),". The output looks like this:"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-json",children:'[\n  {\n    "version" : "v1",\n    "timestamp" : "2012-01-01T00:00:00.000Z",\n    "event" : {\n      "country" : <some_dim_value_one>,\n      "device" : <some_dim_value_two>,\n      "total_usage" : <some_value_one>,\n      "data_transfer" :<some_value_two>,\n      "avg_usage" : <some_avg_usage_value>\n    }\n  },\n  {\n    "version" : "v1",\n    "timestamp" : "2012-01-01T00:00:12.000Z",\n    "event" : {\n      "dim1" : <some_other_dim_value_one>,\n      "dim2" : <some_other_dim_value_two>,\n      "sample_name1" : <some_other_value_one>,\n      "sample_name2" :<some_other_value_two>,\n      "avg_usage" : <some_other_avg_usage_value>\n    }\n  },\n...\n]\n'})}),"\n",(0,s.jsx)(r.h2,{id:"behavior-on-multi-value-dimensions",children:"Behavior on multi-value dimensions"}),"\n",(0,s.jsxs)(r.p,{children:["groupBy queries can group on multi-value dimensions. When grouping on a multi-value dimension, ",(0,s.jsx)(r.em,{children:"all"})," values\nfrom matching rows will be used to generate one group per value. It's possible for a query to return more groups than\nthere are rows. For example, a groupBy on the dimension ",(0,s.jsx)(r.code,{children:"tags"})," with filter ",(0,s.jsx)(r.code,{children:'"t1" AND "t3"'})," would match only row1, and\ngenerate a result with three groups: ",(0,s.jsx)(r.code,{children:"t1"}),", ",(0,s.jsx)(r.code,{children:"t2"}),", and ",(0,s.jsx)(r.code,{children:"t3"}),". If you only need to include values that match\nyour filter, you can use a ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/dimensionspecs#filtered-dimensionspecs",children:"filtered dimensionSpec"}),". This can also\nimprove performance."]}),"\n",(0,s.jsxs)(r.p,{children:["See ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/multi-value-dimensions",children:"Multi-value dimensions"})," for more details."]}),"\n",(0,s.jsx)(r.h2,{id:"more-on-subtotalsspec",children:"More on subtotalsSpec"}),"\n",(0,s.jsxs)(r.p,{children:['The subtotals feature allows computation of multiple sub-groupings in a single query. To use this feature, add a "subtotalsSpec" to your query as a list of subgroup dimension sets. It should contain the ',(0,s.jsx)(r.code,{children:"outputName"})," from dimensions in your ",(0,s.jsx)(r.code,{children:"dimensions"})," attribute, in the same order as they appear in the ",(0,s.jsx)(r.code,{children:"dimensions"})," attribute (although, of course, you may skip some)."]}),"\n",(0,s.jsx)(r.p,{children:"For example, consider a groupBy query like this one:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-json",children:'{\n"type": "groupBy",\n ...\n ...\n"dimensions": [\n  {\n  "type" : "default",\n  "dimension" : "d1col",\n  "outputName": "D1"\n  },\n  {\n  "type" : "extraction",\n  "dimension" : "d2col",\n  "outputName" :  "D2",\n  "extractionFn" : extraction_func\n  },\n  {\n  "type":"lookup",\n  "dimension":"d3col",\n  "outputName":"D3",\n  "name":"my_lookup"\n  }\n],\n...\n...\n"subtotalsSpec":[ ["D1", "D2", D3"], ["D1", "D3"], ["D3"]],\n..\n\n}\n'})}),"\n",(0,s.jsxs)(r.p,{children:['The result of the subtotalsSpec would be equivalent to concatenating the result of three groupBy queries, with the "dimensions" field being ',(0,s.jsx)(r.code,{children:'["D1", "D2", D3"]'}),", ",(0,s.jsx)(r.code,{children:'["D1", "D3"]'})," and ",(0,s.jsx)(r.code,{children:'["D3"]'}),", given the ",(0,s.jsx)(r.code,{children:"DimensionSpec"})," shown above.\nThe response for the query above would look something like:"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-json",children:'[\n  {\n    "version" : "v1",\n    "timestamp" : "t1",\n    "event" : { "D1": "..", "D2": "..", "D3": ".." }\n    }\n  },\n    {\n    "version" : "v1",\n    "timestamp" : "t2",\n    "event" : { "D1": "..", "D2": "..", "D3": ".." }\n    }\n  },\n  ...\n  ...\n\n   {\n    "version" : "v1",\n    "timestamp" : "t1",\n    "event" : { "D1": "..", "D2": null, "D3": ".." }\n    }\n  },\n    {\n    "version" : "v1",\n    "timestamp" : "t2",\n    "event" : { "D1": "..", "D2": null, "D3": ".." }\n    }\n  },\n  ...\n  ...\n\n  {\n    "version" : "v1",\n    "timestamp" : "t1",\n    "event" : { "D1": null, "D2": null, "D3": ".." }\n    }\n  },\n    {\n    "version" : "v1",\n    "timestamp" : "t2",\n    "event" : { "D1": null, "D2": null, "D3": ".." }\n    }\n  },\n...\n]\n'})}),"\n",(0,s.jsx)(r.admonition,{type:"info",children:(0,s.jsxs)(r.p,{children:["Notice that dimensions that are not included in an individual subtotalsSpec grouping are returned with a ",(0,s.jsx)(r.code,{children:"null"})," value. This response format represents a behavior change as of Apache Druid 0.18.0.\nIn release 0.17.0 and earlier, such dimensions were entirely excluded from the result. If you were relying on this old behavior to determine whether a particular dimension was not part of\na subtotal grouping, you can now use ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/aggregations#grouping-aggregator",children:"Grouping aggregator"})," instead."]})}),"\n",(0,s.jsx)(r.h2,{id:"implementation-details",children:"Implementation details"}),"\n",(0,s.jsx)(r.h3,{id:"memory-tuning-and-resource-limits",children:"Memory tuning and resource limits"}),"\n",(0,s.jsx)(r.p,{children:"When using groupBy, four parameters control resource usage and limits:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"druid.processing.buffer.sizeBytes"}),": size of the off-heap hash table used for aggregation, per query, in bytes. At\nmost ",(0,s.jsx)(r.code,{children:"druid.processing.numMergeBuffers"})," of these will be created at once, which also serves as an upper limit on the\nnumber of concurrently running groupBy queries."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"druid.query.groupBy.maxSelectorDictionarySize"}),": size of the on-heap segment-level dictionary used when grouping on\nstring or array-valued expressions that do not have pre-existing dictionaries. There is at most one dictionary per\nprocessing thread; therefore there are up to ",(0,s.jsx)(r.code,{children:"druid.processing.numThreads"})," of these. Note that the size is based on a\nrough estimate of the dictionary footprint."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"druid.query.groupBy.maxMergingDictionarySize"}),": size of the on-heap query-level dictionary used when grouping on\nany string expression. There is at most one dictionary per concurrently-running query; therefore there are up to\n",(0,s.jsx)(r.code,{children:"druid.server.http.numThreads"})," of these. Note that the size is based on a rough estimate of the dictionary footprint."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"druid.query.groupBy.maxOnDiskStorage"}),": amount of space on disk used for aggregation, per query, in bytes. By default,\nthis is 0, which means aggregation will not use disk."]}),"\n"]}),"\n",(0,s.jsxs)(r.p,{children:["If ",(0,s.jsx)(r.code,{children:"maxOnDiskStorage"}),' is 0 (the default) then a query that exceeds either the on-heap dictionary limit, or the off-heap\naggregation table limit, will fail with a "Resource limit exceeded" error describing the limit that was exceeded.']}),"\n",(0,s.jsxs)(r.p,{children:["If ",(0,s.jsx)(r.code,{children:"maxOnDiskStorage"})," is greater than 0, queries that exceed the in-memory limits will start using disk for aggregation.\nIn this case, when either the on-heap dictionary or off-heap hash table fills up, partially aggregated records will be\nsorted and flushed to disk. Then, both in-memory structures will be cleared out for further aggregation. Queries that\nthen go on to exceed ",(0,s.jsx)(r.code,{children:"maxOnDiskStorage"}),' will fail with a "Resource limit exceeded" error indicating that they ran out of\ndisk space.']}),"\n",(0,s.jsxs)(r.p,{children:["With groupBy, cluster operators should make sure that the off-heap hash tables and on-heap merging dictionaries\nwill not exceed available memory for the maximum possible concurrent query load (given by\n",(0,s.jsx)(r.code,{children:"druid.processing.numMergeBuffers"}),"). See the ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/operations/basic-cluster-tuning",children:"basic cluster tuning guide"}),"\nfor more details about direct memory usage, organized by Druid process type."]}),"\n",(0,s.jsxs)(r.p,{children:["Brokers do not need merge buffers for basic groupBy queries. Queries with subqueries (using a ",(0,s.jsx)(r.code,{children:"query"})," dataSource) require one merge buffer if there is a single subquery, or two merge buffers if there is more than one layer of nested subqueries. Queries with ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/groupbyquery#more-on-subtotalsspec",children:"subtotals"})," need one merge buffer. These can stack on top of each other: a groupBy query with multiple layers of nested subqueries, and that also uses subtotals, will need three merge buffers."]}),"\n",(0,s.jsxs)(r.p,{children:["Historicals and ingestion tasks need one merge buffer for each groupBy query, unless ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/groupbyquery#parallel-combine",children:"parallel combination"})," is enabled, in which case they need two merge buffers per query."]}),"\n",(0,s.jsx)(r.h3,{id:"performance-tuning-for-groupby",children:"Performance tuning for groupBy"}),"\n",(0,s.jsx)(r.h4,{id:"limit-pushdown-optimization",children:"Limit pushdown optimization"}),"\n",(0,s.jsxs)(r.p,{children:["Druid pushes down the ",(0,s.jsx)(r.code,{children:"limit"})," spec in groupBy queries to the segments on Historicals wherever possible to early prune unnecessary intermediate results and minimize the amount of data transferred to Brokers. By default, this technique is applied only when all fields in the ",(0,s.jsx)(r.code,{children:"orderBy"})," spec is a subset of the grouping keys. This is because the ",(0,s.jsx)(r.code,{children:"limitPushDown"})," doesn't guarantee the exact results if the ",(0,s.jsx)(r.code,{children:"orderBy"})," spec includes any fields that are not in the grouping keys. However, you can enable this technique even in such cases if you can sacrifice some accuracy for fast query processing like in topN queries. See ",(0,s.jsx)(r.code,{children:"forceLimitPushDown"})," in ",(0,s.jsx)(r.a,{href:"#advanced-configurations",children:"advanced configurations"}),"."]}),"\n",(0,s.jsx)(r.h4,{id:"optimizing-hash-table",children:"Optimizing hash table"}),"\n",(0,s.jsx)(r.p,{children:"The groupBy engine uses an open addressing hash table for aggregation. The hash table is initialized with a given initial bucket number and gradually grows on buffer full. On hash collisions, the linear probing technique is used."}),"\n",(0,s.jsxs)(r.p,{children:["The default number of initial buckets is 1024 and the default max load factor of the hash table is 0.7. If you can see too many collisions in the hash table, you can adjust these numbers. See ",(0,s.jsx)(r.code,{children:"bufferGrouperInitialBuckets"})," and ",(0,s.jsx)(r.code,{children:"bufferGrouperMaxLoadFactor"})," in ",(0,s.jsx)(r.a,{href:"#advanced-configurations",children:"advanced configurations"}),"."]}),"\n",(0,s.jsx)(r.h4,{id:"parallel-combine",children:"Parallel combine"}),"\n",(0,s.jsxs)(r.p,{children:["Once a Historical finishes aggregation using the hash table, it sorts the aggregated results and merges them before sending to the\nBroker for N-way merge aggregation in the broker. By default, Historicals use all their available processing threads\n(configured by ",(0,s.jsx)(r.code,{children:"druid.processing.numThreads"}),") for aggregation, but use a single thread for sorting and merging\naggregates which is an http thread to send data to Brokers."]}),"\n",(0,s.jsxs)(r.p,{children:["This is to prevent some heavy groupBy queries from blocking other queries. In Druid, the processing threads are shared\nbetween all submitted queries and they are ",(0,s.jsx)(r.em,{children:"not interruptible"}),". It means, if a heavy query takes all available\nprocessing threads, all other queries might be blocked until the heavy query is finished. GroupBy queries usually take\nlonger time than timeseries or topN queries, they should release processing threads as soon as possible."]}),"\n",(0,s.jsxs)(r.p,{children:["However, you might care about the performance of some really heavy groupBy queries. Usually, the performance bottleneck\nof heavy groupBy queries is merging sorted aggregates. In such cases, you can use processing threads for it as well.\nThis is called ",(0,s.jsx)(r.em,{children:"parallel combine"}),". To enable parallel combine, see ",(0,s.jsx)(r.code,{children:"numParallelCombineThreads"})," in\n",(0,s.jsx)(r.a,{href:"#advanced-configurations",children:"advanced configurations"}),". Note that parallel combine can be enabled only when\ndata is actually spilled (see ",(0,s.jsx)(r.a,{href:"#memory-tuning-and-resource-limits",children:"Memory tuning and resource limits"}),")."]}),"\n",(0,s.jsxs)(r.p,{children:["Once parallel combine is enabled, the groupBy engine can create a combining tree for merging sorted aggregates. Each\nintermediate node of the tree is a thread merging aggregates from the child nodes. The leaf node threads read and merge\naggregates from hash tables including spilled ones. Usually, leaf processes are slower than intermediate nodes because they\nneed to read data from disk. As a result, less threads are used for intermediate nodes by default. You can change the\ndegree of intermediate nodes. See ",(0,s.jsx)(r.code,{children:"intermediateCombineDegree"})," in ",(0,s.jsx)(r.a,{href:"#advanced-configurations",children:"advanced configurations"}),"."]}),"\n",(0,s.jsx)(r.p,{children:"Please note that each Historical needs two merge buffers to process a groupBy query with parallel combine: one for\ncomputing intermediate aggregates from each segment and another for combining intermediate aggregates in parallel."}),"\n",(0,s.jsx)(r.h3,{id:"alternatives",children:"Alternatives"}),"\n",(0,s.jsx)(r.p,{children:"There are some situations where other query types may be a better choice than groupBy."}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:['For queries with no "dimensions" (i.e. grouping by time only) the ',(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/timeseriesquery",children:"Timeseries query"})," will\ngenerally be faster\xa0than groupBy. The major differences are that it is implemented in a fully streaming manner (taking\nadvantage of the fact that segments are already sorted on time) and does not need to use a hash table for merging."]}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:["\n",(0,s.jsxs)(r.p,{children:['For queries with a single "dimensions" element (i.e. grouping by one string dimension), the ',(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/topnquery",children:"TopN query"}),"\nwill sometimes be faster than groupBy. This is especially true if you are ordering by a metric and find approximate\nresults acceptable."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"nested-groupbys",children:"Nested groupBys"}),"\n",(0,s.jsx)(r.p,{children:'Nested groupBys (dataSource of type "query") are performed with the Broker first running the inner groupBy query in the\nusual way. Next, the outer query is run on the inner query\'s results stream with off-heap fact map and on-heap string\ndictionary that can spill to disk. The outer query is run on the Broker in a single-threaded fashion.'}),"\n",(0,s.jsx)(r.h3,{id:"configurations",children:"Configurations"}),"\n",(0,s.jsxs)(r.p,{children:["This section describes the configurations for groupBy queries. You can set the runtime properties in the ",(0,s.jsx)(r.code,{children:"runtime.properties"})," file on Broker, Historical, and Middle Manager processes. You can set the query context parameters through the ",(0,s.jsx)(r.a,{href:"/docs/32.0.0/querying/query-context",children:"query context"}),"."]}),"\n",(0,s.jsx)(r.p,{children:"Supported runtime properties:"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Property"}),(0,s.jsx)(r.th,{children:"Description"}),(0,s.jsx)(r.th,{children:"Default"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"druid.query.groupBy.maxSelectorDictionarySize"})}),(0,s.jsxs)(r.td,{children:["Maximum amount of heap space (approximately) to use for per-segment string dictionaries.  If set to ",(0,s.jsx)(r.code,{children:"0"})," (automatic), each query's dictionary can use 10% of the Java heap divided by ",(0,s.jsx)(r.code,{children:"druid.processing.numMergeBuffers"}),", or 1GB, whichever is smaller.",(0,s.jsx)("br",{}),(0,s.jsx)("br",{}),"See ",(0,s.jsx)(r.a,{href:"#memory-tuning-and-resource-limits",children:"Memory tuning and resource limits"})," for details on changing this property."]}),(0,s.jsx)(r.td,{children:"0 (automatic)"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"druid.query.groupBy.maxMergingDictionarySize"})}),(0,s.jsxs)(r.td,{children:["Maximum amount of heap space (approximately) to use for per-query string dictionaries. When the dictionary exceeds this size, a spill to disk will be triggered. If set to ",(0,s.jsx)(r.code,{children:"0"})," (automatic), each query's dictionary uses 30% of the Java heap divided by ",(0,s.jsx)(r.code,{children:"druid.processing.numMergeBuffers"}),", or 1GB, whichever is smaller.",(0,s.jsx)("br",{}),(0,s.jsx)("br",{}),"See ",(0,s.jsx)(r.a,{href:"#memory-tuning-and-resource-limits",children:"Memory tuning and resource limits"})," for details on changing this property."]}),(0,s.jsx)(r.td,{children:"0 (automatic)"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"druid.query.groupBy.maxOnDiskStorage"})}),(0,s.jsx)(r.td,{children:"Maximum amount of disk space to use, per-query, for spilling result sets to disk when either the merging buffer or the dictionary fills up. Queries that exceed this limit will fail. Set to zero to disable disk spilling."}),(0,s.jsx)(r.td,{children:"0 (disabled)"})]})]})]}),"\n",(0,s.jsx)(r.p,{children:"Supported query contexts:"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Key"}),(0,s.jsx)(r.th,{children:"Description"})]})}),(0,s.jsx)(r.tbody,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"maxOnDiskStorage"})}),(0,s.jsxs)(r.td,{children:["Can be used to lower the value of ",(0,s.jsx)(r.code,{children:"druid.query.groupBy.maxOnDiskStorage"})," for this query."]})]})})]}),"\n",(0,s.jsx)(r.h3,{id:"advanced-configurations",children:"Advanced configurations"}),"\n",(0,s.jsx)(r.p,{children:"Supported runtime properties:"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Property"}),(0,s.jsx)(r.th,{children:"Description"}),(0,s.jsx)(r.th,{children:"Default"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"druid.query.groupBy.singleThreaded"})}),(0,s.jsx)(r.td,{children:"Merge results using a single thread."}),(0,s.jsx)(r.td,{children:"false"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"druid.query.groupBy.intermediateResultAsMapCompat"})}),(0,s.jsxs)(r.td,{children:["Whether Brokers are able to understand map-based result rows. Setting this to ",(0,s.jsx)(r.code,{children:"true"})," adds some overhead to all groupBy queries. It is required for compatibility with data servers running versions older than 0.16.0, which introduced ",(0,s.jsx)(r.a,{href:"#array-based-result-rows",children:"array-based result rows"}),"."]}),(0,s.jsx)(r.td,{children:"false"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"druid.query.groupBy.bufferGrouperInitialBuckets"})}),(0,s.jsx)(r.td,{children:"Initial number of buckets in the off-heap hash table used for grouping results. Set to 0 to use a reasonable default (1024)."}),(0,s.jsx)(r.td,{children:"0"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"druid.query.groupBy.bufferGrouperMaxLoadFactor"})}),(0,s.jsx)(r.td,{children:"Maximum load factor of the off-heap hash table used for grouping results. When the load factor exceeds this size, the table will be grown or spilled to disk. Set to 0 to use a reasonable default (0.7)."}),(0,s.jsx)(r.td,{children:"0"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"druid.query.groupBy.forceHashAggregation"})}),(0,s.jsx)(r.td,{children:"Force to use hash-based aggregation."}),(0,s.jsx)(r.td,{children:"false"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"druid.query.groupBy.intermediateCombineDegree"})}),(0,s.jsx)(r.td,{children:"Number of intermediate nodes combined together in the combining tree. Higher degrees will need less threads which might be helpful to improve the query performance by reducing the overhead of too many threads if the server has sufficiently powerful cpu cores."}),(0,s.jsx)(r.td,{children:"8"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"druid.query.groupBy.numParallelCombineThreads"})}),(0,s.jsxs)(r.td,{children:["Hint for the number of parallel combining threads. This should be larger than 1 to turn on the parallel combining feature. The actual number of threads used for parallel combining is min(",(0,s.jsx)(r.code,{children:"druid.query.groupBy.numParallelCombineThreads"}),", ",(0,s.jsx)(r.code,{children:"druid.processing.numThreads"}),")."]}),(0,s.jsx)(r.td,{children:"1 (disabled)"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"druid.query.groupBy.applyLimitPushDownToSegment"})}),(0,s.jsx)(r.td,{children:"If Broker pushes limit down to queryable data server (historicals, peons) then limit results during segment scan. If typically there are a large number of segments taking part in a query on a data server, this setting may counterintuitively reduce performance if enabled."}),(0,s.jsx)(r.td,{children:"false (disabled)"})]})]})]}),"\n",(0,s.jsx)(r.p,{children:"Supported query contexts:"}),"\n",(0,s.jsxs)(r.table,{children:[(0,s.jsx)(r.thead,{children:(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.th,{children:"Key"}),(0,s.jsx)(r.th,{children:"Description"}),(0,s.jsx)(r.th,{children:"Default"})]})}),(0,s.jsxs)(r.tbody,{children:[(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"groupByIsSingleThreaded"})}),(0,s.jsxs)(r.td,{children:["Overrides the value of ",(0,s.jsx)(r.code,{children:"druid.query.groupBy.singleThreaded"})," for this query."]}),(0,s.jsx)(r.td,{children:"None"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"bufferGrouperInitialBuckets"})}),(0,s.jsxs)(r.td,{children:["Overrides the value of ",(0,s.jsx)(r.code,{children:"druid.query.groupBy.bufferGrouperInitialBuckets"})," for this query."]}),(0,s.jsx)(r.td,{children:"None"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"bufferGrouperMaxLoadFactor"})}),(0,s.jsxs)(r.td,{children:["Overrides the value of ",(0,s.jsx)(r.code,{children:"druid.query.groupBy.bufferGrouperMaxLoadFactor"})," for this query."]}),(0,s.jsx)(r.td,{children:"None"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"forceHashAggregation"})}),(0,s.jsxs)(r.td,{children:["Overrides the value of ",(0,s.jsx)(r.code,{children:"druid.query.groupBy.forceHashAggregation"})]}),(0,s.jsx)(r.td,{children:"None"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"intermediateCombineDegree"})}),(0,s.jsxs)(r.td,{children:["Overrides the value of ",(0,s.jsx)(r.code,{children:"druid.query.groupBy.intermediateCombineDegree"})]}),(0,s.jsx)(r.td,{children:"None"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"numParallelCombineThreads"})}),(0,s.jsxs)(r.td,{children:["Overrides the value of ",(0,s.jsx)(r.code,{children:"druid.query.groupBy.numParallelCombineThreads"})]}),(0,s.jsx)(r.td,{children:"None"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"maxSelectorDictionarySize"})}),(0,s.jsxs)(r.td,{children:["Overrides the value of ",(0,s.jsx)(r.code,{children:"druid.query.groupBy.maxMergingDictionarySize"})]}),(0,s.jsx)(r.td,{children:"None"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"maxMergingDictionarySize"})}),(0,s.jsxs)(r.td,{children:["Overrides the value of ",(0,s.jsx)(r.code,{children:"druid.query.groupBy.maxMergingDictionarySize"})]}),(0,s.jsx)(r.td,{children:"None"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"mergeThreadLocal"})}),(0,s.jsxs)(r.td,{children:["Whether merge buffers should always be split into thread-local buffers. Setting this to ",(0,s.jsx)(r.code,{children:"true"})," reduces thread contention, but uses memory less efficiently. This tradeoff is beneficial when memory is plentiful."]}),(0,s.jsx)(r.td,{children:"false"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"sortByDimsFirst"})}),(0,s.jsx)(r.td,{children:"Sort the results first by dimension values and then by timestamp."}),(0,s.jsx)(r.td,{children:"false"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"forceLimitPushDown"})}),(0,s.jsx)(r.td,{children:"When all fields in the orderby are part of the grouping key, the Broker will push limit application down to the Historical processes. When the sorting order uses fields that are not in the grouping key, applying this optimization can result in approximate results with unknown accuracy, so this optimization is disabled by default in that case. Enabling this context flag turns on limit push down for limit/orderbys that contain non-grouping key columns."}),(0,s.jsx)(r.td,{children:"false"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"applyLimitPushDownToSegment"})}),(0,s.jsxs)(r.td,{children:["If Broker pushes limit down to queryable nodes (historicals, peons) then limit results during segment scan. This context value can be used to override ",(0,s.jsx)(r.code,{children:"druid.query.groupBy.applyLimitPushDownToSegment"}),"."]}),(0,s.jsx)(r.td,{children:"true"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"groupByEnableMultiValueUnnesting"})}),(0,s.jsx)(r.td,{children:"Safety flag to enable/disable the implicit unnesting on multi value column's as part of the grouping key. 'true' indicates multi-value grouping keys are unnested. 'false' returns an error if a multi value column is found as part of the grouping key."}),(0,s.jsx)(r.td,{children:"true"})]}),(0,s.jsxs)(r.tr,{children:[(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"deferExpressionDimensions"})}),(0,s.jsxs)(r.td,{children:["When an entry in ",(0,s.jsx)(r.code,{children:"dimensions"})," references an ",(0,s.jsx)(r.code,{children:"expression"})," virtual column, this property influences whether expression evaluation is deferred from cursor processing to the merge step. Options are:",(0,s.jsxs)("ul",{children:[(0,s.jsxs)("li",{children:[(0,s.jsx)(r.code,{children:"fixedWidth"}),": Defer expressions with fixed-width inputs (numeric and dictionary-encoded string)."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)(r.code,{children:"fixedWidthNonNumeric"}),": Defer expressions with fixed-width inputs (numeric and dictionary-encoded string), unless the expression output and all inputs are numeric."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)(r.code,{children:"singleString"}),": Defer string-typed expressions with a single dictionary-encoded string input."]}),(0,s.jsxs)("li",{children:[(0,s.jsx)(r.code,{children:"always"}),": Defer all expressions. May require building dictionaries for expression inputs."]})]}),(0,s.jsx)("br",{}),"These properties only take effect when the ",(0,s.jsx)(r.code,{children:"groupBy"})," query can be vectorized. Non-vectorized queries only defer string-typed expressions of single string inputs."]}),(0,s.jsx)(r.td,{children:(0,s.jsx)(r.code,{children:"fixedWidthNonNumeric"})})]})]})]}),"\n",(0,s.jsx)(r.h4,{id:"array-based-result-rows",children:"Array based result rows"}),"\n",(0,s.jsxs)(r.p,{children:["Internally Druid always uses an array based representation of groupBy result rows, but by default this is translated\ninto a map based result format at the Broker. To reduce the overhead of this translation, results may also be returned\nfrom the Broker directly in the array based format if ",(0,s.jsx)(r.code,{children:"resultAsArray"})," is set to ",(0,s.jsx)(r.code,{children:"true"})," on the query context."]}),"\n",(0,s.jsx)(r.p,{children:"Each row is positional, and has the following fields, in order:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Timestamp (optional; only if granularity != ALL)"}),"\n",(0,s.jsx)(r.li,{children:"Dimensions (in order)"}),"\n",(0,s.jsx)(r.li,{children:"Aggregators (in order)"}),"\n",(0,s.jsx)(r.li,{children:"Post-aggregators (optional; in order, if present)"}),"\n"]}),"\n",(0,s.jsx)(r.p,{children:"This schema is not available on the response, so it must be computed from the issued query in order to properly read\nthe results."})]})}function h(e={}){const{wrapper:r}={...(0,t.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);