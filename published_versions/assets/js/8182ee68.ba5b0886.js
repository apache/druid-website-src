"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[4676],{28453:(e,o,t)=>{t.d(o,{R:()=>i,x:()=>d});var s=t(96540);const n={},r=s.createContext(n);function i(e){const o=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(o):{...o,...e}}),[o,e])}function d(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:i(e.components),s.createElement(r.Provider,{value:o},e.children)}},51329:(e,o,t)=>{t.r(o),t.d(o,{assets:()=>a,contentTitle:()=>d,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"development/extensions-core/hdfs","title":"HDFS","description":"\x3c!--","source":"@site/docs/32.0.0/development/extensions-core/hdfs.md","sourceDirName":"development/extensions-core","slug":"/development/extensions-core/hdfs","permalink":"/docs/32.0.0/development/extensions-core/hdfs","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"hdfs","title":"HDFS"}}');var n=t(74848),r=t(28453);const i={id:"hdfs",title:"HDFS"},d=void 0,a={},c=[{value:"Deep Storage",id:"deep-storage",level:2},{value:"Configuration for HDFS",id:"configuration-for-hdfs",level:3},{value:"Configuration for Cloud Storage",id:"configuration-for-cloud-storage",level:3},{value:"Configuration for Amazon S3",id:"configuration-for-amazon-s3",level:4},{value:"Configuration for Google Cloud Storage",id:"configuration-for-google-cloud-storage",level:4},{value:"Reading data from HDFS or Cloud Storage",id:"reading-data-from-hdfs-or-cloud-storage",level:2},{value:"Native batch ingestion",id:"native-batch-ingestion",level:3},{value:"Hadoop-based ingestion",id:"hadoop-based-ingestion",level:3}];function l(e){const o={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",p:"p",pre:"pre",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsxs)(o.p,{children:["To use this Apache Druid extension, ",(0,n.jsx)(o.a,{href:"/docs/32.0.0/configuration/extensions#loading-extensions",children:"include"})," ",(0,n.jsx)(o.code,{children:"druid-hdfs-storage"})," in the extensions load list and run druid processes with ",(0,n.jsx)(o.code,{children:"GOOGLE_APPLICATION_CREDENTIALS=/path/to/service_account_keyfile"})," in the environment."]}),"\n",(0,n.jsx)(o.h2,{id:"deep-storage",children:"Deep Storage"}),"\n",(0,n.jsx)(o.h3,{id:"configuration-for-hdfs",children:"Configuration for HDFS"}),"\n",(0,n.jsxs)(o.table,{children:[(0,n.jsx)(o.thead,{children:(0,n.jsxs)(o.tr,{children:[(0,n.jsx)(o.th,{children:"Property"}),(0,n.jsx)(o.th,{children:"Possible Values"}),(0,n.jsx)(o.th,{children:"Description"}),(0,n.jsx)(o.th,{children:"Default"})]})}),(0,n.jsxs)(o.tbody,{children:[(0,n.jsxs)(o.tr,{children:[(0,n.jsx)(o.td,{children:(0,n.jsx)(o.code,{children:"druid.storage.type"})}),(0,n.jsx)(o.td,{children:"hdfs"}),(0,n.jsx)(o.td,{}),(0,n.jsx)(o.td,{children:"Must be set."})]}),(0,n.jsxs)(o.tr,{children:[(0,n.jsx)(o.td,{children:(0,n.jsx)(o.code,{children:"druid.storage.storageDirectory"})}),(0,n.jsx)(o.td,{}),(0,n.jsx)(o.td,{children:"Directory for storing segments."}),(0,n.jsx)(o.td,{children:"Must be set."})]}),(0,n.jsxs)(o.tr,{children:[(0,n.jsx)(o.td,{children:(0,n.jsx)(o.code,{children:"druid.hadoop.security.kerberos.principal"})}),(0,n.jsx)(o.td,{children:(0,n.jsx)(o.code,{children:"druid@EXAMPLE.COM"})}),(0,n.jsx)(o.td,{children:"Principal user name"}),(0,n.jsx)(o.td,{children:"empty"})]}),(0,n.jsxs)(o.tr,{children:[(0,n.jsx)(o.td,{children:(0,n.jsx)(o.code,{children:"druid.hadoop.security.kerberos.keytab"})}),(0,n.jsx)(o.td,{children:(0,n.jsx)(o.code,{children:"/etc/security/keytabs/druid.headlessUser.keytab"})}),(0,n.jsx)(o.td,{children:"Path to keytab file"}),(0,n.jsx)(o.td,{children:"empty"})]})]})]}),"\n",(0,n.jsxs)(o.p,{children:["Besides the above settings, you also need to include all Hadoop configuration files (such as ",(0,n.jsx)(o.code,{children:"core-site.xml"}),", ",(0,n.jsx)(o.code,{children:"hdfs-site.xml"}),")\nin the Druid classpath. One way to do this is copying all those files under ",(0,n.jsx)(o.code,{children:"${DRUID_HOME}/conf/_common"}),"."]}),"\n",(0,n.jsxs)(o.p,{children:["If you are using the Hadoop ingestion, set your output directory to be a location on Hadoop and it will work.\nIf you want to eagerly authenticate against a secured hadoop/hdfs cluster you must set ",(0,n.jsx)(o.code,{children:"druid.hadoop.security.kerberos.principal"})," and ",(0,n.jsx)(o.code,{children:"druid.hadoop.security.kerberos.keytab"}),", this is an alternative to the cron job method that runs ",(0,n.jsx)(o.code,{children:"kinit"})," command periodically."]}),"\n",(0,n.jsx)(o.h3,{id:"configuration-for-cloud-storage",children:"Configuration for Cloud Storage"}),"\n",(0,n.jsx)(o.p,{children:"You can also use the Amazon S3 or the Google Cloud Storage as the deep storage via HDFS."}),"\n",(0,n.jsx)(o.h4,{id:"configuration-for-amazon-s3",children:"Configuration for Amazon S3"}),"\n",(0,n.jsxs)(o.p,{children:["To use the Amazon S3 as the deep storage, you need to configure ",(0,n.jsx)(o.code,{children:"druid.storage.storageDirectory"})," properly."]}),"\n",(0,n.jsxs)(o.table,{children:[(0,n.jsx)(o.thead,{children:(0,n.jsxs)(o.tr,{children:[(0,n.jsx)(o.th,{children:"Property"}),(0,n.jsx)(o.th,{children:"Possible Values"}),(0,n.jsx)(o.th,{children:"Description"}),(0,n.jsx)(o.th,{children:"Default"})]})}),(0,n.jsxs)(o.tbody,{children:[(0,n.jsxs)(o.tr,{children:[(0,n.jsx)(o.td,{children:(0,n.jsx)(o.code,{children:"druid.storage.type"})}),(0,n.jsx)(o.td,{children:"hdfs"}),(0,n.jsx)(o.td,{}),(0,n.jsx)(o.td,{children:"Must be set."})]}),(0,n.jsxs)(o.tr,{children:[(0,n.jsx)(o.td,{children:(0,n.jsx)(o.code,{children:"druid.storage.storageDirectory"})}),(0,n.jsx)(o.td,{children:"s3a://bucket/example/directory or s3n://bucket/example/directory"}),(0,n.jsx)(o.td,{children:"Path to the deep storage"}),(0,n.jsx)(o.td,{children:"Must be set."})]})]})]}),"\n",(0,n.jsxs)(o.p,{children:["You also need to include the ",(0,n.jsx)(o.a,{href:"https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/",children:"Hadoop AWS module"}),", especially the ",(0,n.jsx)(o.code,{children:"hadoop-aws.jar"})," in the Druid classpath.\nRun the below command to install the ",(0,n.jsx)(o.code,{children:"hadoop-aws.jar"})," file under ",(0,n.jsx)(o.code,{children:"${DRUID_HOME}/extensions/druid-hdfs-storage"})," in all nodes."]}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{className:"language-bash",children:'${DRUID_HOME}/bin/run-java -classpath "${DRUID_HOME}/lib/*" org.apache.druid.cli.Main tools pull-deps -h "org.apache.hadoop:hadoop-aws:${HADOOP_VERSION}";\ncp ${DRUID_HOME}/hadoop-dependencies/hadoop-aws/${HADOOP_VERSION}/hadoop-aws-${HADOOP_VERSION}.jar ${DRUID_HOME}/extensions/druid-hdfs-storage/\n'})}),"\n",(0,n.jsxs)(o.p,{children:["Finally, you need to add the below properties in the ",(0,n.jsx)(o.code,{children:"core-site.xml"}),".\nFor more configurations, see the ",(0,n.jsx)(o.a,{href:"https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/",children:"Hadoop AWS module"}),"."]}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{className:"language-xml",children:"<property>\n  <name>fs.s3a.impl</name>\n  <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n  <description>The implementation class of the S3A Filesystem</description>\n</property>\n\n<property>\n  <name>fs.AbstractFileSystem.s3a.impl</name>\n  <value>org.apache.hadoop.fs.s3a.S3A</value>\n  <description>The implementation class of the S3A AbstractFileSystem.</description>\n</property>\n\n<property>\n  <name>fs.s3a.access.key</name>\n  <description>AWS access key ID. Omit for IAM role-based or provider-based authentication.</description>\n  <value>your access key</value>\n</property>\n\n<property>\n  <name>fs.s3a.secret.key</name>\n  <description>AWS secret key. Omit for IAM role-based or provider-based authentication.</description>\n  <value>your secret key</value>\n</property>\n"})}),"\n",(0,n.jsx)(o.h4,{id:"configuration-for-google-cloud-storage",children:"Configuration for Google Cloud Storage"}),"\n",(0,n.jsxs)(o.p,{children:["To use the Google Cloud Storage as the deep storage, you need to configure ",(0,n.jsx)(o.code,{children:"druid.storage.storageDirectory"})," properly."]}),"\n",(0,n.jsxs)(o.table,{children:[(0,n.jsx)(o.thead,{children:(0,n.jsxs)(o.tr,{children:[(0,n.jsx)(o.th,{children:"Property"}),(0,n.jsx)(o.th,{children:"Possible Values"}),(0,n.jsx)(o.th,{children:"Description"}),(0,n.jsx)(o.th,{children:"Default"})]})}),(0,n.jsxs)(o.tbody,{children:[(0,n.jsxs)(o.tr,{children:[(0,n.jsx)(o.td,{children:(0,n.jsx)(o.code,{children:"druid.storage.type"})}),(0,n.jsx)(o.td,{children:"hdfs"}),(0,n.jsx)(o.td,{}),(0,n.jsx)(o.td,{children:"Must be set."})]}),(0,n.jsxs)(o.tr,{children:[(0,n.jsx)(o.td,{children:(0,n.jsx)(o.code,{children:"druid.storage.storageDirectory"})}),(0,n.jsx)(o.td,{children:"gs://bucket/example/directory"}),(0,n.jsx)(o.td,{children:"Path to the deep storage"}),(0,n.jsx)(o.td,{children:"Must be set."})]})]})]}),"\n",(0,n.jsxs)(o.p,{children:["All services that need to access GCS need to have the ",(0,n.jsx)(o.a,{href:"https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#other_sparkhadoop_clusters",children:"GCS connector jar"})," in their class path.\nPlease read the ",(0,n.jsx)(o.a,{href:"https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md",children:"install instructions"}),"\nto properly set up the necessary libraries and configurations.\nOne option is to place this jar in ",(0,n.jsx)(o.code,{children:"${DRUID_HOME}/lib/"})," and ",(0,n.jsx)(o.code,{children:"${DRUID_HOME}/extensions/druid-hdfs-storage/"}),"."]}),"\n",(0,n.jsxs)(o.p,{children:["Finally, you need to configure the ",(0,n.jsx)(o.code,{children:"core-site.xml"})," file with the filesystem\nand authentication properties needed for GCS. You may want to copy the below\nexample properties. Please follow the instructions at\n",(0,n.jsx)(o.a,{href:"https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md",children:"https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md"}),"\nfor more details.\nFor more configurations, ",(0,n.jsx)(o.a,{href:"https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v2.0.0/gcs/conf/gcs-core-default.xml",children:"GCS core default"}),"\nand ",(0,n.jsx)(o.a,{href:"https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/master/gcs/src/test/resources/core-site.xml",children:"GCS core template"}),"."]}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{className:"language-xml",children:"<property>\n  <name>fs.gs.impl</name>\n  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem</value>\n  <description>The FileSystem for gs: (GCS) uris.</description>\n</property>\n\n<property>\n  <name>fs.AbstractFileSystem.gs.impl</name>\n  <value>com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS</value>\n  <description>The AbstractFileSystem for gs: uris.</description>\n</property>\n\n<property>\n  <name>google.cloud.auth.service.account.enable</name>\n  <value>true</value>\n  <description>\n    Whether to use a service account for GCS authorization.\n    Setting this property to `false` will disable use of service accounts for\n    authentication.\n  </description>\n</property>\n\n<property>\n  <name>google.cloud.auth.service.account.json.keyfile</name>\n  <value>/path/to/keyfile</value>\n  <description>\n    The JSON key file of the service account used for GCS\n    access when google.cloud.auth.service.account.enable is true.\n  </description>\n</property>\n"})}),"\n",(0,n.jsx)(o.h2,{id:"reading-data-from-hdfs-or-cloud-storage",children:"Reading data from HDFS or Cloud Storage"}),"\n",(0,n.jsx)(o.h3,{id:"native-batch-ingestion",children:"Native batch ingestion"}),"\n",(0,n.jsxs)(o.p,{children:["The ",(0,n.jsx)(o.a,{href:"/docs/32.0.0/ingestion/input-sources#hdfs-input-source",children:"HDFS input source"})," is supported by the ",(0,n.jsx)(o.a,{href:"/docs/32.0.0/ingestion/native-batch",children:"Parallel task"}),"\nto read files directly from the HDFS Storage. You may be able to read objects from cloud storage\nwith the HDFS input source, but we highly recommend to use a proper\n",(0,n.jsx)(o.a,{href:"/docs/32.0.0/ingestion/input-sources",children:"Input Source"})," instead if possible because\nit is simple to set up. For now, only the ",(0,n.jsx)(o.a,{href:"/docs/32.0.0/ingestion/input-sources#s3-input-source",children:"S3 input source"}),"\nand the ",(0,n.jsx)(o.a,{href:"/docs/32.0.0/ingestion/input-sources#google-cloud-storage-input-source",children:"Google Cloud Storage input source"}),"\nare supported for cloud storage types, and so you may still want to use the HDFS input source\nto read from cloud storage other than those two."]}),"\n",(0,n.jsx)(o.h3,{id:"hadoop-based-ingestion",children:"Hadoop-based ingestion"}),"\n",(0,n.jsxs)(o.p,{children:["If you use the ",(0,n.jsx)(o.a,{href:"/docs/32.0.0/ingestion/hadoop",children:"Hadoop ingestion"}),", you can read data from HDFS\nby specifying the paths in your ",(0,n.jsx)(o.a,{href:"/docs/32.0.0/ingestion/hadoop#inputspec",children:(0,n.jsx)(o.code,{children:"inputSpec"})}),".\nSee the ",(0,n.jsx)(o.a,{href:"/docs/32.0.0/ingestion/hadoop#static",children:"Static"})," inputSpec for details."]})]})}function h(e={}){const{wrapper:o}={...(0,r.R)(),...e.components};return o?(0,n.jsx)(o,{...e,children:(0,n.jsx)(l,{...e})}):l(e)}}}]);