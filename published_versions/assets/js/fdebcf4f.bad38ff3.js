"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[945],{28453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var a=t(96540);const i={},s=a.createContext(i);function r(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),a.createElement(s.Provider,{value:n},e.children)}},96123:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>o,default:()=>u,frontMatter:()=>r,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"data-management/update","title":"Data updates","description":"\x3c!--","source":"@site/docs/33.0.0/data-management/update.md","sourceDirName":"data-management","slug":"/data-management/update","permalink":"/docs/33.0.0/data-management/update","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"update","title":"Data updates"},"sidebar":"docs","previous":{"title":"Overview","permalink":"/docs/33.0.0/data-management/"},"next":{"title":"Data deletion","permalink":"/docs/33.0.0/data-management/delete"}}');var i=t(74848),s=t(28453);const r={id:"update",title:"Data updates"},o=void 0,d={},c=[{value:"Overwrite",id:"overwrite",level:2},{value:"Reindex",id:"reindex",level:2},{value:"Rolled-up datasources",id:"rolled-up-datasources",level:2},{value:"Lookups",id:"lookups",level:2}];function l(e){const n={a:"a",admonition:"admonition",code:"code",h2:"h2",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"overwrite",children:"Overwrite"}),"\n",(0,i.jsxs)(n.p,{children:["Apache Druid stores data ",(0,i.jsx)(n.a,{href:"/docs/33.0.0/design/storage",children:"partitioned by time chunk"})," and supports\noverwriting existing data using time ranges. Data outside the replacement time range is not touched. Overwriting of\nexisting data is done using the same mechanisms as ",(0,i.jsx)(n.a,{href:"/docs/33.0.0/ingestion/#batch",children:"batch ingestion"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"For example:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"/docs/33.0.0/ingestion/native-batch",children:"Native batch"})," with ",(0,i.jsx)(n.code,{children:"appendToExisting: false"}),", and ",(0,i.jsx)(n.code,{children:"intervals"})," set to a specific\ntime range, overwrites data for that time range."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.a,{href:"/docs/33.0.0/multi-stage-query/reference#replace",children:["SQL ",(0,i.jsx)(n.code,{children:"REPLACE <table> OVERWRITE [ALL | WHERE ...]"})]})," overwrites data for\nthe entire table or for a specified time range."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"In both cases, Druid's atomic update mechanism ensures that queries will flip seamlessly from the old data to the new\ndata on a time-chunk-by-time-chunk basis."}),"\n",(0,i.jsx)(n.p,{children:"Ingestion and overwriting cannot run concurrently for the same time range of the same datasource. While an overwrite job\nis ongoing for a particular time range of a datasource, new ingestions for that time range are queued up. Ingestions for\nother time ranges proceed as normal. Read-only queries also proceed as normal, using the pre-existing version of the\ndata."}),"\n",(0,i.jsx)(n.admonition,{type:"info",children:(0,i.jsx)(n.p,{children:"Druid does not support single-record updates by primary key."})}),"\n",(0,i.jsx)(n.h2,{id:"reindex",children:"Reindex"}),"\n",(0,i.jsxs)(n.p,{children:["Reindexing is an ",(0,i.jsx)(n.a,{href:"#overwrite",children:"overwrite of existing data"})," where the source of new data is the existing data itself. It\nis used to perform schema changes, repartition data, filter out unwanted data, enrich existing data, and so on. This\nbehaves just like any other ",(0,i.jsx)(n.a,{href:"#overwrite",children:"overwrite"})," with regard to atomic updates and locking."]}),"\n",(0,i.jsxs)(n.p,{children:["With ",(0,i.jsx)(n.a,{href:"/docs/33.0.0/ingestion/native-batch",children:"native batch"}),", use the ",(0,i.jsxs)(n.a,{href:"/docs/33.0.0/ingestion/input-sources#druid-input-source",children:[(0,i.jsx)(n.code,{children:"druid"})," input\nsource"]}),". If needed,\n",(0,i.jsx)(n.a,{href:"/docs/33.0.0/ingestion/ingestion-spec#transformspec",children:(0,i.jsx)(n.code,{children:"transformSpec"})})," can be used to filter or modify data during the\nreindexing job."]}),"\n",(0,i.jsxs)(n.p,{children:["With SQL, use ",(0,i.jsx)(n.a,{href:"/docs/33.0.0/multi-stage-query/reference#replace",children:(0,i.jsx)(n.code,{children:"REPLACE <table> OVERWRITE"})})," with ",(0,i.jsx)(n.code,{children:"SELECT ... FROM <table>"}),".\n(Druid does not have ",(0,i.jsx)(n.code,{children:"UPDATE"})," or ",(0,i.jsx)(n.code,{children:"ALTER TABLE"})," statements.) Any SQL SELECT query can be used to filter,\nmodify, or enrich the data during the reindexing job."]}),"\n",(0,i.jsx)(n.h2,{id:"rolled-up-datasources",children:"Rolled-up datasources"}),"\n",(0,i.jsx)(n.p,{children:"Rolled-up datasources can be effectively updated using appends, without rewrites. When you append a row that has an\nidentical set of dimensions to an existing row, queries that use aggregation operators automatically combine those two\nrows together at query time."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"/docs/33.0.0/data-management/compaction",children:"Compaction"})," or ",(0,i.jsx)(n.a,{href:"/docs/33.0.0/data-management/automatic-compaction",children:"automatic compaction"})," can be used to physically combine these\nmatching rows together later on, by rewriting segments in the background."]}),"\n",(0,i.jsx)(n.h2,{id:"lookups",children:"Lookups"}),"\n",(0,i.jsxs)(n.p,{children:["If you have a dimension where values need to be updated frequently, try first using ",(0,i.jsx)(n.a,{href:"/docs/33.0.0/querying/lookups",children:"lookups"}),". A\nclassic use case of lookups is when you have an ID dimension stored in a Druid segment, and want to map the ID dimension to a\nhuman-readable string that may need to be updated periodically."]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);