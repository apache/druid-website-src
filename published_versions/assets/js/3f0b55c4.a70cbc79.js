"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[8821],{5180:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>d,contentTitle:()=>i,default:()=>c,frontMatter:()=>a,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"operations/deep-storage-migration","title":"Deep storage migration","description":"\x3c!--","source":"@site/docs/latest/operations/deep-storage-migration.md","sourceDirName":"operations","slug":"/operations/deep-storage-migration","permalink":"/docs/latest/operations/deep-storage-migration","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"deep-storage-migration","title":"Deep storage migration"},"sidebar":"docs","previous":{"title":"pull-deps tool","permalink":"/docs/latest/operations/pull-deps"},"next":{"title":"Export Metadata Tool","permalink":"/docs/latest/operations/export-metadata"}}');var r=o(74848),n=o(28453);const a={id:"deep-storage-migration",title:"Deep storage migration"},i=void 0,d={},l=[{value:"Shut down cluster services",id:"shut-down-cluster-services",level:2},{value:"Copy segments from old deep storage to new deep storage.",id:"copy-segments-from-old-deep-storage-to-new-deep-storage",level:2},{value:"Export segments with rewritten load specs",id:"export-segments-with-rewritten-load-specs",level:2},{value:"Import metadata",id:"import-metadata",level:3},{value:"Restart cluster",id:"restart-cluster",level:3}];function p(e){const t={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",p:"p",ul:"ul",...(0,n.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.p,{children:"If you have been running an evaluation Druid cluster using local deep storage and wish to migrate to a\nmore production-capable deep storage system such as S3 or HDFS, this document describes the necessary steps."}),"\n",(0,r.jsx)(t.p,{children:"Migration of deep storage involves the following steps at a high level:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Copying segments from local deep storage to the new deep storage"}),"\n",(0,r.jsx)(t.li,{children:"Exporting Druid's segments table from metadata"}),"\n",(0,r.jsx)(t.li,{children:"Rewriting the load specs in the exported segment data to reflect the new deep storage location"}),"\n",(0,r.jsx)(t.li,{children:"Reimporting the edited segments into metadata"}),"\n"]}),"\n",(0,r.jsx)(t.h2,{id:"shut-down-cluster-services",children:"Shut down cluster services"}),"\n",(0,r.jsx)(t.p,{children:"To ensure a clean migration, shut down the non-coordinator services to ensure that metadata state will not\nchange as you do the migration."}),"\n",(0,r.jsx)(t.p,{children:"When migrating from Derby, the coordinator processes will still need to be up initially, as they host the Derby database."}),"\n",(0,r.jsx)(t.h2,{id:"copy-segments-from-old-deep-storage-to-new-deep-storage",children:"Copy segments from old deep storage to new deep storage."}),"\n",(0,r.jsx)(t.p,{children:"Before migrating, you will need to copy your old segments to the new deep storage."}),"\n",(0,r.jsxs)(t.p,{children:["For information on what path structure to use in the new deep storage, please see ",(0,r.jsx)(t.a,{href:"/docs/latest/operations/export-metadata#deep-storage-migration",children:"deep storage migration options"}),"."]}),"\n",(0,r.jsx)(t.h2,{id:"export-segments-with-rewritten-load-specs",children:"Export segments with rewritten load specs"}),"\n",(0,r.jsxs)(t.p,{children:["Druid provides an ",(0,r.jsx)(t.a,{href:"/docs/latest/operations/export-metadata",children:"Export Metadata Tool"})," for exporting metadata from Derby into CSV files\nwhich can then be reimported."]}),"\n",(0,r.jsxs)(t.p,{children:["By setting ",(0,r.jsx)(t.a,{href:"/docs/latest/operations/export-metadata#deep-storage-migration",children:"deep storage migration options"}),", the ",(0,r.jsx)(t.code,{children:"export-metadata"})," tool will export CSV files where the segment load specs have been rewritten to load from your new deep storage location."]}),"\n",(0,r.jsxs)(t.p,{children:["Run the ",(0,r.jsx)(t.code,{children:"export-metadata"})," tool on your existing cluster, using the migration options appropriate for your new deep storage location, and save the CSV files it generates. After a successful export, you can shut down the coordinator."]}),"\n",(0,r.jsx)(t.h3,{id:"import-metadata",children:"Import metadata"}),"\n",(0,r.jsx)(t.p,{children:"After generating the CSV exports with the modified segment data, you can reimport the contents of the Druid segments table from the generated CSVs."}),"\n",(0,r.jsxs)(t.p,{children:["Please refer to ",(0,r.jsx)(t.a,{href:"/docs/latest/operations/export-metadata#importing-metadata",children:"import commands"})," for examples. Only the ",(0,r.jsx)(t.code,{children:"druid_segments"})," table needs to be imported."]}),"\n",(0,r.jsx)(t.h3,{id:"restart-cluster",children:"Restart cluster"}),"\n",(0,r.jsx)(t.p,{children:"After importing the segment table successfully, you can now restart your cluster."})]})}function c(e={}){const{wrapper:t}={...(0,n.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(p,{...e})}):p(e)}},28453:(e,t,o)=>{o.d(t,{R:()=>a,x:()=>i});var s=o(96540);const r={},n=s.createContext(r);function a(e){const t=s.useContext(n);return s.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function i(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:a(e.components),s.createElement(n.Provider,{value:t},e.children)}}}]);