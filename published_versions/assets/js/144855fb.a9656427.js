"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[2062],{28453:(e,o,n)=>{n.d(o,{R:()=>r,x:()=>d});var s=n(96540);const i={},a=s.createContext(i);function r(e){const o=s.useContext(a);return s.useMemo((function(){return"function"==typeof e?e(o):{...o,...e}}),[o,e])}function d(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(a.Provider,{value:o},e.children)}},28658:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>t,contentTitle:()=>d,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"operations/other-hadoop","title":"Working with different versions of Apache Hadoop","description":"\x3c!--","source":"@site/docs/latest/operations/other-hadoop.md","sourceDirName":"operations","slug":"/operations/other-hadoop","permalink":"/docs/latest/operations/other-hadoop","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"other-hadoop","title":"Working with different versions of Apache Hadoop"},"sidebar":"docs","previous":{"title":"Migrate from firehose","permalink":"/docs/latest/operations/migrate-from-firehose"},"next":{"title":"dump-segment tool","permalink":"/docs/latest/operations/dump-segment"}}');var i=n(74848),a=n(28453);const r={id:"other-hadoop",title:"Working with different versions of Apache Hadoop"},d=void 0,t={},c=[{value:"Tip #1: Place Hadoop XMLs on Druid classpath",id:"tip-1-place-hadoop-xmls-on-druid-classpath",level:2},{value:"Tip #2: Classloader modification on Hadoop (Map/Reduce jobs only)",id:"tip-2-classloader-modification-on-hadoop-mapreduce-jobs-only",level:2},{value:"Overriding specific classes",id:"overriding-specific-classes",level:3},{value:"Tip #3: Use specific versions of Hadoop libraries",id:"tip-3-use-specific-versions-of-hadoop-libraries",level:2},{value:"Preferred: Load using Druid&#39;s standard mechanism",id:"preferred-load-using-druids-standard-mechanism",level:3},{value:"Alternative: Append your Hadoop jars to the Druid classpath",id:"alternative-append-your-hadoop-jars-to-the-druid-classpath",level:3},{value:"Notes on specific Hadoop distributions",id:"notes-on-specific-hadoop-distributions",level:2},{value:"CDH",id:"cdh",level:3}];function l(e){const o={a:"a",code:"code",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(o.p,{children:"Apache Druid can interact with Hadoop in two ways:"}),"\n",(0,i.jsxs)(o.ol,{children:["\n",(0,i.jsxs)(o.li,{children:[(0,i.jsx)(o.a,{href:"/docs/latest/development/extensions-core/hdfs",children:"Use HDFS for deep storage"})," using the druid-hdfs-storage extension."]}),"\n",(0,i.jsxs)(o.li,{children:[(0,i.jsx)(o.a,{href:"/docs/latest/ingestion/hadoop",children:"Batch-load data from Hadoop"})," using Map/Reduce jobs."]}),"\n"]}),"\n",(0,i.jsx)(o.p,{children:"These are not necessarily linked together; you can load data with Hadoop jobs into a non-HDFS deep storage (like S3),\nand you can use HDFS for deep storage even if you're loading data from streams rather than using Hadoop jobs."}),"\n",(0,i.jsx)(o.p,{children:"For best results, use these tips when configuring Druid to interact with your favorite Hadoop distribution."}),"\n",(0,i.jsx)(o.h2,{id:"tip-1-place-hadoop-xmls-on-druid-classpath",children:"Tip #1: Place Hadoop XMLs on Druid classpath"}),"\n",(0,i.jsxs)(o.p,{children:["Place your Hadoop configuration XMLs (core-site.xml, hdfs-site.xml, yarn-site.xml, mapred-site.xml) on the classpath\nof your Druid processes. You can do this by copying them into ",(0,i.jsx)(o.code,{children:"conf/druid/_common/core-site.xml"}),",\n",(0,i.jsx)(o.code,{children:"conf/druid/_common/hdfs-site.xml"}),", and so on. This allows Druid to find your Hadoop cluster and properly submit jobs."]}),"\n",(0,i.jsx)(o.h2,{id:"tip-2-classloader-modification-on-hadoop-mapreduce-jobs-only",children:"Tip #2: Classloader modification on Hadoop (Map/Reduce jobs only)"}),"\n",(0,i.jsxs)(o.p,{children:["Druid uses a number of libraries that are also likely present on your Hadoop cluster, and if these libraries conflict,\nyour Map/Reduce jobs can fail. This problem can be avoided by enabling classloader isolation using the Hadoop job\nproperty ",(0,i.jsx)(o.code,{children:"mapreduce.job.classloader = true"}),". This instructs Hadoop to use a separate classloader for Druid dependencies\nand for Hadoop's own dependencies."]}),"\n",(0,i.jsxs)(o.p,{children:["If your version of Hadoop does not support this functionality, you can also try setting the property\n",(0,i.jsx)(o.code,{children:"mapreduce.job.user.classpath.first = true"}),". This instructs Hadoop to prefer loading Druid's version of a library when\nthere is a conflict."]}),"\n",(0,i.jsx)(o.p,{children:"Generally, you should only set one of these parameters, not both."}),"\n",(0,i.jsx)(o.p,{children:"These properties can be set in either one of the following ways:"}),"\n",(0,i.jsxs)(o.ul,{children:["\n",(0,i.jsxs)(o.li,{children:["Using the task definition, e.g. add ",(0,i.jsx)(o.code,{children:'"mapreduce.job.classloader": "true"'})," to the ",(0,i.jsx)(o.code,{children:"jobProperties"})," of the ",(0,i.jsx)(o.code,{children:"tuningConfig"})," of your indexing task (see the ",(0,i.jsx)(o.a,{href:"/docs/latest/ingestion/hadoop",children:"Hadoop batch ingestion documentation"}),")."]}),"\n",(0,i.jsxs)(o.li,{children:["Using system properties, e.g. on the Middle Manager set ",(0,i.jsx)(o.code,{children:"druid.indexer.runner.javaOpts=... -Dhadoop.mapreduce.job.classloader=true"})," in ",(0,i.jsx)(o.a,{href:"/docs/latest/configuration/#middle-manager-configuration",children:"Middle Manager configuration"}),"."]}),"\n"]}),"\n",(0,i.jsx)(o.h3,{id:"overriding-specific-classes",children:"Overriding specific classes"}),"\n",(0,i.jsxs)(o.p,{children:["When ",(0,i.jsx)(o.code,{children:"mapreduce.job.classloader = true"}),", it is also possible to specifically define which classes should be loaded from the hadoop system classpath and which should be loaded from job-supplied JARs."]}),"\n",(0,i.jsxs)(o.p,{children:["This is controlled by defining class inclusion/exclusion patterns in the ",(0,i.jsx)(o.code,{children:"mapreduce.job.classloader.system.classes"})," property in the ",(0,i.jsx)(o.code,{children:"jobProperties"})," of ",(0,i.jsx)(o.code,{children:"tuningConfig"}),"."]}),"\n",(0,i.jsx)(o.p,{children:"For example, some community members have reported version incompatibility errors with the Validator class:"}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{children:"Error: java.lang.ClassNotFoundException: javax.validation.Validator\n"})}),"\n",(0,i.jsxs)(o.p,{children:["The following ",(0,i.jsx)(o.code,{children:"jobProperties"})," excludes ",(0,i.jsx)(o.code,{children:"javax.validation."})," classes from being loaded from the system classpath, while including those from ",(0,i.jsx)(o.code,{children:"java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop."}),"."]}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{children:'"jobProperties": {\n  "mapreduce.job.classloader": "true",\n  "mapreduce.job.classloader.system.classes": "-javax.validation.,java.,javax.,org.apache.commons.logging.,org.apache.log4j.,org.apache.hadoop."\n}\n'})}),"\n",(0,i.jsxs)(o.p,{children:[(0,i.jsx)(o.a,{href:"https://hadoop.apache.org/docs/stable/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml",children:"mapred-default.xml"})," documentation contains more information about this property."]}),"\n",(0,i.jsx)(o.h2,{id:"tip-3-use-specific-versions-of-hadoop-libraries",children:"Tip #3: Use specific versions of Hadoop libraries"}),"\n",(0,i.jsx)(o.p,{children:"Druid loads Hadoop client libraries from two different locations. Each set of libraries is loaded in an isolated\nclassloader."}),"\n",(0,i.jsxs)(o.ol,{children:["\n",(0,i.jsxs)(o.li,{children:["HDFS deep storage uses jars from ",(0,i.jsx)(o.code,{children:"extensions/druid-hdfs-storage/"})," to read and write Druid data on HDFS."]}),"\n",(0,i.jsxs)(o.li,{children:["Batch ingestion uses jars from ",(0,i.jsx)(o.code,{children:"hadoop-dependencies/"})," to submit Map/Reduce jobs (location customizable via the\n",(0,i.jsx)(o.code,{children:"druid.extensions.hadoopDependenciesDir"})," runtime property; see ",(0,i.jsx)(o.a,{href:"/docs/latest/configuration/#extensions",children:"Configuration"}),")."]}),"\n"]}),"\n",(0,i.jsxs)(o.p,{children:["The default version of the Hadoop client bundled with Druid is ",(0,i.jsx)(o.code,{children:"3.3.6"}),". This works with\nmany Hadoop distributions (the version does not necessarily need to match), but if you run into issues, you can instead\nhave Druid load libraries that exactly match your distribution. To do this, either copy the jars from your Hadoop\ncluster, or use the ",(0,i.jsx)(o.code,{children:"pull-deps"})," tool to download the jars from a Maven repository."]}),"\n",(0,i.jsx)(o.h3,{id:"preferred-load-using-druids-standard-mechanism",children:"Preferred: Load using Druid's standard mechanism"}),"\n",(0,i.jsxs)(o.p,{children:["If you have issues with HDFS deep storage, you can switch your Hadoop client libraries by recompiling the\ndruid-hdfs-storage extension using an alternate version of the Hadoop client libraries. You can do this by editing\nthe main Druid pom.xml and rebuilding the distribution by running ",(0,i.jsx)(o.code,{children:"mvn package"}),"."]}),"\n",(0,i.jsxs)(o.p,{children:["If you have issues with Map/Reduce jobs, you can switch your Hadoop client libraries without rebuilding Druid. You can\ndo this by adding a new set of libraries to the ",(0,i.jsx)(o.code,{children:"hadoop-dependencies/"})," directory (or another directory specified by\ndruid.extensions.hadoopDependenciesDir) and then using ",(0,i.jsx)(o.code,{children:"hadoopDependencyCoordinates"})," in the\n",(0,i.jsx)(o.a,{href:"/docs/latest/ingestion/hadoop",children:"Hadoop Index Task"})," to specify the Hadoop dependencies you want Druid to load."]}),"\n",(0,i.jsx)(o.p,{children:"Example:"}),"\n",(0,i.jsxs)(o.p,{children:["Suppose you specify ",(0,i.jsx)(o.code,{children:"druid.extensions.hadoopDependenciesDir=/usr/local/druid_tarball/hadoop-dependencies"}),", and you have downloaded\n",(0,i.jsx)(o.code,{children:"hadoop-client"})," 2.3.0 and 2.4.0, either by copying them from your Hadoop cluster or by using ",(0,i.jsx)(o.code,{children:"pull-deps"})," to download\nthe jars from a Maven repository. Then underneath ",(0,i.jsx)(o.code,{children:"hadoop-dependencies"}),", your jars should look like this:"]}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{children:"hadoop-dependencies/\n\u2514\u2500\u2500 hadoop-client\n    \u251c\u2500\u2500 2.3.0\n    \u2502\xa0\xa0 \u251c\u2500\u2500 activation-1.1.jar\n    \u2502\xa0\xa0 \u251c\u2500\u2500 avro-1.7.4.jar\n    \u2502\xa0\xa0 \u251c\u2500\u2500 commons-beanutils-1.7.0.jar\n    \u2502\xa0\xa0 \u251c\u2500\u2500 commons-beanutils-core-1.8.0.jar\n    \u2502\xa0\xa0 \u251c\u2500\u2500 commons-cli-1.2.jar\n    \u2502\xa0\xa0 \u251c\u2500\u2500 commons-codec-1.4.jar\n    ..... lots of jars\n    \u2514\u2500\u2500 2.4.0\n        \u251c\u2500\u2500 activation-1.1.jar\n        \u251c\u2500\u2500 avro-1.7.4.jar\n        \u251c\u2500\u2500 commons-beanutils-1.7.0.jar\n        \u251c\u2500\u2500 commons-beanutils-core-1.8.0.jar\n        \u251c\u2500\u2500 commons-cli-1.2.jar\n        \u251c\u2500\u2500 commons-codec-1.4.jar\n    ..... lots of jars\n"})}),"\n",(0,i.jsxs)(o.p,{children:["As you can see, under ",(0,i.jsx)(o.code,{children:"hadoop-client"}),", there are two sub-directories, each denotes a version of ",(0,i.jsx)(o.code,{children:"hadoop-client"}),"."]}),"\n",(0,i.jsxs)(o.p,{children:["Next, use ",(0,i.jsx)(o.code,{children:"hadoopDependencyCoordinates"})," in ",(0,i.jsx)(o.a,{href:"/docs/latest/ingestion/hadoop",children:"Hadoop Index Task"})," to specify the Hadoop dependencies you want Druid to load."]}),"\n",(0,i.jsx)(o.p,{children:"For example, in your Hadoop Index Task spec file, you can write:"}),"\n",(0,i.jsx)(o.p,{children:(0,i.jsx)(o.code,{children:'"hadoopDependencyCoordinates": ["org.apache.hadoop:hadoop-client:2.4.0"]'})}),"\n",(0,i.jsxs)(o.p,{children:["This instructs Druid to load hadoop-client 2.4.0 when processing the task. What happens behind the scene is that Druid first looks for a folder\ncalled ",(0,i.jsx)(o.code,{children:"hadoop-client"})," underneath ",(0,i.jsx)(o.code,{children:"druid.extensions.hadoopDependenciesDir"}),", then looks for a folder called ",(0,i.jsx)(o.code,{children:"2.4.0"}),"\nunderneath ",(0,i.jsx)(o.code,{children:"hadoop-client"}),", and upon successfully locating these folders, hadoop-client 2.4.0 is loaded."]}),"\n",(0,i.jsx)(o.h3,{id:"alternative-append-your-hadoop-jars-to-the-druid-classpath",children:"Alternative: Append your Hadoop jars to the Druid classpath"}),"\n",(0,i.jsx)(o.p,{children:"You can also load Hadoop client libraries in Druid's main classloader, rather than an isolated classloader. This\nmechanism is relatively easy to reason about, but it also means that you have to ensure that all dependency jars on the\nclasspath are compatible. That is, Druid makes no provisions while using this method to maintain class loader isolation\nso you must make sure that the jars on your classpath are mutually compatible."}),"\n",(0,i.jsxs)(o.ol,{children:["\n",(0,i.jsxs)(o.li,{children:["Set ",(0,i.jsx)(o.code,{children:"druid.indexer.task.defaultHadoopCoordinates=[]"}),". By setting this to an empty list, Druid will not load any other Hadoop dependencies except the ones specified in the classpath."]}),"\n",(0,i.jsx)(o.li,{children:"Append your Hadoop jars to Druid's classpath. Druid will load them into the system."}),"\n"]}),"\n",(0,i.jsx)(o.h2,{id:"notes-on-specific-hadoop-distributions",children:"Notes on specific Hadoop distributions"}),"\n",(0,i.jsx)(o.p,{children:"If the tips above do not solve any issues you are having with HDFS deep storage or Hadoop batch indexing, you may\nhave luck with one of the following suggestions contributed by the Druid community."}),"\n",(0,i.jsx)(o.h3,{id:"cdh",children:"CDH"}),"\n",(0,i.jsx)(o.p,{children:"Members of the community have reported dependency conflicts between the version of Jackson used in CDH and Druid when running a Mapreduce job like:"}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{children:"java.lang.VerifyError: class com.fasterxml.jackson.datatype.guava.deser.HostAndPortDeserializer overrides final method deserialize.(Lcom/fasterxml/jackson/core/JsonParser;Lcom/fasterxml/jackson/databind/DeserializationContext;)Ljava/lang/Object;\n"})}),"\n",(0,i.jsx)(o.p,{children:(0,i.jsx)(o.strong,{children:"Preferred workaround"})}),"\n",(0,i.jsxs)(o.p,{children:['First, try the tip under "Classloader modification on Hadoop" above. More recent versions of CDH have been reported to\nwork with the classloader isolation option (',(0,i.jsx)(o.code,{children:"mapreduce.job.classloader = true"}),")."]}),"\n",(0,i.jsx)(o.p,{children:(0,i.jsx)(o.strong,{children:"Alternate workaround - 1"})}),"\n",(0,i.jsx)(o.p,{children:"You can try editing Druid's pom.xml dependencies to match the version of Jackson in your Hadoop version and recompile Druid."}),"\n",(0,i.jsxs)(o.p,{children:["For more about building Druid, please see ",(0,i.jsx)(o.a,{href:"/docs/latest/development/build",children:"Building Druid"}),"."]}),"\n",(0,i.jsx)(o.p,{children:(0,i.jsx)(o.strong,{children:"Alternate workaround - 2"})}),"\n",(0,i.jsxs)(o.p,{children:["Another workaround solution is to build a custom fat jar of Druid using ",(0,i.jsx)(o.a,{href:"http://www.scala-sbt.org/",children:"sbt"}),", which manually excludes all the conflicting Jackson dependencies, and then put this fat jar in the classpath of the command that starts Overlord indexing service. To do this, please follow the following steps."]}),"\n",(0,i.jsx)(o.p,{children:"(1) Download and install sbt."}),"\n",(0,i.jsx)(o.p,{children:"(2) Make a new directory named 'druid_build'."}),"\n",(0,i.jsxs)(o.p,{children:["(3) Cd to 'druid_build' and create the build.sbt file with the content ",(0,i.jsx)(o.a,{href:"/docs/latest/operations/use_sbt_to_build_fat_jar",children:"here"}),"."]}),"\n",(0,i.jsx)(o.p,{children:"You can always add more building targets or remove the ones you don't need."}),"\n",(0,i.jsx)(o.p,{children:"(4) In the same directory create a new directory named 'project'."}),"\n",(0,i.jsx)(o.p,{children:"(5) Put the druid source code into 'druid_build/project'."}),"\n",(0,i.jsx)(o.p,{children:"(6) Create a file 'druid_build/project/assembly.sbt' with content as follows."}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{children:'addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.13.0")\n'})}),"\n",(0,i.jsx)(o.p,{children:"(7) In the 'druid_build' directory, run 'sbt assembly'."}),"\n",(0,i.jsx)(o.p,{children:"(8) In the 'druid_build/target/scala-2.10' folder, you will find the fat jar you just build."}),"\n",(0,i.jsx)(o.p,{children:"(9) Make sure the jars you've uploaded has been completely removed. The HDFS directory is by default '/tmp/druid-indexing/classpath'."}),"\n",(0,i.jsx)(o.p,{children:"(10) Include the fat jar in the classpath when you start the indexing service. Make sure you've removed 'lib/*' from your classpath because now the fat jar includes all you need."}),"\n",(0,i.jsx)(o.p,{children:(0,i.jsx)(o.strong,{children:"Alternate workaround - 3"})}),"\n",(0,i.jsxs)(o.p,{children:["If sbt is not your choice, you can also use ",(0,i.jsx)(o.code,{children:"maven-shade-plugin"})," to make a fat jar: relocation all Jackson packages will resolve it too. In this way, druid will not be affected by Jackson library embedded in hadoop. Please follow the steps below:"]}),"\n",(0,i.jsxs)(o.p,{children:["(1) Add all extensions you needed to ",(0,i.jsx)(o.code,{children:"services/pom.xml"})," like"]}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-xml",children:"<dependency>\n     <groupId>org.apache.druid.extensions</groupId>\n     <artifactId>druid-avro-extensions</artifactId>\n     <version>${project.parent.version}</version>\n </dependency>\n\n <dependency>\n     <groupId>org.apache.druid.extensions</groupId>\n     <artifactId>druid-parquet-extensions</artifactId>\n     <version>${project.parent.version}</version>\n </dependency>\n\n <dependency>\n     <groupId>org.apache.druid.extensions</groupId>\n     <artifactId>druid-hdfs-storage</artifactId>\n     <version>${project.parent.version}</version>\n </dependency>\n\n <dependency>\n     <groupId>org.apache.druid.extensions</groupId>\n     <artifactId>mysql-metadata-storage</artifactId>\n     <version>${project.parent.version}</version>\n </dependency>\n"})}),"\n",(0,i.jsx)(o.p,{children:"(2) Shade Jackson packages and assemble a fat jar."}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-xml",children:'<plugin>\n     <groupId>org.apache.maven.plugins</groupId>\n     <artifactId>maven-shade-plugin</artifactId>\n     <executions>\n         <execution>\n             <phase>package</phase>\n             <goals>\n                 <goal>shade</goal>\n             </goals>\n             <configuration>\n                 <outputFile>\n                     ${project.build.directory}/${project.artifactId}-${project.version}-selfcontained.jar\n                 </outputFile>\n                 <relocations>\n                     <relocation>\n                         <pattern>com.fasterxml.jackson</pattern>\n                         <shadedPattern>shade.com.fasterxml.jackson</shadedPattern>\n                     </relocation>\n                 </relocations>\n                 <artifactSet>\n                     <includes>\n                         <include>*:*</include>\n                     </includes>\n                 </artifactSet>\n                 <filters>\n                     <filter>\n                         <artifact>*:*</artifact>\n                         <excludes>\n                             <exclude>META-INF/*.SF</exclude>\n                             <exclude>META-INF/*.DSA</exclude>\n                             <exclude>META-INF/*.RSA</exclude>\n                         </excludes>\n                     </filter>\n                 </filters>\n                 <transformers>\n                     <transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/>\n                 </transformers>\n             </configuration>\n         </execution>\n     </executions>\n </plugin>\n'})}),"\n",(0,i.jsxs)(o.p,{children:["Copy out ",(0,i.jsx)(o.code,{children:"services/target/xxxxx-selfcontained.jar"})," after ",(0,i.jsx)(o.code,{children:"mvn install"})," in project root for further usage."]}),"\n",(0,i.jsxs)(o.p,{children:["(3) run hadoop indexer (post an indexing task is not possible now) as below. ",(0,i.jsx)(o.code,{children:"lib"})," is not needed anymore. As hadoop indexer is a standalone tool, you don't have to replace the jars of your running services:"]}),"\n",(0,i.jsx)(o.pre,{children:(0,i.jsx)(o.code,{className:"language-bash",children:"java -Xmx32m \\\n  -Dfile.encoding=UTF-8 -Duser.timezone=UTC \\\n  -classpath config/hadoop:config/overlord:config/_common:$SELF_CONTAINED_JAR:$HADOOP_DISTRIBUTION/etc/hadoop \\\n  -Djava.security.krb5.conf=$KRB5 \\\n  org.apache.druid.cli.Main index hadoop \\\n  $config_path\n"})})]})}function h(e={}){const{wrapper:o}={...(0,a.R)(),...e.components};return o?(0,i.jsx)(o,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);